# ğŸ“ OPTIMIZED PHASE D & E SYLLABUS v13 (DSA MASTERY)

**Version:** 13.0 (Paradigm-Focused, v12-Based with Enhancements)  
**Date:** January 24, 2026  
**Status:** âœ… PRODUCTION READY  
**Philosophy:** Algorithm paradigms first, then integration across problem types

---

## ğŸ“– CURRICULUM ROLE & CONTEXT

### What You've Learned (Phases A-C, Weeks 1-11)
- âœ… Memory models, recursion, complexity analysis
- âœ… Arrays, linked lists, stacks, queues
- âœ… Sorting (bubble, merge, quick, heap) and hashing
- âœ… Two pointers, sliding windows, binary search, monotonic stack
- âœ… Trees, BSTs, balanced trees, heaps
- âœ… Graph representations, BFS, DFS, topological sort
- âœ… Shortest paths, MST, connectivity algorithms
- âœ… Tries, segment trees, BIT, Union-Find, suffix structures
- âœ… Dynamic programming (1D, 2D, trees, DAGs, bitmask)

### What Phase D & E Adds (Weeks 12-15)
- ğŸ”· **Algorithm paradigms** as design frameworks (greedy, backtracking, branch & bound)
- ğŸ”· **Proof techniques** for algorithm correctness (exchange arguments, induction)
- ğŸ”· **Formal analysis** (amortized analysis: aggregate, accounting, potential)
- ğŸ”· **Advanced string algorithms** (KMP, Z-algorithm, Manacher)
- ğŸ”· **Network flow optimization** (max-flow, min-cut, matching)
- ğŸ”· **Integration skills** (combining paradigms on complex problems)

---

# ğŸ§© **PHASE D â€” ALGORITHM PARADIGMS (Weeks 12â€“13)**

---

## **WEEK 12: GREEDY ALGORITHMS & PROOF TECHNIQUES**

### ğŸ“Œ **Primary Goal**
Master the greedy paradigm: understand when locally optimal choices lead to globally optimal solutions, and how to prove correctness rigorously via exchange arguments.

### ğŸ“Œ **Why This Week Comes Here**
By now you've learned DP (week 11) as one approach to optimization. Greedy provides a **fundamentally different strategy** with different proof requirements. Learning to distinguish when greedy works vs when DP is needed is a critical skill.

### ğŸ“Œ **Learning Outcomes**
By end of week, you should:
- [ ] Define greedy choice property and optimal substructure
- [ ] Prove greedy algorithms correct using exchange arguments
- [ ] Identify problems where greedy fails
- [ ] Choose between greedy, DP, and other paradigms confidently

---

### ğŸ“… **Day 1 (Core): Greedy Paradigm Foundations**

**Conceptual Goals:** Understand the structure of greedy algorithms and what makes them work.

**Topics & Subtopics:**
- ğŸ¯ **Greedy Choice Property**
  - Definition: local best choice is in some optimal solution
  - Why it's not always true (counterexamples)
  - How to recognize it in problems
  
- ğŸ§® **Optimal Substructure**
  - Definition: optimal solution built from optimal sub-solutions
  - Contrast with DP (DP also requires this, but different subproblems)
  - Why both properties needed simultaneously
  
- ğŸ“Š **Problem Classification**
  - Optimization vs decision vs enumeration problems
  - Which types suit greedy approach
  - What to check before claiming "greedy works"
  
- ğŸ”§ **Greedy Paradigm Template**
  - Make locally optimal choice
  - Recurse on remaining problem
  - Combine results
  - Generic pseudocode structure

**Key Insight:** Greedy is seductive but dangerous. Not all locally optimal choices lead globally. You must prove it works!

**Real-World Connection:** Greedy underlies many heuristics in scheduling, routing, and resource allocation.

---

### ğŸ“… **Day 2 (Core): Interval Scheduling (Canonical Greedy Example)**

**Conceptual Goals:** Work through a complete greedy algorithm with rigorous proof.

**Topics & Subtopics:**
- ğŸ“… **Problem Statement**
  - Input: n intervals with start times s_i and finish times f_i
  - Output: maximum number of non-overlapping intervals
  - Real applications: classroom scheduling, conference room booking, event scheduling
  
- ğŸ“Š **Why Greedy Works Here**
  - Greedy choice: always pick interval with earliest finish time
  - Why earliest finish? It leaves room for future intervals
  - Optimal substructure: remaining problem is selecting from remaining intervals
  
- ğŸ”‘ **The Algorithm**
  - Sort intervals by finish time
  - Greedily pick earliest finishing non-overlapping interval
  - Time: O(n log n) sorting + O(n) selection = O(n log n)
  - Space: O(n) for sorted list
  
- âŒ **Why Not Other Choices?**
  - Earliest start time: fails (reserves early time unnecessarily)
  - Shortest duration: fails (blocks more intervals than necessary)
  - Most overlaps: fails (doesn't guarantee optimality)
  - **Intuitive proof:** earliest finish leaves most room
  
- ğŸ“– **Exchange Argument Proof**
  - Lemma: If OPT picks different first interval, swap to our choice
  - After swap, OPT still valid (our interval finishes sooner)
  - Induction on remaining intervals
  - Greedy is optimal
  
- âš–ï¸ **Weighted Variant (Why Greedy Fails)**
  - Each interval has value/weight
  - Greedy by value/weight ratio doesn't work
  - Counterexample: interval [1,10] value 100 vs intervals [1,5] value 60 and [6,10] value 50
  - **Lesson:** Adding weights breaks greedy choice property
  - **Fix:** Use DP (overlapping subproblems with weights)
  
- ğŸ”„ **Related Problems**
  - Job completion time scheduling
  - Maximum disjoint activities
  - Minimizing lateness with deadlines

**Key Insight:** Exchange argument is the universal proof technique for greedy. Master it!

---

### ğŸ“… **Day 3 (Core): Minimum Spanning Trees (Greedy on Graphs)**

**Conceptual Goals:** Understand greedy applied to graph optimization; see cut property as justification.

**Topics & Subtopics:**
- ğŸŒ³ **MST Problem Revisited**
  - Definition: spanning tree with minimum total edge weight
  - Why greedy perspective? Pick lightest edges first
  - Optimal substructure: remaining edges form MST of remaining graph
  - Greedy choice property: light edge always in some MST
  
- âœ‚ï¸ **Kruskal's Algorithm (Edge-Centric Greedy)**
  - Sort all edges by weight (ascending)
  - Greedily add lightest edge that doesn't create cycle
  - Use Union-Find for cycle detection
  - Time: O(E log E) sorting + O(E Î±(n)) union-find = O(E log E)
  
- âš™ï¸ **Prim's Algorithm (Vertex-Centric Greedy)**
  - Start from arbitrary vertex
  - Greedily add lightest edge leaving current tree
  - Maintain priority queue of edges from tree to non-tree vertices
  - Time: O((V + E) log V) with heap
  
- ğŸ“Š **Kruskal vs Prim Comparison**
  
  | Aspect | Kruskal | Prim |
  |--------|---------|------|
  | **Approach** | Edge-centric | Vertex-centric |
  | **Data Structure** | Sort + Union-Find | Priority queue |
  | **Time (dense Eâ‰ˆVÂ²)** | O(VÂ² log V) | O(VÂ² log V) |
  | **Time (sparse Eâ‰ˆV)** | O(V log V) | O(V log V) |
  | **Time (intermediate)** | O(E log V) | O(E log V) |
  | **Parallelizable** | Better (processes edges independently) | Harder (tree grows sequentially) |
  | **Best for** | Sparse graphs | Dense graphs |
  
- ğŸ§® **Cut Property (Theoretical Justification)**
  - Definition: partition vertices into (S, T), lightest crossing edge in some MST
  - Proof by contradiction: if not included, swap with heavier edge
  - Why Kruskal satisfies cut property: edge connects two components
  - Why Prim satisfies cut property: tree-to-non-tree edge is light
  
- ğŸ”‘ **Key Difference from Interval Scheduling**
  - Interval: "earliest finish leaves room"
  - MST: "lightest edge connects components"
  - Both exemplify greedy choice property + proof by exchange
  
- ğŸŒ **Real-World Applications**
  - Network design (minimum cost cable)
  - Electrical circuit design
  - Clustering and community detection
  - Approximate TSP (MST gives 2-approximation)

**Key Insight:** MST algorithms show greedy works on graphs via cut property. Exchange arguments generalize!

---

### ğŸ“… **Day 4 (Optional Advanced): Huffman Coding & Greedy on Data Structures**

**Conceptual Goals:** See greedy applied to building optimal structures; understand information-theoretic optimality.

**Topics & Subtopics:**
- ğŸ“¦ **Huffman Coding Problem**
  - Goal: encode symbols with minimum average code length
  - Motivation: compression (text, images, communication)
  - Prefix property: no codeword is prefix of another (enables unique decoding)
  
- ğŸŒ³ **Building Huffman Trees**
  - Represent codes as binary tree (left=0, right=1)
  - Leaf for each symbol; path gives code
  - Shorter paths for frequent symbols
  
- ğŸ”§ **Huffman's Greedy Algorithm**
  - Maintain priority queue of nodes (initially symbols with frequencies)
  - Repeatedly extract two smallest-frequency nodes
  - Create parent node with combined frequency
  - Insert parent back into queue
  - Continue until single root
  - Time: O(n log n) with heap
  
- ğŸ“– **Correctness Proof (Exchange Argument + Induction)**
  - **Lemma 1:** Two minimum-frequency symbols are siblings in optimal tree
    - Proof: If not, swap with actual siblings â†’ cost decreases (contradiction)
  - **Lemma 2:** Merging smallest frequencies is optimal for subproblem
    - Proof: Induction on merged node treated as single symbol
  - Greedy choice property: create node with smallest combined frequency
  - Optimal substructure: remaining tree is optimal for remaining symbols
  
- ğŸ“š **Information-Theoretic Optimality**
  - Shannon entropy: H(X) = Î£ -p_i logâ‚‚(p_i)
  - Huffman achieves: H(X) â‰¤ L < H(X) + 1
  - Where L = average code length
  - Huffman is optimal within 1 bit per symbol!
  
- ğŸŒ **Real-World Impact**
  - DEFLATE (gzip), JPEG, MP3 all use Huffman coding
  - Basis for optimal prefix codes
  - Motivation for arithmetic coding (even better)

**Key Insight:** Huffman shows greedy on structures. Proof via induction + lemmas about minimum elements.

---

### ğŸ“… **Day 5 (Core): When Greedy Fails & Decision Framework**

**Conceptual Goals:** Recognize greedy failure patterns; master the decision process for algorithm selection.

**Topics & Subtopics:**
- âŒ **Five Detailed Counterexamples**

  1. **Weighted Interval Scheduling**
     - Problem: maximize total value of non-overlapping intervals
     - Greedy by value fails: high-value interval blocks two lower-value ones
     - Greedy by value/duration fails: similar issue
     - Root cause: value per unit time varies by interval
     - Why DP works: overlapping subproblems (decision: include/exclude each interval)
  
  2. **Coin Change with Arbitrary Denominations**
     - Problem: minimum coins to make amount A
     - Greedy by largest coin fails: coins {1, 3, 4}, amount 6
       - Greedy: 4+1+1 (3 coins)
       - Optimal: 3+3 (2 coins)
     - Root cause: denominations don't have "greedy property"
     - Why DP works: state = (amount), transition = try each coin
  
  3. **Graph Coloring (NP-Hard)**
     - Problem: color graph with minimum colors
     - Greedy (First-Fit): color vertices in some order with lowest available color
     - May use many colors; optimal uses far fewer
     - Root cause: NP-hard problem; no known polynomial algorithm
     - Why exchange argument impossible: no local characterization of global optimum
  
  4. **Longest Path in DAG (vs Shortest Path)**
     - Shortest path: greedy works (Dijkstra)
     - Longest path: greedy fails (NP-hard in general graphs, solvable in DAGs via DP)
     - Why different? Negative weights conceptually turn longest into shortest
     - Longest in DAGs uses DP (topological order, not greedy)
  
  5. **Job Scheduling with Deadlines**
     - Problem: schedule jobs with deadlines to maximize profit
     - Greedy by profit fails: low-profit job blocks high-profit ones
     - Greedy by deadline fails: creates infeasible schedule
     - Root cause: conflicting objectives (maximize profit AND meet deadline)
     - Why DP works: state = (time slot, jobs considered), transition = include/exclude
  
- ğŸ§­ **Pattern Recognition: When Greedy Fails**
  - Conflicting objectives: maximize profit AND minimize time
  - Overlapping constraints: each choice affects future feasibility
  - Weighted decisions: value per unit changes
  - NP-hard problems: greedy rarely optimal
  - Future information needed: current choice should depend on future
  
- âš–ï¸ **Greedy vs DP Decision Framework**

  | Dimension | Greedy | DP |
  |-----------|--------|-----|
  | **Subproblems** | Independent (1 choice per step) | Overlapping (multiple paths) |
  | **Optimality** | Local â†’ global (if greedy choice + OS) | Optimal combination of suboptimal solutions |
  | **Proof** | Exchange argument | Induction on DP recurrence |
  | **Complexity** | Usually O(n log n) or O(n) | Usually O(nÂ²) or worse |
  | **Space** | O(1) to O(n) | O(n) to O(nÂ²) |
  | **Indicator** | "Pick best now" | "Store choices, reconsider later" |
  | **Examples** | MST, activity selection, Huffman | Knapsack, LCS, edit distance |
  
- ğŸ” **Problem-Solving Strategy**
  1. Try greedy first (simpler)
  2. Look for counterexample
  3. If found, switch to DP or other paradigm
  4. If not found, prove via exchange argument
  5. If proof fails, reconsider problem structure
  
- ğŸ”„ **Hybrid Approaches**
  - Greedy heuristic + approximation analysis (e.g., greedy set cover has ln n approximation)
  - Greedy for initial solution, DP for refinement
  - Greedy on some substructure, DP on others
  
- ğŸ¯ **Meta-Lesson**
  - Problem understanding trumps algorithm selection
  - Greedy's elegance tempts overuse
  - Exchange arguments are discipline forcing rigor
  - "Prove it works" is your alarm bell

**Key Insight:** Master the decision process. Knowing when greedy fails is as important as knowing when it works!

---

## **WEEK 13: BACKTRACKING, BRANCH & BOUND & AMORTIZED ANALYSIS**

### ğŸ“Œ **Primary Goal**
Introduce backtracking as systematic search with pruning; branch and bound as optimization; formalize amortized analysis as mathematical framework.

### ğŸ“Œ **Why This Week Comes Here**
Previous weeks covered paradigms (greedy, DP) and data structures. This week introduces **exploration paradigms** (backtracking, B&B) and **formal cost analysis** (amortized), expanding your algorithmic toolkit.

### ğŸ“Œ **Learning Outcomes**
By end of week, you should:
- [ ] Design backtracking algorithms with pruning strategies
- [ ] Distinguish backtracking (feasibility) from branch & bound (optimality)
- [ ] Design bounding functions for B&B
- [ ] Analyze amortized costs using three formal methods
- [ ] Apply to constraint satisfaction, optimization, and data structure problems

---

### ğŸ“… **Day 1 (Core): Backtracking Paradigm & Constraint-Driven Pruning**

**Conceptual Goals:** Understand backtracking as systematic DFS with constraint-driven pruning.

**Topics & Subtopics:**
- ğŸ” **Backtracking Definition**
  - Systematic exploration of solution space via DFS
  - Build solution incrementally
  - Prune when constraints violated (don't explore further)
  - Backtrack: undo choice, try alternative
  - Goal: find ANY or ALL feasible solutions
  
- ğŸŒ³ **State Space Tree**
  - Nodes: partial solutions at each decision level
  - Edges: possible choices at current level
  - Leaves: complete solutions (valid or invalid)
  - Depth: number of decisions made
  - Breadth: number of choices per decision
  
- âš™ï¸ **Generic Backtracking Algorithm**
  ```
  Backtrack(current_solution, remaining_choices):
    if is_complete(current_solution):
      if is_valid(current_solution):
        report_solution(current_solution)
      return
    
    for choice in remaining_choices:
      if is_feasible(choice, current_solution):  // PRUNING HERE
        add_choice_to_solution(choice)
        new_remaining = remaining_choices - {choice}
        Backtrack(current_solution, new_remaining)
        remove_choice_from_solution(choice)  // BACKTRACK
  ```
  
- ğŸ“Š **Complexity Analysis**
  - Worst case: O(b^d) where b = branching factor, d = depth
  - With pruning: drastically reduced (problem-dependent)
  - Example: N-Queens worst O(N!), typical O(N) with good pruning
  - Lesson: pruning is everything
  
- ğŸ’¾ **Space Complexity**
  - O(d) for recursion call stack
  - O(d) for partial solution storage
  - O(d) total; much better than storing all solutions
  
- ğŸ¯ **Key Decisions in Backtracking Design**
  - State representation (what defines partial solution)
  - Choice enumeration (how to generate candidates)
  - Feasibility check (when to prune)
  - Pruning heuristics (how to prune aggressively)
  
- ğŸ§  **Pruning Strategies**
  - **Constraint checking:** if choice violates hard constraint, skip
  - **Bound checking:** if remaining can't improve best, skip (preview of B&B)
  - **Variable ordering:** choose most constrained variable first (MRV heuristic)
  - **Forward checking:** check if any remaining variable impossible

**Key Insight:** Backtracking success depends entirely on pruning. Weak pruning = exponential blowup.

---

### ğŸ“… **Day 2 (Core): Backtracking Applications (N-Queens, Sudoku, Constraint Satisfaction)**

**Conceptual Goals:** See backtracking on realistic constraint satisfaction problems.

**Topics & Subtopics:**
- â™š **N-Queens Problem (Canonical)**
  - Problem: place N queens on NÃ—N board, no two attack
  - State: current row's decision (which column for queen)
  - Choices: each column in current row
  - Constraints: no conflicts with previous queens
  - Feasibility check: check column, both diagonals
  - Algorithm:
    - Place queen in each row
    - For each column, check if safe
    - If safe, place and recurse to next row
    - If no column safe, backtrack
  - Time: O(N!) worst, O(N) typical with pruning
  - Space: O(N) for recursion stack
  - Real optimization: track available columns/diagonals to avoid re-checking
  
- ğŸ² **Sudoku Solver (Advanced Constraint Satisfaction)**
  - Problem: fill 9Ã—9 grid with 1-9, constraints:
    - Each row unique 1-9
    - Each column unique 1-9
    - Each 3Ã—3 box unique 1-9
  - State: current grid fill (81 cells)
  - Choices: unassigned cells, valid digits for that cell
  - Feasibility: check row/column/box for duplicates
  - Algorithm:
    - Find empty cell with minimum remaining values (MRV heuristic)
    - Try each valid digit
    - Constraint propagation: deduce implications
    - Backtrack if contradiction found
  - Optimization: constraint propagation (naked singles, hidden singles) reduces branching drastically
  - Time: O(9^81) theoretical, O(10^6-10^7) with propagation, O(10^3-10^4) with MRV heuristic
  
- ğŸ”¤ **Word Search / Boggle (Path-Finding Backtracking)**
  - Problem: find word path in letter grid
  - State: current position, characters matched
  - Choices: adjacent unvisited neighbors
  - Constraints: next letter must match word
  - Algorithm:
    - DFS from each grid cell
    - Try adjacent neighbors
    - Mark visited to avoid reuse
    - Backtrack when letter doesn't match
  - Time: O(N Ã— M Ã— 4^L) where NÃ—M grid, L word length
  - Optimization: early termination (letter mismatch), trie for multiple words
  
- ğŸ¨ **Graph Coloring Variants**
  - Problem: color vertices with minimum colors
  - State: current color assignment
  - Choices: colors for current vertex
  - Constraints: neighbors have different colors
  - Algorithm:
    - Try to color with k colors (binary search on k)
    - For each vertex, try each color
    - Check no adjacent vertex same color
    - Backtrack if stuck
  - Heuristics: color highest-degree vertices first
  
- ğŸ§­ **Heuristics for Better Pruning**
  - **Minimum Remaining Values (MRV):** choose variable with fewest remaining choices first
  - **Least Constraining Value (LCV):** try values that eliminate fewest future choices
  - **Constraint Propagation:** after assignment, deduce forced assignments
  - **Forward Checking:** after assignment, reduce domains of unassigned variables
  
- ğŸ§  **Pattern Recognition**
  - Backtracking for: puzzles, CSP, enumeration, search
  - NP-hard problems: backtracking explores exponential space
  - Pruning critical: without it, exponential blowup
  - Heuristics: dramatically reduce practical complexity

**Key Insight:** Heuristics transform exponential worst-case into practical for small-medium problems!

---

### ğŸ“… **Day 3 (Core): Branch and Bound Paradigm (Optimized Backtracking)**

**Conceptual Goals:** Understand branch and bound as backtracking enhanced with bounding functions for optimization.

**Topics & Subtopics:**
- ğŸ¯ **Definition & Distinction from Backtracking**
  - Backtracking: prune when constraints violated (feasibility)
  - Branch & Bound: prune when bound worse than best (optimality)
  - Both use decision trees; B&B adds numerical comparison
  - Goal: find optimal solution, not just feasible
  
- ğŸ—ï¸ **Node Structure in B&B Tree**
  - Level: depth (how many decisions made)
  - Value: actual cost/benefit of partial solution
  - Bound: estimated best achievable from this node
  - For minimization: bound is lower bound (can't do better)
  - For maximization: bound is upper bound (can't do better)
  
- âš™ï¸ **Generic B&B Algorithm**
  ```
  BranchAndBound(root_problem):
    best_value = initial_bound (e.g., infinity for minimization)
    best_solution = None
    queue = [root_node]
    
    while queue not empty:
      node = select_from_queue(queue)  // strategy: FIFO, LIFO, best-first
      
      if node.bound >= best_value:  // for minimization
        continue  // prune: can't improve
      
      if is_leaf(node):  // complete solution
        if node.value < best_value:
          best_value = node.value
          best_solution = node
      else:  // expand node
        for each child in expand(node):
          if child.bound < best_value:
            queue.add(child)  // may improve
    
    return best_solution
  ```
  
- ğŸ§® **Bounding Function Design (Critical!)**
  - Tight bounds: close to actual (more pruning)
  - Fast computation: O(1) or O(log n) per node
  - Lower bound feasible: actually achievable or lower estimate
  - Design techniques:
    - **Relaxation:** remove constraints, solve relaxed problem
    - **Greedy:** use greedy on remaining choices
    - **Linear Programming:** solve LP relaxation
    - **Problem-specific:** use domain knowledge
  
- ğŸ“Š **Queue Selection Strategies & Trade-offs**
  
  | Strategy | Queue | Pruning | Memory | Time |
  |----------|-------|---------|--------|------|
  | **FIFO (BFS)** | Queue | Medium | High | Slow |
  | **LIFO (DFS)** | Stack | Medium | Low | Medium |
  | **Best-First** | Priority by bound | Best | High | Fast |
  | **LC-Search** | Priority by cost | Good | High | Medium |
  
  - FIFO: breadth-first, explores level-by-level
  - LIFO: depth-first, finds solution quickly, reuses memory
  - Best-First: explores most promising first, best pruning but memory intensive
  
- ğŸ’¡ **Real-World Insight**
  - B&B turns some NP-hard problems practical (for small-medium sizes)
  - Good bounding is crucial (determines pruning rate)
  - Used in: integer programming, scheduling, optimization

**Key Insight:** B&B success depends on bounding function tightness. Loose bounds = little pruning!

---

### ğŸ“… **Day 4 (Core): Branch and Bound Applications (Knapsack, TSP, Assignment)**

**Conceptual Goals:** Implement B&B on classic optimization problems with effective bounding.

**Topics & Subtopics:**
- ğŸ’ **0/1 Knapsack via B&B**
  - State: items 0..i, included/excluded decisions
  - Decision tree: binary for each item (include/exclude)
  - Bound: use **greedy relaxation** on remaining items
    - Sort remaining by value/weight ratio
    - Fill fractional knapsack greedily
    - Upper bound = current value + fractional profit
  - Pruning: if bound â‰¤ best_value found, prune branch
  - Algorithm:
    ```
    KnapsackBB(items, capacity):
      items.sort_by(lambda i: i.value/i.weight, descending)
      best_value = 0
      best_solution = None
      
      def branch_and_bound(index, weight, value):
        nonlocal best_value, best_solution
        
        if weight > capacity:
          return  // infeasible
        
        if index == num_items:
          if value > best_value:
            best_value = value
            best_solution = ...
          return
        
        # Bound: greedy on remaining
        bound = value + greedy_relax(items[index:], capacity - weight)
        if bound < best_value:
          return  // prune
        
        # Try including item
        if weight + items[index].weight <= capacity:
          branch_and_bound(index+1, weight+items[index].weight, value+items[index].value)
        
        # Try excluding item
        branch_and_bound(index+1, weight, value)
      
      branch_and_bound(0, 0, 0)
      return best_solution
    ```
  - Time: O(2^n) worst, but often dramatically better with pruning
  - Practical: works well for n â‰¤ 30-40 with good bounds
  - Key: greedy relaxation is tight bound (fractional knapsack optimal)
  
- ğŸš— **Traveling Salesman Problem (TSP) via B&B**
  - State: partial tour, cities visited
  - Decision tree: at each city, choose next unvisited city
  - Bound: **MST of unvisited cities** + current cost
    - MST gives lower bound on remaining tour cost
    - Tighter bound: Held-Karp bound (more complex)
  - Pruning: if current + bound â‰¥ best_tour, prune
  - Algorithm:
    ```
    TSPBB(cities, distances):
      best_tour = nearest_neighbor_heuristic(cities)
      best_length = tour_length(best_tour)  // good initial bound
      
      def branch_and_bound(current_tour, visited, current_cost):
        nonlocal best_tour, best_length
        
        remaining = all_cities - visited
        
        if not remaining:
          if current_cost < best_length:
            best_length = current_cost
            best_tour = current_tour
          return
        
        # Bound: MST of remaining
        lower_bound = current_cost + mst_weight(remaining)
        if lower_bound >= best_length:
          return  // prune
        
        # Try each remaining city
        for next_city in remaining:
          new_cost = current_cost + dist(last(current_tour), next_city)
          branch_and_bound(current_tour + [next_city], visited | {next_city}, new_cost)
      
      branch_and_bound([start_city], {start_city}, 0)
      return best_tour
    ```
  - Optimizations:
    - Start with good heuristic (nearest neighbor)
    - Branch on most promising cities first
    - Use tighter bounds (Held-Karp)
  - Time: O(n!) worst, practical for n â‰¤ 20-25
  
- ğŸ‘¥ **Assignment Problem (Min-Cost Matching)**
  - Problem: assign n workers to n jobs, minimize total cost
  - State: current partial assignment
  - Bound: **Hungarian algorithm** on unassigned workers/jobs
  - Time: significantly better than TSP due to tighter structure
  
- ğŸ” **Comparison: Backtracking vs Branch & Bound**

  | Aspect | Backtracking | Branch & Bound |
  |--------|---|---|
  | **Purpose** | Find ANY/ALL feasible | Find OPTIMAL |
  | **Pruning** | Constraint violation | Suboptimal bound |
  | **Goal** | Feasible solution | Best solution |
  | **Examples** | Puzzles, CSP, enumeration | Optimization, integer programming |
  | **Complexity** | Problem-dependent | Exponential (pruning-dependent) |
  | **Proof of optimality** | None (find first feasible) | Bounds guarantee optimality |

**Key Insight:** Good bounding turns exponential into practical. MST bounds work well for TSP!

---

### ğŸ“… **Day 5 (Core): Comprehensive Algorithm Selection Framework**

**Conceptual Goals:** Master decision process for paradigm selection across all learned paradigms.

**Topics & Subtopics:**
- ğŸ¯ **Master Paradigm Comparison**

  | Paradigm | Use Case | Complexity | Space | Optimality | Difficulty |
  |----------|----------|-----------|-------|-----------|------------|
  | **Brute Force** | Verify, small n | Exponential | O(1)-O(n) | Optimal | Easy |
  | **Divide & Conquer** | Sorting, searching, geometry | Poly (O(n log n)) | Poly | Optimal | Medium |
  | **Decrease & Conquer** | Sorted sequences | Poly | O(1)-O(n) | Optimal | Easy |
  | **Dynamic Programming** | Overlapping subproblems | Poly | Poly | Optimal | Hard |
  | **Greedy** | Matroid, exchange property | Poly | O(1)-O(n) | Sometimes | Easy-Medium |
  | **Backtracking** | Puzzles, CSP, enumeration | Exponential | O(d) | Feasible | Medium |
  | **Branch & Bound** | Optimization, integer prog | Exponential | Exponential | Optimal | Hard |
  | **Approximation** | NP-hard, large n | Poly | Poly | Bounded ratio | Medium-Hard |
  | **Local Search** | Non-convex, large n | Poly/Exponential | O(1)-O(n) | Local optimum | Easy |
  
- ğŸ§­ **Problem-Solving Flowchart**
  ```
  Problem Classification:
  â”œâ”€ Find ANY feasible solution?
  â”‚  â”œâ”€ Yes, fast â†’ Backtracking with pruning
  â”‚  â””â”€ No (need optimal)
  â”‚
  â”œâ”€ NP-hard?
  â”‚  â”œâ”€ Yes, n small (â‰¤20) â†’ Brute force, backtracking, B&B
  â”‚  â”œâ”€ Yes, n medium (20-30) â†’ B&B with good bounds, approximation
  â”‚  â”œâ”€ Yes, n large (>30) â†’ Approximation, local search, heuristics
  â”‚  â””â”€ No (polynomial)
  â”‚
  â”œâ”€ Overlapping subproblems?
  â”‚  â”œâ”€ Yes â†’ Dynamic Programming
  â”‚  â””â”€ No, independent subproblems? â†’ Divide and Conquer
  â”‚
  â”œâ”€ Can divide problem into two? â†’ Decrease & Conquer (binary search)
  â”‚
  â”œâ”€ Greedy choice property?
  â”‚  â”œâ”€ Yes, optimal substructure? â†’ Greedy (prove via exchange)
  â”‚  â””â”€ No â†’ DP or other
  â”‚
  â”œâ”€ Can model as network/LP? â†’ Network Flow, Linear Programming
  â”‚
  â””â”€ Else â†’ Approximation, local search, heuristics
  ```
  
- ğŸ“š **Problem-Type to Paradigm Mapping**

  | Problem | Best Paradigm | Why | Complexity |
  |---------|---------------|-----|-----------|
  | **N-Queens, Sudoku** | Backtracking | Constraint satisfaction | Exponential (pruned) |
  | **MST** | Greedy (Kruskal/Prim) | Cut property + exchange argument | O(E log E) or O(E log V) |
  | **Shortest path** | Greedy (Dijkstra) | Greedy choice property | O(E log V) |
  | **0/1 Knapsack** | DP or B&B | Overlapping subproblems / optimization | O(nW) or O(2^n) |
  | **Fractional Knapsack** | Greedy | Greedy choice (value/weight ratio) | O(n log n) |
  | **TSP** | B&B (exact) or approximation (approx) | NP-hard, exponential / practical | O(n!) or O(nÂ²) |
  | **Edit Distance** | DP | Overlapping subproblems | O(mn) |
  | **LCS** | DP | Overlapping subproblems | O(mn) |
  | **Activity Selection** | Greedy | Greedy choice + optimal substructure | O(n log n) |
  | **Huffman Coding** | Greedy | Build optimal tree incrementally | O(n log n) |
  | **Graph Coloring** | Backtracking + heuristics | CSP, NP-hard | Exponential |
  | **Bipartite Matching** | Network Flow | Reduction to max-flow | O(VEÂ²) |
  | **Assignment** | Min-cost flow or Hungarian | Optimal matching with costs | O(nÂ³) |
  | **Text Justification** | DP | Story-driven, optimize badness function | O(nÂ²) |
  
- ğŸ” **Case Study: Weighted Interval Scheduling**
  - Seems greedy (pick by some metric)
  - Why greedy fails: two small intervals may have better total value
  - Why DP works: state = (interval index), decision = include/exclude
  - Complexity: DP O(nÂ²), greedy attempt O(n log n)
  
- ğŸ” **Case Study: Fractional vs 0/1 Knapsack**
  - Fractional: Greedy value/weight works (can take partial items)
  - 0/1: Greedy fails (must decide on whole items)
  - Why different: fractionality enables greedy choice property
  - Lesson: problem constraints determine algorithm choice
  
- ğŸ” **Case Study: Shortest Path Variants**
  - Non-negative weights: Greedy (Dijkstra) O(E log V)
  - Negative weights (no negative cycles): Bellman-Ford O(VE)
  - All-pairs: DP (Floyd-Warshall) O(VÂ³)
  - Why different: negative weights break greedy property
  
- ğŸ§  **Meta-Skills for Algorithm Selection**
  1. Understand problem structure (constraints, objectives)
  2. Identify problem type (optimization, decision, enumeration)
  3. Check for problem properties (overlapping subproblems, greedy choice, etc.)
  4. Select paradigm (greedy, DP, backtracking, etc.)
  5. Prove correctness (exchange argument, induction, etc.)
  6. Analyze complexity (time, space, amortized)
  7. Consider trade-offs (optimality, simplicity, practical performance)

**Key Insight:** Algorithm selection is systematic. Learn to ask right questions!

---

### ğŸ“… **Day 6 (Core): Amortized Analysis (Formal Methods)**

**Conceptual Goals:** Master three formal methods for amortized cost analysis.

**Topics & Subtopics:**
- ğŸ’¡ **Motivation: Why Amortized Analysis?**
  - Problem: some operations are expensive (e.g., resizing), others cheap
  - Naive analysis: O(n) per push in dynamic array (resize cost)
  - Reality: amortized O(1) per push (resize rare, amortized over many pushes)
  - Amortized analysis: average cost guarantee, not probabilistic
  
- ğŸ“Š **Method 1: Aggregate Method**
  - Total cost of sequence / number of operations
  - Proof: compute total cost over all operations, divide by count
  
  - **Example: Dynamic Array Push**
    - n pushes into initially empty array
    - Capacity doubling: start 1, then 2, 4, 8, ..., 2^k
    - Resize costs: 1 + 2 + 4 + ... + 2^k = 2^(k+1) - 1 â‰ˆ 2n
    - Other pushes: n
    - Total: O(n)
    - Amortized per push: O(n) / n = O(1)
  
  - **Example: Stack with MultiPop**
    - Operation: pop multiple elements (pop k)
    - Observation: total elements popped â‰¤ total elements pushed
    - Total cost: O(n) (at most n pops total)
    - Amortized per operation: O(1) average
  
  - Strengths: simple, intuitive
  - Weaknesses: doesn't identify which operations pay for which
  
- ğŸ’³ **Method 2: Accounting Method**
  - Assign "amortized cost" to each operation
  - Some operations pay extra, some pay less
  - Invariant: accumulated credit â‰¥ 0 always
  - Proof: show credit invariant holds, then amortized cost â‰¤ actual cost on average
  
  - **Example: Dynamic Array Push**
    - Amortized cost = 3 per push
    - Breakdown:
      - 1 unit for storing element
      - 2 units for future resize (cost of moving)
    - Push without resize: cost 1, charge 3, accumulate 2 credits
    - After n/2 pushes: 2Ã—(n/2) = n credits accumulated
    - Resize cost: n (move n elements)
    - Use accumulated n credits to pay for resize
    - Push with resize: cost n+1, charge 3, use n credits â†’ net +2 credits
  
  - Strengths: identifies which operations pay for which
  - Weaknesses: requires intuition about credit allocation
  
- ğŸ”‹ **Method 3: Potential Method**
  - Define potential function Î¦(state)
  - Amortized cost = actual cost + Î”Î¦
  - Prove potential increases with operations, then amortized cost â‰¤ actual
  
  - **Example: Dynamic Array Push**
    - Define Î¦ = 2n - c (n = size, c = capacity)
    - Initially empty: Î¦ = 0
    - Push without resize:
      - Actual cost = 1
      - Î¦ changes: 2(n-1) - c â†’ 2n - c
      - Î”Î¦ = 2
      - Amortized = 1 + 2 = 3
    - Push with resize (capacity c â†’ 2c):
      - Actual cost = n + 1 (move n + add 1)
      - Î¦ changes: 2n - n â†’ 2(n+1) - 2(n+1) = 0
      - Î”Î¦ = 0 - (2n - c) = c - 2n â‰ˆ -n
      - Amortized = (n+1) + (-n) = 1 (not 3!)
      - Better analysis: potential makes resize cheap!
  
  - Why it works: potential "stores" credit for future use
  - Strengths: mathematically elegant, handles complex cases
  - Weaknesses: requires intuition about potential design
  
- ğŸ”— **Union-Find with Path Compression**
  - State: forest of trees with path compression
  - Potential: number of nodes minus (rank + 1) per node (intuitive explanation)
  - Result: near-constant amortized time O(Î±(n)) per operation
  - Why it works: path compression amortizes over many operations
  
- ğŸŒ² **Fibonacci Heaps (Concept-Level)**
  - Problem: decrease-key in binary heaps is O(log n)
  - Solution: Fibonacci heap with amortized O(1) decrease-key
  - Cascading cuts mechanism: defers work until needed
  - Amortized analysis crucial: actual operations exponential, amortized O(1)
  
- ğŸ§  **Choosing Method for Your Problem**
  - Aggregate: when total cost is obvious
  - Accounting: when some operations clearly pay for others
  - Potential: when operations change a "state" property
  - Try aggregate first; if doesn't work, switch to accounting or potential

**Key Insight:** Amortized analysis formalizes "expensive operations are rare" intuition!

---

# ğŸ§µ **PHASE E â€” INTEGRATION & EXTENSIONS (Weeks 14-15)**

---

## **WEEK 14: MATRIX ALGORITHMS, BACKTRACKING & PROBLEM INTEGRATION**

### ğŸ“Œ **Primary Goal**
Apply multiple paradigms (backtracking, bitmask DP, number theory) to specific problem types (matrices, combinatorics, number problems).

### ğŸ“Œ **Why This Week Comes Here**
You've learned paradigms (weeks 12-13); now apply them to real problems, integrating across topics.

### ğŸ“Œ **Learning Outcomes**
By end of week, you should:
- [ ] Solve matrix problems with multiple algorithmic approaches
- [ ] Apply backtracking to grid-based constraint problems
- [ ] Use bitmask DP for subset and permutation problems
- [ ] Apply number theory algorithms (GCD, modular arithmetic)
- [ ] Implement probability-based algorithms (reservoir sampling)

---

### ğŸ“… **Day 1 (Core): Matrix Algorithms**

**Topics & Subtopics:**
- ğŸ“Š **Standard Traversals**
  - Row-major, column-major, diagonal, spiral
  - Real applications: image processing, grid exploration
  
- ğŸ” **Matrix Search Patterns**
  - Staircase search in sorted matrix: O(m+n)
  - Why not binary search: not globally sorted
  
- ğŸ”„ **Matrix Transformations**
  - In-place 90Â° rotation
  - Time O(nÂ²), space O(1)

---

### ğŸ“… **Day 2 (Core): Backtracking on Grids (Word Search, Sudoku, Path Finding)**

**Topics & Subtopics:**
- ğŸ”¡ **Word Search / Boggle**
  - DFS + visited tracking
  - Time O(N Ã— M Ã— 4^L) with pruning

- ğŸ² **Sudoku Solver**
  - Constraint propagation + MRV heuristic
  - Time O(10^6-10^7) with good heuristics

- ğŸ›¤ï¸ **Rat in Maze / Path Finding**
  - BFS for shortest path, DFS for any path
  - Variants: obstacles, multiple targets, weighted

---

### ğŸ“… **Day 3 (Core): Bitmask Tricks & Bitmask DP**

**Topics & Subtopics:**
- ğŸ”§ **Bit Operations** (15 essential tricks)
  - Check, set, clear, toggle bits
  - Isolate rightmost set bit
  - Check if power of 2
  - Count set bits

- ğŸ­ **Subset Representation**
  - Integer as bitmask for subset
  - Iterate all subsets: O(2^n)
  - Iterate submasks: O(3^n) total

- ğŸš **Bitmask DP: Traveling Salesman**
  - State dp[mask][i] = shortest path visiting cities in mask, ending at i
  - Time O(2^n Ã— nÂ²), Space O(2^n Ã— n)
  - Applicable for n â‰¤ 20

---

### ğŸ“… **Day 4 (Optional Advanced): Number Theory & Modular Arithmetic**

**Topics & Subtopics:**
- ğŸ”¢ **Euclid's Algorithm (GCD)**
  - Time O(log(min(a,b)))
  - LCM = a Ã— b / gcd(a,b)
  - Extended GCD for modular inverse

- ğŸ§® **Modular Arithmetic**
  - Avoiding overflow with modulo
  - Fast exponentiation: a^b mod m in O(log b)
  - Applications: cryptography, hash functions

---

### ğŸ“… **Day 5 (Optional Advanced): Probability & Sampling**

**Topics & Subtopics:**
- ğŸ² **Expected Value & Linearity**
  - E[X+Y] = E[X] + E[Y] (even if dependent)
  - Application: quicksort expected analysis

- ğŸ’§ **Reservoir Sampling**
  - Select k items uniformly from unknown-size stream
  - Time O(n), Space O(k)
  - Weighted variant: exponential weights

---

## **WEEK 15: ADVANCED STRINGS & NETWORK FLOW**

### ğŸ“Œ **Primary Goal**
Master advanced string algorithms and network flow optimization.

### ğŸ“Œ **Why This Week Comes Here**
Strings and graphs are universal; these advanced techniques unlock powerful applications.

### ğŸ“Œ **Learning Outcomes**
By end of week, you should:
- [ ] Implement KMP, Z-algorithm, Manacher for string problems
- [ ] Understand max-flow min-cut theorem and applications
- [ ] Reduce matching/assignment problems to flow
- [ ] Choose between string algorithms and flow approaches

---

### ğŸ“… **Day 1 (Core): KMP String Matching**

**Topics & Subtopics:**
- ğŸ” **Problem & Naive Approach**
  - Naive: O(nm) time (character re-checking)

- ğŸ§µ **Failure Function (LPS Array)**
  - LPS[i] = longest proper prefix that is suffix of P[0..i]
  - Computation: O(n)

- ğŸ” **KMP Algorithm**
  - Linear time O(n+m)
  - Avoids character re-checking via failure function

---

### ğŸ“… **Day 2 (Core): Z-Algorithm & Advanced String Problems**

**Topics & Subtopics:**
- ğŸ§µ **Z-Function**
  - Z[i] = length of prefix starting at i
  - Linear computation: O(n)

- ğŸ§ª **Applications**
  - Pattern matching
  - Periodicity detection
  - Rotation detection

---

### ğŸ“… **Day 3 (Optional Advanced): Manacher's Algorithm**

**Topics & Subtopics:**
- ğŸ” **Longest Palindromic Substring**
  - Naive: O(nÂ²) with expand around center
  - Manacher: O(n) via symmetry

---

### ğŸ“… **Day 4 (Optional Advanced): Suffix Structures**

**Topics & Subtopics:**
- ğŸ“š **Suffix Arrays**
  - Sorted suffix indices
  - Applications: pattern matching, LCS, text indexing
  - Construction: O(n log n) practical, O(n) advanced

- ğŸŒ² **Suffix Trees**
  - Compressed trie of suffixes
  - Time: O(|pattern|) matching
  - Space: high overhead, specialized use

---

### ğŸ“… **Day 5 (Core): Network Flow & Max-Flow Min-Cut**

**Topics & Subtopics:**
- ğŸ’§ **Flow Network Model**
  - Capacities, flows, conservation constraint

- ğŸ” **Ford-Fulkerson Method**
  - Augmenting paths, residual graph
  - Time depends on augmenting path selection

- ğŸ§ª **Edmonds-Karp Algorithm**
  - BFS-based, guaranteed O(VEÂ²) convergence

- âœ‚ï¸ **Max-Flow Min-Cut Theorem**
  - Intuition and proof sketch
  - Applications: cut capacity = max flow

---

### ğŸ“… **Day 6 (Core): Matching & Assignment via Flow**

**Topics & Subtopics:**
- ğŸ‘¥ **Bipartite Matching via Max Flow**
  - Reduction: flow network from bipartite graph
  - Max matching = max flow value

- ğŸ§© **Assignment Problem**
  - Min-cost matching via min-cost max-flow
  - Time: O(VEÂ²) via successive shortest paths

---

---

## ğŸ¯ **FINAL SYNTHESIS: WEEK 15-END**

### **Algorithm Paradigms Mastered** (All 15)
- âœ… Brute Force
- âœ… Divide and Conquer
- âœ… Decrease and Conquer
- âœ… Dynamic Programming
- âœ… Greedy
- âœ… Backtracking
- âœ… Branch and Bound
- âœ… Randomized
- âœ… Approximation (implicit in real problems)
- âœ… Local Search (implicit in optimizations)
- âœ… Transform/Reduction
- âœ… Amortized Analysis
- âœ… String Matching
- âœ… Network Flow
- âœ… Bitmask DP

### **Problem Solving Skills**
- âœ… Identify problem structure and constraints
- âœ… Select appropriate paradigm
- âœ… Prove correctness (exchange arguments, induction, bounds)
- âœ… Analyze complexity (time, space, amortized)
- âœ… Optimize with heuristics, pruning, bounding
- âœ… Integrate multiple paradigms for complex problems

### **DSA Mastery Achievement**
- You understand data structures as tools for specific problems
- You understand algorithms as systematic approaches to problem classes
- You can design novel algorithms combining paradigms
- You can reason about correctness and efficiency rigorously
- You can adapt to new problems by recognizing patterns

---

**Status:** âœ… **OPTIMIZED PHASE D & E SYLLABUS v13 COMPLETE**

Ready for immediate curriculum integration and instruction.
