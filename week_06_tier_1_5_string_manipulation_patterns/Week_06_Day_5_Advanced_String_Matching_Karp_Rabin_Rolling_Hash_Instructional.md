# üìò Week 06 Day 5: Advanced String Matching ‚Äî Karp-Rabin & Rolling Hash ‚Äî Engineering Guide

**Metadata:**
- **Week:** 06 | **Day:** 5
- **Category:** String Patterns (Optional Advanced)
- **Difficulty:** üî¥ Hard / Advanced
- **Real-World Impact:** Rolling hash powers plagiarism detection systems at scale, enables fast substring matching in genomics, and optimizes search engines. Karp-Rabin algorithm reduces O(nm) naive pattern matching to O(n + m) on average.
- **Prerequisites:** Week 02 (Strings, Hashing), Week 03 (Hash tables), Week 06 Days 1-4 (All core string patterns)

---

## üéØ LEARNING OBJECTIVES

*By the end of this chapter, you will be able to:*

- üéØ **Internalize** rolling hash as incremental polynomial evaluation and how to update it in O(1) as a window slides.
- ‚öôÔ∏è **Implement** Karp-Rabin pattern matching with collision handling and probability analysis.
- ‚öñÔ∏è **Evaluate** trade-offs between Karp-Rabin, KMP, Boyer-Moore, and naive approaches for different problem sizes and distributions.
- üè≠ **Connect** rolling hash to real systems like plagiarism detection (Turnitin), DNA matching, and forensic analysis.

---

## üìñ CHAPTER 1: CONTEXT & MOTIVATION

### The Engineering Challenge

Imagine Turnitin processing 10 million student submissions yearly. Each submission is 10,000 words‚Äîroughly 50,000 characters. The reference corpus contains 100 million previously submitted papers.

A naive substring matching approach for each submission:
- Extract every substring of "interesting" length (say, 30 characters)
- Compare against every substring in the corpus
- Complexity: O(10M submissions √ó 50K chars/submission √ó 100M corpus chars) ‚Äî that's quintillions of operations.

Even with a cluster of servers, this is infeasible.

Or consider DNA sequencing. A human genome is 3 billion base pairs. Researchers need to find which known disease sequences appear in a genome. A naive search for each disease sequence against the genome is O(disease_length √ó 3 billion) per disease. With thousands of disease sequences, it's prohibitive.

Then there's **network packet inspection**. A firewall scans incoming traffic for malicious byte patterns. At gigabit speeds (125 million bytes/sec), you need to match thousands of signatures against the stream in real time. Naive matching doesn't scale.

These problems share a structure: **find a pattern in a large text at massive scale**. The key insight is that **comparing strings character-by-character is slow**. Instead, compute a **hash** of the pattern and each text window, then compare hashes. If hashes match, verify the actual strings (to handle collisions).

### The Solution: Rolling Hash with Karp-Rabin

The breakthrough: represent each substring as a **polynomial evaluated at a prime base**. As the window slides, the polynomial value updates in O(1) by:
1. Removing the contribution of the leftmost character
2. Shifting all coefficients left (multiply by base)
3. Adding the rightmost character

This reduces pattern matching from O(nm) to O(n + m) on average (with low collision probability using large primes).

> **üí° Insight:** Hash values can be updated incrementally. Rolling hash trades one-time computation (polynomial) for fast per-character updates. Karp-Rabin makes pattern matching practical at scale.

---

## üß† CHAPTER 2: BUILDING THE MENTAL MODEL

### The Core Analogy

Think of a **rolling window of digits** in a number. If you have 12345 and you want to shift to 23456:
- You remove the leftmost digit (1), subtract its value
- You shift everything left (multiply by 10)
- You add the rightmost digit (6)
- Mathematically: `(12345 - 1√ó10000) √ó 10 + 6 = 23456`

String hashing works exactly this way. Each character is a "digit" in a base-p number. Rolling updates the hash in O(1).

### üñº Visualizing Polynomial Rolling Hash

```
String: "ABCDE"
Pattern: "BC" (length 2)

Step 1: Hash the pattern
  "BC" ‚Üí B√óp + C (where p is a prime, say 31, and A=1, B=2, ..., Z=26)
  = 2√ó31 + 3 = 62 + 3 = 65

Step 2: Hash first window of text (same length as pattern)
  "AB" ‚Üí 1√ó31 + 2 = 31 + 2 = 33

Step 3: Roll the window to next position
  Remove 'A' (leftmost): hash = 33 - (1 √ó 31^(2-1))
                               = 33 - 31 = 2
  Shift left (multiply by p): hash = 2 √ó 31 = 62
  Add 'C' (rightmost): hash = 62 + 3 = 65
  
  Result: "BC" hash = 65 ‚úì MATCH!

Step 4: Continue rolling
  Current: "BC" hash = 65, window at position [1,2]
  Next window: "CD"
  
  Remove 'B' (leftmost): hash = 65 - (2 √ó 31^(2-1))
                               = 65 - 62 = 3
  Shift left: hash = 3 √ó 31 = 93
  Add 'D': hash = 93 + 4 = 97
  
  Result: "CD" hash = 97 ‚â† 65 (no match)

Step 5: Continue
  Current: "CD" hash = 97
  Next window: "DE"
  
  Remove 'C': hash = 97 - (3 √ó 31^(2-1))
                   = 97 - 93 = 4
  Shift left: hash = 4 √ó 31 = 124
  Add 'E': hash = 124 + 5 = 129
  
  Result: "DE" hash = 129 ‚â† 65 (no match)

Summary: Found pattern "BC" at position 1 in "ABCDE"
```

The key insight: each rolling update is O(1), so matching the entire text is O(n).

### Invariants & Properties

**The Rolling Hash Invariant:**

At position i:
- `hash[i] = (s[i] √ó p^(m-1) + s[i+1] √ó p^(m-2) + ... + s[i+m-1]) mod P`
- Where `m` is pattern length, `p` is base (usually prime), `P` is large modulus

The hash uniquely represents the m-character substring (with collision probability ‚âà 1/P for a good hash).

**Collision Probability:**

If two different strings have the same hash:
- Probability of false positive: O(1/P) where P is the modulus
- By choosing P large (e.g., 10^9 + 7), collision probability < 10^-9 per comparison
- Expected collisions in n comparisons: < 10^-9 √ó n (negligible)

**Correctness with Verification:**

1. Hash mismatch ‚Üí no match (guaranteed)
2. Hash match ‚Üí potential match (verify with character-by-character comparison)

This two-tier approach (hash filter + verification) is provably correct.

### üìê Mathematical & Theoretical Foundations

**Polynomial Representation:**

String S of length m over alphabet A:
```
H(S) = Œ£(i=0 to m-1) S[i] √ó p^(m-1-i) mod P
```

Where:
- S[i] is the numeric value of character at position i
- p is a prime base (often 31 or 37 for strings)
- P is a large prime modulus (often 10^9 + 7)

**Rolling Update Formula:**

```
H(S[i..i+m]) = (H(S[i-1..i+m-1]) - S[i-1] √ó p^(m-1)) √ó p + S[i+m]
              = ((H_old - S_leftmost √ó p^(m-1)) √ó p + S_rightmost) mod P
```

Time: O(1) after precomputing p^(m-1) mod P.

**Complexity Analysis:**

- Preprocessing: O(m) to compute hash of pattern and p^(m-1)
- Hashing text: O(n) rolling updates, each O(1)
- Verification on collision: O(m) character comparison
- Expected collisions: O(n/P) (negligible if P is large)
- **Total expected time: O(n + m)** (versus O(nm) naive)

### Taxonomy of String Matching Approaches

| Algorithm | Best Case | Worst Case | Avg Case | Space | Notes |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Naive** | O(n) | O(nm) | O(nm) | O(1) | Simple, slow |
| **Karp-Rabin** | O(n+m) | O(nm) with many collisions | **O(n+m)** | O(1) | Fast avg, good for multiple patterns |
| **KMP** | O(n+m) | **O(n+m)** | **O(n+m)** | O(m) | Guaranteed linear, complex |
| **Boyer-Moore** | O(n/m) best | O(nm) | **O(n/m)** avg | O(k) alphabet | Very fast in practice, long patterns |
| **Aho-Corasick** | O(n+m+z) | O(n+m+z) | O(n+m+z) | O(km) | Multiple patterns simultaneously |

Where z = number of pattern occurrences, k = alphabet size.

---

## ‚öôÔ∏è CHAPTER 3: MECHANICS & IMPLEMENTATION

### The State Machine & Memory Layout

Karp-Rabin matching state machine:

```
State:
  pattern      : immutable pattern string
  text         : immutable text string
  patternHash  : single hash value (precomputed)
  base         : prime base (e.g., 31)
  modulus      : large prime (e.g., 10^9 + 7)
  basePower    : base^(patternLen-1) mod modulus (precomputed)
  textHash     : current rolling hash of text window
  matches      : list of match positions

Preprocessing:
  1. Compute patternHash via polynomial
  2. Precompute basePower
  3. Hash first window of text

Rolling phase:
  For each new text position:
    1. Update rolling hash in O(1)
    2. Compare hashes
    3. On match, verify substring (to handle collisions)
    4. Record match position if verified

Memory: O(n + m) for text and pattern, O(1) auxiliary (hash values are single integers)
```

### üîß Operation 1: Computing Rolling Hash (Preprocessing)

**Narrative Walkthrough:**

We precompute the hash of the pattern and calculate the base power (p^(m-1) mod P). Then we hash the first window of the text. All preprocessing is O(m).

**Inline Trace (Computing Hashes):**

```
Pattern: "AB" (length 2)
Text: "AABAA"

Parameters:
  base p = 31
  modulus P = 10^9 + 7
  Character mapping: A=1, B=2, ..., Z=26

Step 1: Compute basePower
  basePower = 31^(2-1) mod P = 31^1 mod P = 31

Step 2: Compute pattern hash
  H("AB") = (1 √ó 31 + 2) mod P
          = (31 + 2) mod P
          = 33

Step 3: Compute hash of first text window "AA"
  H("AA") = (1 √ó 31 + 1) mod P
          = (31 + 1) mod P
          = 32

Step 4: Save for rolling updates
  patternHash = 33
  textHash = 32
  basePower = 31

Ready for rolling phase!
```

Step 2 is crucial: compute basePower once, reuse for all m rolling updates.

### üîß Operation 2: Rolling Hash Updates (Pattern Matching)

**Narrative Walkthrough:**

For each text position, update the rolling hash by:
1. Removing the leftmost character's contribution
2. Shifting left
3. Adding the rightmost character

Then compare with pattern hash. On match, verify the actual strings.

**Inline Trace (Full Matching):**

```
Pattern: "AB" (hash 33), basePower = 31
Text: "AABAA"

Position 0: Window "AA"
  Hash: 32 (computed in preprocessing)
  Matches pattern hash (33)? No, 32 ‚â† 33

Position 1: Window "AB"
  Remove 'A' (leftmost): hash = 32 - (1 √ó 31) = 32 - 31 = 1
  Shift left: hash = 1 √ó 31 = 31
  Add 'B' (rightmost, text[2]='B'): hash = 31 + 2 = 33
  
  Matches pattern hash? YES, 33 = 33
  Verify actual strings: text[1..2] = "AB" vs pattern = "AB"? YES!
  Record match at position 1 ‚úì

Position 2: Window "BA"
  Remove 'B' (leftmost): hash = 33 - (2 √ó 31) = 33 - 62 = -29 mod P
                               = (10^9 + 7 - 29) mod P (handle negative)
  Shift left: hash = (-29 √ó 31) mod P
  Add 'A' (rightmost, text[3]='A'): hash = (...) + 1
  
  (After calculation) hash ‚â† 33
  No match

Position 3: Window "AA"
  (Similar calculation)
  Hash ‚â† 33
  No match

Result: Pattern "AB" found at position 1 in "AABAA"
```

Notice: Each rolling update is O(1). Total time for matching: O(n + m).

### üìâ Progressive Example: Multiple Pattern Occurrences

Find pattern "AA" in text "AABAAA":

```
Pattern: "AA" (hash = 1√ó31 + 1 = 32)
Text: "AABAAA"

Position 0: Window "AA"
  Hash: 32
  Match? YES ‚úì Position 0

Position 1: Window "AB"
  Rolling: (32 - 1√ó31) √ó 31 + 2 = (32 - 31) √ó 31 + 2 = 1√ó31 + 2 = 33
  Match? NO (33 ‚â† 32)

Position 2: Window "BA"
  Rolling: (33 - 2√ó31) √ó 31 + 1 = (33 - 62) mod P √ó 31 + 1
  Hash ‚â† 32
  Match? NO

Position 3: Window "AA"
  Rolling: ... = 32
  Match? YES ‚úì Position 3

Position 4: Window "AA"
  Rolling: ... = 32
  Match? YES ‚úì Position 4

Result: Pattern "AA" found at positions 0, 3, 4 in "AABAAA"
```

The rolling hash efficiently identifies all occurrences without repeated full comparisons.

---

## ‚öñÔ∏è CHAPTER 4: PERFORMANCE, TRADE-OFFS & REAL SYSTEMS

### Beyond Big-O: Performance Reality

**Hash Collision Impact:**

With a modulus P = 10^9 + 7 and n = 10^6 rolling comparisons:
- Expected collisions: n / P ‚âà 10^6 / 10^9 ‚âà 0.001 (negligible)
- Verification on collision: O(m) per false positive
- Expected verification cost: 0.001 √ó O(m) = O(0.001m) (negligible)

So the O(n + m) average case holds in practice.

**Comparison Table:**

| Scenario | Naive | Karp-Rabin | KMP | Boyer-Moore |
| :--- | :--- | :--- | :--- | :--- |
| Single short pattern | O(nm) | **O(n+m)** | O(n+m) | O(n) best |
| Single long pattern | O(nm) | **O(n+m)** | O(n+m) | **O(n/m)** best |
| Multiple patterns (k) | O(knm) | **O(n+km)** | O(n+km) | Impractical |
| Plagiarism detection | Infeasible | **Practical** | Possible | Impractical |
| Real-time (constrained time) | No | **Yes** | Yes | Yes |

**Memory Hierarchy:** Rolling hash uses O(1) auxiliary space, making it cache-friendly. KMP uses O(m) for failure tables. Boyer-Moore uses O(k) for shift tables (k = alphabet size).

### üè≠ Real-World Systems

**Story 1: Turnitin Plagiarism Detection**

Turnitin processes 10 million submissions yearly, each 50,000 characters, against a corpus of 100 billion documents (roughly 5 trillion characters of indexed text).

Without Karp-Rabin: O(50K √ó 5T) = 2.5 √ó 10^14 character comparisons per submission. Even at 10^9 comparisons/second, that's 250,000 seconds per submission (infeasible).

With Karp-Rabin (rolling hash):
1. Extract 30-character substrings from submission (‚âà50K strings)
2. For each substring, hash-match against indexed corpus hashes (pre-computed)
3. Hash lookups: O(50K log corpus) with hash table indexing
4. On hash collision, verify full substring: O(30 characters)
5. Total per submission: O(50K √ó (log lookup + occasional 30-char verify)) ‚âà seconds

Turnitin **relies on rolling hash** to make plagiarism detection practical.

**Story 2: DNA Sequence Alignment (Bioinformatics)**

The human genome is 3 billion base pairs. Researchers use rolling hash to search for disease sequences (say, 100 bp mutations).

Naive search: O(100 bp √ó 3B bp) = 3 √ó 10^11 operations per disease sequence.

With rolling hash: O(3B + 100) ‚âà seconds per sequence.

For 10,000 disease sequences: O(10K √ó 3B) naively is infeasible; with rolling hash, it's O(10K √ó (3B + 100)) ‚âà hours (manageable with parallelization).

Genomic tools (like BLAST, which uses similar hashing) enable modern medicine.

**Story 3: Intrusion Detection in Network Firewalls**

A firewall scans incoming packets at gigabit speeds (125 million bytes/sec). It must match against 10,000 known malware signatures simultaneously.

Naive approach: For each packet (e.g., 1500 bytes), compare against each signature. That's 1500 √ó 10,000 = 15 million comparisons per packet. At 125M packets/sec, that's... prohibitive.

With Aho-Corasick (which uses rolling hash ideas) and rolling window:
- Hash each signature once
- For each incoming byte, update rolling window hash (O(1))
- Check if hash matches any signature hash (hash table lookup, O(1) avg)
- On match, verify (O(signature_len) only if hash matches)

This **reduces per-packet cost from O(1500 √ó 10000) to O(1500 √ó hash lookups + occasional verifications)**, enabling real-time packet filtering.

### Failure Modes & Robustness

**Failure Mode 1: Integer Overflow in Hash Computation**

```
WRONG:
  hash = (hash * base + charValue)  // Can overflow!
  if (hash > modulus):
    hash = hash mod modulus
  
  Problem: Intermediate hash * base can overflow before modulo

CORRECT:
  hash = ((hash * base) mod modulus + charValue) mod modulus
  // Apply modulo after each operation to keep values bounded
```

**Failure Mode 2: Negative Modulo in Rolling Update**

```
WRONG:
  hash = (hash - leftChar * basePower) * base + rightChar
  // If hash < leftChar * basePower, result is negative!

CORRECT:
  hash = ((hash - leftChar * basePower + modulus) % modulus * base + rightChar) % modulus
  // Add modulus before taking modulo to handle negative
```

**Failure Mode 3: Incorrect Base Power Precomputation**

```
WRONG:
  basePower = base ^ (patternLen - 1)  // Computed naively
  // For large patternLen, this overflows!

CORRECT:
  basePower = 1
  for i in 1...patternLen-1:
    basePower = (basePower * base) mod modulus
  // Use modular exponentiation
```

**Failure Mode 4: Not Verifying on Hash Match**

```
WRONG:
  if (hash == patternHash):
    recordMatch(position)  // Assume hash match = string match!
  
  Problem: Hash collisions exist. False positives if not verified.

CORRECT:
  if (hash == patternHash):
    if (text[position..position+patternLen] == pattern):
      recordMatch(position)  // Only record after verification
```

**Failure Mode 5: Using Non-Prime Modulus**

```
WRONG:
  modulus = 10^9 (not prime!)
  
  Problem: Non-prime moduli have worse collision properties.
  Certain patterns may hash to same value frequently.

CORRECT:
  modulus = 10^9 + 7 (prime!)
  // Prime moduli have better distribution properties
```

---

## üîó CHAPTER 5: INTEGRATION & MASTERY

### Connections (Precursors & Successors)

**Precursors:**
- Week 03: Hash tables and collision handling
- Week 06 Days 1-4: All core string patterns (foundations for pattern matching)

**Successors:**
- Week 15: KMP, Z-algorithm, other advanced string algorithms
- Week 15: Aho-Corasick (multi-pattern matching with rolling hash ideas)
- Week 15: Suffix arrays and suffix trees (advanced string data structures)
- Week 18+: Approximate string matching, edit distance with rolling hash optimization

### üß© Pattern Recognition & Decision Framework

**When to use Karp-Rabin:**

- **Single pattern, text known upfront**: KMP might be better (guaranteed O(n+m) without collisions)
- **Multiple patterns in same text**: Karp-Rabin shines (hash all patterns, single pass through text)
- **Streaming text, dynamic patterns**: Karp-Rabin (update hashes incrementally)
- **Approximate matching with character mismatches**: Karp-Rabin with threshold (e.g., Hamming distance)
- **Very long patterns, text known to be random**: Boyer-Moore might be faster in practice

**Decision Tree:**

```
Is the problem PATTERN MATCHING?

‚îú‚îÄ Yes, SINGLE pattern?
‚îÇ  ‚îú‚îÄ Very long pattern (m > 100)?
‚îÇ  ‚îÇ  ‚îî‚îÄ Use Boyer-Moore (O(n/m) avg)
‚îÇ  ‚îú‚îÄ Short pattern, many searches?
‚îÇ  ‚îÇ  ‚îî‚îÄ Use Karp-Rabin (O(n+m) avg, easy to parallelize)
‚îÇ  ‚îî‚îÄ Guaranteed linear needed?
‚îÇ     ‚îî‚îÄ Use KMP (O(n+m) worst-case)
‚îÇ
‚îú‚îÄ Yes, MULTIPLE patterns?
‚îÇ  ‚îú‚îÄ Small number (< 10)?
‚îÇ  ‚îÇ  ‚îî‚îÄ Karp-Rabin (hash each, one pass)
‚îÇ  ‚îî‚îÄ Many patterns (> 100)?
‚îÇ     ‚îî‚îÄ Use Aho-Corasick (O(n + m + z) for all)
‚îÇ
‚îî‚îÄ Yes, APPROXIMATE matching?
   ‚îî‚îÄ Karp-Rabin with Hamming threshold
```

- **‚úÖ Use when:** Multiple patterns, online/streaming matching, approximate matching, large-scale systems
- **üõë Avoid when:** Need guaranteed worst-case O(n+m) (use KMP), or pattern very long and text very short (use naive)

**üö© Red Flags (Interview Signals):**
- "Find all occurrences of pattern"
- "String matching", "substring search"
- "Multiple patterns"
- "Plagiarism", "plagiarism detection"
- "Streaming", "online"
- "Rolling", "hash"

### üß™ Socratic Reflection

1. **Why can rolling hash update in O(1) when naive hashing the new window would be O(m)?** (Hint: think about what doesn't change between adjacent windows.)

2. **What happens to hash collision probability if you choose modulus P = 10^6 instead of 10^9 + 7?** (Hint: more collisions mean more verifications; what's the trade-off?)

3. **In Aho-Corasick, how would you extend rolling hash to match multiple patterns simultaneously?** (Hint: think about hashing all patterns and looking up in a hash table.)

### üìå Retention Hook

> **The Essence:** "Rolling hash updates in O(1) by removing the past, shifting, and adding the new. Karp-Rabin makes pattern matching fast on average. Hash matches filter candidates; verify matches to handle collisions. Use for multiple patterns or streaming‚ÄîKMP is better for guaranteed worst-case."

---

## üß† 5 COGNITIVE LENSES

### 1. üíª The Hardware Lens (Cache, CPU, Memory)

Rolling hash is **cache-optimal**: each update touches O(1) memory (just a few integers). Compare to KMP, which must access failure tables (potential cache misses). For modern CPUs with deep pipelines, rolling hash's simplicity means fewer branch mispredictions.

### 2. üìâ The Trade-off Lens (Time vs Space, Simplicity vs Guarantees)

| Algorithm | Time Guarantee | Space | Simplicity | Practice |
| :--- | :--- | :--- | :--- | :--- |
| Naive | O(nm) | O(1) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Worst |
| Karp-Rabin | O(n+m) avg | O(1) | ‚≠ê‚≠ê‚≠ê‚≠ê | Best avg |
| KMP | **O(n+m)** worst | O(m) | ‚≠ê‚≠ê‚≠ê | Guaranteed |
| Boyer-Moore | O(n/m) avg | O(k) | ‚≠ê‚≠ê | Best long patterns |

Karp-Rabin is the sweet spot for most real systems: good average case, simple code, low space.

### 3. üë∂ The Learning Lens (Misconceptions, Psychology)

**Misconception 1:** "Hash collisions will ruin Karp-Rabin's performance."

**Reality:** With a large prime modulus (10^9 + 7), collisions are rare (< 0.001%). Verification on collision is O(m), but expected cost is O(0.001m) ‚Äî negligible.

**Misconception 2:** "Rolling hash is only for substring search."

**Reality:** Rolling hash principles extend to plagiarism detection, DNA matching, anomaly detection, and anywhere you need fast similarity checking at scale.

**Misconception 3:** "KMP is always better because it has O(n+m) worst-case guarantee."

**Reality:** KMP has worse constants. Karp-Rabin is simpler, faster in practice for most inputs, and better for multiple patterns.

### 4. ü§ñ The AI/ML Lens (Analogies to Neural Networks)

Rolling hash is analogous to **convolutional filters in neural networks**. A filter slides over an input, computing a dot product at each position. Similarly, rolling hash slides over text, computing a hash value at each position. Both achieve efficiency through **parameter reuse** (the filter weights / hash base stays the same) and **incremental updates** (convolution can be computed incrementally with sliding window).

### 5. üìú The Historical Lens (Origins, Inventors)

**Karp-Rabin Algorithm:** Published by Michael Rabin and Richard Karp in 1987. Breakthrough because it showed **probabilistic algorithms** could match or beat deterministic algorithms for string matching.

Before Karp-Rabin, researchers believed O(n+m) was only achievable with complex constructions (like KMP). Karp-Rabin's elegance‚Äîusing hashing probabilistically‚Äîsparked a revolution in randomized algorithms.

Modern applications (plagiarism detection, DNA matching) rely heavily on Karp-Rabin variants, making it one of the most practically useful algorithms ever published.

---

## ‚öîÔ∏è SUPPLEMENTARY OUTCOMES

### üèãÔ∏è Practice Problems (6-8)

| Problem | Source | Difficulty | Key Concept |
| :--- | :--- | :--- | :--- |
| Repeated DNA Sequences | LeetCode #187 | üü° Medium | Rolling hash with modulus |
| Find All Anagrams in String | LeetCode #438 | üü° Medium | Fixed-window rolling hash |
| Repeated String Match | LeetCode #686 | üü° Medium | Pattern repetition, hash |
| Shortest Palindrome | LeetCode #214 | üî¥ Hard | Karp-Rabin for palindrome |
| Substring with Concatenation | LeetCode #30 | üî¥ Hard | Multi-pattern hash matching |
| License Key Formatting | LeetCode #482 | üü¢ Easy | Character transformation |
| Implement strstr() | LeetCode #28 | üü¢ Easy | Basic pattern matching |

### üéôÔ∏è Interview Questions (5+)

1. **Q:** Implement the Karp-Rabin algorithm for pattern matching.
   - **Follow-up:** How do you handle hash collisions?
   - **Follow-up:** What modulus would you use and why?

2. **Q:** Find all anagrams of a pattern in a text.
   - **Follow-up:** How is this different from exact pattern matching?
   - **Follow-up:** Can you do it in O(n) time?

3. **Q:** Detect plagiarism using rolling hash. Design the system.
   - **Follow-up:** How do you handle false positives?
   - **Follow-up:** What about documents in different languages or with formatting?

4. **Q:** Compare Karp-Rabin, KMP, and Boyer-Moore. When would you use each?
   - **Follow-up:** What about approximate matching (Hamming distance)?
   - **Follow-up:** Multiple patterns simultaneously?

5. **Q:** Implement Aho-Corasick using rolling hash ideas for multi-pattern matching.
   - **Follow-up:** How does it differ from running Karp-Rabin multiple times?
   - **Follow-up:** What's the complexity for k patterns and n text length?

### ‚ùå Common Misconceptions (3-5)

- **Myth:** "Karp-Rabin is slower than naive because of hash computation overhead."
  - **Reality:** Hash computation is O(1) after preprocessing. Naive is O(nm). Karp-Rabin is O(n+m) on average‚Äîmuch faster.

- **Myth:** "Hash collisions are a critical problem for Karp-Rabin."
  - **Reality:** With large prime moduli, collisions are rare (< 0.001%). Even with collisions, verify-on-collision handles them correctly.

- **Myth:** "Karp-Rabin requires complex modular arithmetic."
  - **Reality:** Modern implementations use simple modulo operations. No fancy number theory needed beyond choosing a prime.

- **Myth:** "You need KMP for guaranteed O(n+m) performance."
  - **Reality:** Karp-Rabin guarantees O(n+m) expected time with high probability. For practical purposes, this is "good enough."

- **Myth:** "Rolling hash only works for exact string matching."
  - **Reality:** Variations (with Hamming distance, edit distance thresholds) enable approximate matching.

### üöÄ Advanced Concepts (4-5)

1. **Rabin's Fingerprinting (Polynomial Rolling Hash):** Full polynomial representation with multiple moduli for even better collision resistance.

2. **Multiple Moduli for Reduced Collisions:** Use two independent large primes; collision probability becomes product of individual probabilities.

3. **Karp-Rabin with Edit Distance:** Approximate matching by checking Hamming distance threshold (number of mismatches allowed).

4. **Suffix Hashing:** Hash all suffixes of a string for comparison, enabling longest common substring in O(n log n).

5. **Aho-Corasick Trie Integration:** Combine rolling hash with state machines for simultaneous multi-pattern matching.

### üìö External Resources

- **"Introduction to Algorithms" (CLRS):** Chapter on string matching (KMP, Karp-Rabin comparison).
- **"Algorithms" by Sedgewick & Wayne:** Excellent visual explanation of rolling hash.
- **MIT 6.006 Lecture Notes:** On string matching and rolling hash (MIT OpenCourseWare).
- **Turnitin Patent 6757841:** Describes plagiarism detection system using rolling hash.
- **BLAST Documentation:** DNA sequence matching using hash-based approaches.

---

## üìä Summary Table: String Matching Algorithms Comparison

| Algorithm | Time Best | Time Worst | Time Avg | Space | Preprocessing | When to Use |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Naive | O(n) | O(nm) | O(nm) | O(1) | O(1) | Tiny strings |
| **Karp-Rabin** | **O(n+m)** | O(nm) collisions | **O(n+m)** | **O(1)** | **O(m)** | **Multiple patterns** |
| KMP | O(n+m) | O(n+m) | O(n+m) | O(m) | O(m) | Worst-case guarantee |
| Boyer-Moore | **O(n/m)** | O(nm) | **O(n/m)** | O(k) | O(k) | Long patterns |
| Aho-Corasick | O(n+m+z) | O(n+m+z) | O(n+m+z) | O(km) | O(km) | Many patterns |

Where z = number of matches, k = alphabet size.

---

## üèÅ Conclusion: From Theory to Mastery

You've journeyed from understanding rolling hash as incremental polynomial evaluation to recognizing Karp-Rabin as a practical, elegant solution for pattern matching at scale. The key principles:

1. **Hash updates are O(1):** Exploit the structure of adjacent windows.
2. **Collisions are rare but handle them:** Verify on hash match.
3. **Karp-Rabin shines for multiple patterns:** Hash all patterns once, single pass through text.
4. **Trade-offs matter:** Karp-Rabin vs KMP vs Boyer-Moore depend on problem characteristics.

When you encounter a pattern matching problem at scale‚Äîplagiarism detection, DNA analysis, intrusion detection, or streaming data‚Äîpause. Ask:

- **How many patterns?** (Single: consider Boyer-Moore. Multiple: use Karp-Rabin.)
- **Is the text streaming or known upfront?** (Streaming: Karp-Rabin. Known: Boyer-Moore might win.)
- **Do you need worst-case guarantees?** (Yes: KMP. No: Karp-Rabin is simpler.)

These questions will guide you to the right tool.

---

**Generated:** January 10, 2026 | **Version:** 1.0 Production Ready | **Status:** ‚úÖ Ready for Deployment