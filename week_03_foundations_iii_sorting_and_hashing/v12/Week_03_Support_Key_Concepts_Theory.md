# ðŸ“˜ Week 03 Support: Key Concepts & Theory Reference

**Week:** 3 | **Category:** Foundations / Theoretical Underpinnings  
**Purpose:** Deep conceptual understanding of sorting, heaps, and hashing  
**Audience:** Engineers seeking theoretical mastery before implementation

---

## ðŸ“Š PART I: FOUNDATIONAL CONCEPTS

### 1. Comparison-Based Sorting Lower Bound

**Theorem:** Any comparison-based sorting algorithm requires Î©(n log n) comparisons in the worst case.

**Proof Sketch:**
- There are n! possible permutations of n elements
- Each comparison answers one binary question (< or â‰¥)
- To distinguish all n! permutations, need logâ‚‚(n!) = Î˜(n log n) comparisons
- Therefore, no comparison algorithm can do better than O(n log n)

**Implications:**
- Merge sort and heap sort achieve this lower bound â†’ **optimal**
- Quick sort achieves it on average but not worst-case
- Bubble/selection/insertion sorts are O(nÂ²) â†’ **suboptimal**

**Non-Comparative Sorting Exceptions:**
- Counting sort: O(n + k) where k = key range
- Radix sort: O(d(n + 10)) where d = number of digits
- These avoid lower bound by exploiting problem structure (integers, bounded range)

---

### 2. The Recurrence Relation Master Theorem

**Form:** T(n) = aÂ·T(n/b) + f(n)

**Solutions (Simplified):**
- If f(n) = O(n^(log_b(a) - Îµ)) â†’ T(n) = Î˜(n^(log_b(a)))
- If f(n) = Î˜(n^(log_b(a))) â†’ T(n) = Î˜(n^(log_b(a)) Â· log n)
- If f(n) = Î©(n^(log_b(a) + Îµ)) and aÂ·f(n/b) â‰¤ cÂ·f(n) â†’ T(n) = Î˜(f(n))

**Examples:**
```
Merge sort: T(n) = 2Â·T(n/2) + O(n)
  a=2, b=2, f(n)=O(n)
  log_b(a) = log_2(2) = 1
  f(n) = O(n^1) = Î˜(n^(log_b(a)))
  â†’ T(n) = Î˜(n log n) âœ“

Quick sort: T(n) = 2Â·T(n/2) + O(n)  [average case]
  Same as merge sort
  â†’ T(n) = Î˜(n log n) average âœ“

Binary search: T(n) = 1Â·T(n/2) + O(1)
  a=1, b=2, f(n)=O(1)
  log_b(a) = 0
  f(n) = O(1) = Î˜(n^0)
  â†’ T(n) = Î˜(log n) âœ“
```

---

### 3. Heap Property and Completeness

**Heap Properties:**
- **Heap Property:** Parent â‰¥ all children (max-heap) or Parent â‰¤ all children (min-heap)
- **Complete Binary Tree:** All levels filled except possibly last; last level filled left-to-right
- **Height:** Always logâ‚‚(n), ensuring O(log n) operations

**Why Completeness Matters:**
- Guarantees tree is **balanced** (no skewed subtrees)
- Enables **array representation** (index arithmetic works)
- Ensures **O(log n) height** (not O(n) like unbalanced BSTs)

**Array Index Arithmetic (0-indexed):**
```
Parent of i:    (i - 1) / 2
Left child:     2i + 1
Right child:    2i + 2
```

---

### 4. Load Factor and Amortized Analysis

**Load Factor:** Î± = n / m (keys in table / bucket capacity)

**Expected Chain Length:** Î±  
**Expected Search Time:** O(1 + Î±)

**Amortized Analysis (Intuitive):**
- Insert n items; some trigger resize at Î± = 0.75
- Resize cost: O(n) to rehash
- Total cost: O(n) pure inserts + O(n) resizing = O(n)
- **Amortized per insertion:** O(n) / n = O(1)

**Why Resize?**
- Without resize: Î± grows unbounded â†’ search becomes O(n)
- With resize: Î± stays constant â‰ˆ 0.5 â†’ search stays O(1)

---

### 5. Hash Function Requirements

**Requirement 1: Determinism**
- Same key always produces same hash
- Enables correctness (find inserted keys)

**Requirement 2: Uniformity**
- Keys spread evenly across m buckets
- Expected chain length = Î± (not concentrated)

**Requirement 3: Speed**
- Computed in O(1) time
- Not O(n) per computation

**Requirement 4: Avalanche Effect**
- Small input change â†’ large output change
- Prevents adversarial patterns (all keys hash to same bucket)

**Bad Hash Function (violates uniformity):**
```csharp
// Hash only depends on first character
int BadHash(string key) => key[0] % 26;
// All strings starting with 'a' hash to same bucket
// Violates uniformity â†’ chains become O(n)
```

**Good Hash Function (FNV-1a):**
```csharp
// Depends on all characters, good distribution
int FnvHash(string key) {
    int hash = 2166136261;
    foreach (char c in key) {
        hash ^= c;
        hash *= 16777619;
    }
    return Math.Abs(hash);
}
```

---

## ðŸ“Š PART II: COMPLEXITY TABLES & COMPARISONS

### Sorting Algorithms Comparison Table

| Algorithm | Best | Average | Worst | Space | Stable | In-Place |
|-----------|------|---------|-------|-------|--------|----------|
| **Bubble** | O(n) | O(nÂ²) | O(nÂ²) | O(1) | Yes | Yes |
| **Selection** | O(nÂ²) | O(nÂ²) | O(nÂ²) | O(1) | No | Yes |
| **Insertion** | O(n) | O(nÂ²) | O(nÂ²) | O(1) | Yes | Yes |
| **Merge** | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes | No |
| **Quick** | O(n log n) | O(n log n) | O(nÂ²) | O(log n) | No | Yes |
| **Heap** | O(n log n) | O(n log n) | O(n log n) | O(1) | No | Yes |
| **Counting** | O(n+k) | O(n+k) | O(n+k) | O(k) | Yes | No |
| **Radix** | O(d(n+10)) | O(d(n+10)) | O(d(n+10)) | O(n+10) | Yes | No |

**Key Insights:**
- Elementary sorts: O(nÂ²), small constants, adaptive (bubble on sorted data)
- Divide-and-conquer: O(n log n) guaranteed
- Non-comparative: O(n+k) or O(dÂ·n) if domain is restricted

---

### Data Structure Operations Comparison

| Operation | Array | Linked List | Heap | BST | Hash Table |
|-----------|-------|-------------|------|-----|------------|
| **Search** | O(log n)* | O(n) | O(n) | O(log n)* | O(1)* |
| **Insert** | O(n) | O(1)** | O(log n) | O(log n)* | O(1)* |
| **Delete** | O(n) | O(1)** | O(log n) | O(log n)* | O(1)* |
| **Find Min** | O(n) | O(n) | O(1) | O(log n) | O(n) |
| **Successor** | N/A | O(n) | N/A | O(log n) | N/A |

*\*Requires sorted array for array search; average-case for BST/hash*  
*\*\*After finding position*

---

### Heap vs Priority Queue Implementations

| Operation | Array | Linked List | Heap | Fibonacci Heap |
|-----------|-------|-------------|------|---|
| **Insert** | O(1) | O(1) | O(log n) | O(1) |
| **Find-Min** | O(1) | O(1) | O(1) | O(1) |
| **Extract-Min** | O(n) | O(1) | O(log n) | O(log n) amortized |
| **Decrease-Key** | O(1) | O(1) | O(log n) | O(1) amortized |
| **Delete** | O(n) | O(1) | O(log n) | O(log n) amortized |
| **Merge** | O(n+m) | O(1) | O(n+m) | O(1) |

**When to use:**
- **Array:** Simple, small n
- **Linked list:** Only extract-min matters
- **Heap:** General priority queue, good balance
- **Fibonacci Heap:** Dijkstra's algorithm (fewer decrease-key calls)

---

### Hash Table Collision Resolution

| Strategy | Chaining | Linear Probing | Quadratic | Double Hash |
|----------|----------|---|---|---|
| **Average Lookup** | O(1+Î±) | O(1/(1-Î±)) | O(1/(1-Î±)) | O(1/(1-Î±)) |
| **Clustering** | None | Primary | Secondary | None |
| **Max Î±** | > 1 | â‰¤ 0.75 | â‰¤ 0.75 | â‰¤ 0.75 |
| **Cache-Friendly** | Poor | Excellent | Excellent | Good |
| **Implementation** | Simple | Medium | Medium | Complex |

---

## ðŸ“Š PART III: BIG-O CHEAT SHEET FOR WEEK 3

### Sorting Algorithms

```
Elementary Sorts (O(nÂ²))
â”œâ”€ Bubble Sort:     O(nÂ²) worst, O(n) best, O(1) space
â”œâ”€ Selection Sort:  O(nÂ²) always, O(1) space, not stable
â””â”€ Insertion Sort:  O(nÂ²) worst, O(n) best, O(1) space, stable

Divide-and-Conquer (O(n log n))
â”œâ”€ Merge Sort:      O(n log n) always, O(n) space, stable
â”œâ”€ Quick Sort:      O(n log n) avg, O(nÂ²) worst, O(log n) space
â””â”€ Heap Sort:       O(n log n) always, O(1) space, not stable

Non-Comparative
â”œâ”€ Counting Sort:   O(n + k) where k = key range
â”œâ”€ Radix Sort:      O(d(n + 10)) where d = digit count
â””â”€ Bucket Sort:     O(n + k) average (depends on bucket distribution)
```

### Heaps

```
Single Heap Operations
â”œâ”€ Insert:          O(log n) bubble-up
â”œâ”€ Extract-Min/Max: O(log n) bubble-down
â”œâ”€ Build-Heap:      O(n) bottom-up heapify
â”œâ”€ Heap Sort:       O(n log n)
â””â”€ Heap Property:   Parent â‰¥ children (max-heap)

Priority Queue (via Heap)
â”œâ”€ Insert:          O(log n)
â”œâ”€ Extract-Min:     O(log n)
â””â”€ Decrease-Key:    O(log n)
```

### Hash Tables

```
Hash Table (Separate Chaining)
â”œâ”€ Insert:          O(1) average, O(n) worst
â”œâ”€ Search:          O(1 + Î±) average
â”œâ”€ Delete:          O(1 + Î±) average
â”œâ”€ Resize:          O(n)
â””â”€ Load Factor:     Î± = n/m, keep Î± < 0.75

Hash Table (Open Addressing)
â”œâ”€ Insert:          O(1/(1-Î±)) average
â”œâ”€ Search:          O(1/(1-Î±)) average
â”œâ”€ Delete:          O(1/(1-Î±)) average (with tombstones)
â””â”€ Clustering:      Primary (linear), Secondary (quadratic)

Karp-Rabin Rolling Hash
â”œâ”€ Preprocess:      O(m) for pattern
â”œâ”€ Scan Text:       O(n) with rolling hash
â”œâ”€ Total:           O(n + m) expected
â””â”€ Worst-Case:      O(nm) if all positions match
```

---

## ðŸ“Š PART IV: DECISION TREES

### Which Sorting Algorithm?

```
START: Choosing a sorting algorithm
  â”‚
  â”œâ”€ "I need guaranteed O(n log n) worst-case"
  â”‚  â””â”€ Use MERGE SORT (stable, predictable)
  â”‚
  â”œâ”€ "I want fast average-case and in-place"
  â”‚  â””â”€ Use QUICK SORT (with good pivot selection)
  â”‚
  â”œâ”€ "Data is nearly sorted"
  â”‚  â””â”€ Use INSERTION SORT (O(n) on sorted data)
  â”‚
  â”œâ”€ "Keys are integers in range [0, k]"
  â”‚  â””â”€ k < n log n? â†’ COUNTING SORT
  â”‚     â””â”€ k large? â†’ RADIX SORT or QUICK SORT
  â”‚
  â”œâ”€ "I need stable sort"
  â”‚  â””â”€ Use MERGE SORT or ensure stability wrapper
  â”‚
  â”œâ”€ "Data > RAM (external sort)"
  â”‚  â””â”€ Use EXTERNAL MERGE SORT
  â”‚
  â””â”€ "I need simple, educational implementation"
     â””â”€ Use BUBBLE or INSERTION SORT
```

### Which Hash Table Strategy?

```
START: Choosing collision resolution
  â”‚
  â”œâ”€ "I'm writing an interpreter (Python, Java, Ruby)"
  â”‚  â””â”€ Use SEPARATE CHAINING (simple, flexible)
  â”‚
  â”œâ”€ "I need maximum performance (C++, systems code)"
  â”‚  â””â”€ Use LINEAR PROBING (cache-friendly)
  â”‚
  â”œâ”€ "I need theoretical guarantees"
  â”‚  â””â”€ Use DOUBLE HASHING (minimal clustering)
  â”‚
  â”œâ”€ "I'm concerned about hash flooding"
  â”‚  â””â”€ Use UNIVERSAL HASHING (randomized seed)
  â”‚
  â”œâ”€ "I have sparse data (many empty buckets)"
  â”‚  â””â”€ Use SEPARATE CHAINING (less memory)
  â”‚
  â””â”€ "I need fastest average-case lookup"
     â””â”€ Use OPEN ADDRESSING (load factor â‰¤ 0.75)
```

### When to Use Each Data Structure for Priority Queues?

```
START: Choosing priority queue implementation
  â”‚
  â”œâ”€ "General-purpose priority queue"
  â”‚  â””â”€ Use BINARY HEAP (O(log n) insert/extract)
  â”‚
  â”œâ”€ "Many decrease-key operations (Dijkstra)"
  â”‚  â””â”€ Use FIBONACCI HEAP (O(1) decrease-key amortized)
  â”‚
  â”œâ”€ "Need to merge two heaps"
  â”‚  â””â”€ Use BINOMIAL HEAP (O(log n) merge)
  â”‚
  â”œâ”€ "Only need to extract min, never insert"
  â”‚  â””â”€ Use UNSORTED ARRAY (O(1) insert, O(n) extract)
  â”‚
  â””â”€ "Simple implementation priority"
     â””â”€ Use SORTED ARRAY (O(1) extract, O(n) insert)
```

---

## ðŸ“Š PART V: STABILITY & IN-PLACE DEFINITIONS

### Stability in Sorting

**Definition:** A sort is stable if equal elements retain their original relative order.

**Example:**
```
Original:  [(3, "apple"), (1, "banana"), (3, "cherry"), (2, "date")]
           (sorted by first element)

Stable Sort Result:
  [(1, "banana"), (2, "date"), (3, "apple"), (3, "cherry")]
   Notice: "apple" before "cherry" (original order preserved for equal keys)

Unstable Sort Result:
  [(1, "banana"), (2, "date"), (3, "cherry"), (3, "apple")]
   Notice: "cherry" before "apple" (original order NOT preserved)
```

**Why It Matters:**
- Multi-level sorting (first by priority, then by timestamp)
- Database queries (ORDER BY multiple columns)
- Visualization (maintain user-expected order for ties)

**Stable Algorithms:** Bubble, insertion, merge, radix, counting  
**Unstable Algorithms:** Selection, quick sort (standard), heap sort

### In-Place Sorting

**Definition:** A sort is in-place if it uses O(log n) or less extra space (recursive stack doesn't count).

**Examples:**
```
In-Place (O(1) or O(log n) space):
â”œâ”€ Bubble:    O(1)
â”œâ”€ Selection: O(1)
â”œâ”€ Insertion: O(1)
â”œâ”€ Quick:     O(log n) recursion stack
â””â”€ Heap:      O(1)

Not In-Place (O(n) extra space):
â”œâ”€ Merge:     O(n) temp arrays
â”œâ”€ Counting:  O(k) count array
â””â”€ Radix:     O(n + 10) buckets
```

**Why It Matters:**
- Memory constraints (embedded systems, large datasets)
- Cache efficiency (in-place avoids allocation overhead)
- Theoretical elegance (sorts with minimal extra resources)

---

**Document Status:** âœ… COMPLETE  
**Total Sections:** 5 major parts with 15 subsections  
**Visual Reference:** Comprehensive for Week 03 theory

