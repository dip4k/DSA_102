# ğŸŸ§ WEEK 12: GREEDY ALGORITHMS & PROOFS

**Duration:** 5 days | 450 minutes | Algorithm Paradigm Mastery  
**Focus:** Greedy algorithm design, correctness proofs, and when greedy is optimal  
**Prerequisites:** Basic recursion, sorting, algorithm analysis  

---

## ğŸ¯ WEEK 12 LEARNING OUTCOMES

By end of Week 12, you will be able to:

1. **Design greedy algorithms** using template and best practices
2. **Prove greedy correctness** using exchange arguments and induction
3. **Solve interval problems** using greedy selection criteria
4. **Implement Huffman coding** for optimal prefix trees
5. **Analyze trade-offs** between greedy and dynamic programming approaches
6. **Recognize patterns** where greedy is provably optimal

---

---

## ğŸ“… DAY 1: GREEDY FUNDAMENTALS

### â±ï¸ Duration: 90 minutes

---

## ğŸ“ CONCEPT 1: What is Greedy?

### The Core Idea

**Greedy Algorithm:** At each step of solving a problem, make the choice that looks best **at that moment**, without reconsidering earlier choices.

Think of this real-world analogy:

```
Making Change Problem:
Goal: Give change of $2.41 using minimum coins

Coins Available: $1, 25Â¢, 10Â¢, 5Â¢, 1Â¢

Greedy Approach:
- Take as many $1 coins as possible â†’ 2 coins ($2.00)
- Remaining: $0.41
- Take as many 25Â¢ coins as possible â†’ 1 coin ($0.25)
- Remaining: $0.16
- Take as many 10Â¢ coins as possible â†’ 1 coin ($0.10)
- Remaining: $0.06
- Take as many 5Â¢ coins as possible â†’ 1 coin ($0.05)
- Remaining: $0.01
- Take 1Â¢ coins â†’ 1 coin ($0.01)

Total: 6 coins
Result: OPTIMAL âœ“
```

However, greedy doesn't always work!

```
Counter-Example:
Coins: $1, 6Â¢, 4Â¢, 1Â¢
Make: $0.12

Greedy approach:
- Take 6Â¢ â†’ 2 coins ($0.12) â†’ DONE
- Total: 2 coins

But there's also:
- Take 4Â¢ â†’ 3 coins (4Â¢ + 4Â¢ + 4Â¢)

Now try to make $0.13:
Greedy: 6Â¢ + 6Â¢ + 1Â¢ = $0.13 â†’ 3 coins
Optimal: 4Â¢ + 4Â¢ + 4Â¢ + 1Â¢ = $0.13 â†’ 4 coins

Greedy failed! âœ—
```

### Why Greedy is Appealing

âœ… **Simple to implement**  
âœ… **Fast execution** (often O(n) or O(n log n))  
âœ… **Intuitive thinking**  
âœ… **When correct, very elegant**  

### Why Greedy is Dangerous

âŒ **Can produce suboptimal solutions**  
âŒ **Looks correct but isn't**  
âŒ **Requires careful proof for each problem**  
âŒ **Easy to get wrong in interviews**  

---

## ğŸ“ CONCEPT 2: When Does Greedy Work?

### Necessary Conditions

For greedy to be guaranteed optimal, a problem must satisfy **TWO KEY PROPERTIES**:

#### 1ï¸âƒ£ **Optimal Substructure**

An optimal solution to the problem contains optimal solutions to subproblems.

```
Definition:
If OPT(problem) uses greedy choice, then
OPT(subproblem) must also be optimal.

Example (Activity Selection):
If greedy picks activity A, and optimal solution includes A,
then the remaining activities must form an optimal solution 
for the remaining time slots.

Visual:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Overall Optimal Solution         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Greedy Choice (Activity) â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Subproblem       â”‚ â† Must be Optimal
â”‚  â”‚  â”‚ (Remaining Time) â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2ï¸âƒ£ **Greedy Choice Property**

A globally optimal solution can always be arrived at by making a **locally optimal (greedy) choice**.

```
Definition:
There exists a greedy choice such that:
- The choice is feasible (satisfies constraints)
- Making it leads to a subproblem of the same structure
- Combining greedy choice + optimal subproblem = optimal solution

Example (Activity Selection):
Greedy Choice: Pick activity with earliest finish time
Why? Because it leaves the maximum time for remaining activities

Timeline:
â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼
0                          Time                      END

Activity A: â”œâ”€â”€â”€â”€â”€â”¤ (Finishes early)
Activity B:   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ (Finishes late)

Greedy picks A (finishes earliest)
Remaining time: Maximum! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Now apply greedy recursively...
```

### Visual Summary

```
GREEDY WORKS IF:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OPTIMAL SUBSTRUCTURE                   â”‚
â”‚  (Solution contains optimal subsolutions)â”‚
â”‚              âˆ§                          â”‚
â”‚              â”‚                          â”‚
â”‚              â”‚ Both must                â”‚
â”‚              â”‚ be true!                 â”‚
â”‚              â”‚                          â”‚
â”‚              âˆ¨                          â”‚
â”‚  GREEDY CHOICE PROPERTY                 â”‚
â”‚  (Greedy choice always safe)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

If one is missing â†’ Greedy may fail
```

---

## ğŸ“ CONCEPT 3: Greedy Algorithm Template

### The Standard Pattern

```
GREEDY ALGORITHM TEMPLATE:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CHOOSE A SORTING CRITERION           â”‚
â”‚     (How to compare choices)             â”‚
â”‚                                          â”‚
â”‚  2. SORT the input by criterion          â”‚
â”‚                                          â”‚
â”‚  3. ITERATE through sorted input:        â”‚
â”‚     IF choice is feasible:               â”‚
â”‚        SELECT it (commit forever)        â”‚
â”‚     ELSE:                                â”‚
â”‚        SKIP it                           â”‚
â”‚                                          â”‚
â”‚  4. RETURN accumulated selections        â”‚
â”‚                                          â”‚
â”‚  5. PROVE correctness of criterion       â”‚
â”‚     (Most important part!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Generic Algorithm Structure

```
Algorithm Steps:
1. Determine best sorting criterion
2. Sort input by criterion
3. Initialize solution = empty set
4. For each item in sorted order:
     If item is feasible:
         Add item to solution
5. Return solution

Key Insight:
The sorting criterion IS THE ALGORITHM
Choose wisely! It determines everything.
```

---

## ğŸ“ CONCEPT 4: Exchange Argument (Correctness Proof)

### The Key Proof Technique

**Exchange Argument:** Prove greedy is optimal by showing that any optimal solution can be transformed into the greedy solution without losing optimality.

### How It Works

```
EXCHANGE ARGUMENT TECHNIQUE:

Step 1: Assume there exists an optimal solution OPT
Step 2: If OPT â‰  Greedy solution:
        - Find first position where they differ
        - OPT chose X, Greedy chose G
Step 3: Exchange X â†” G in OPT
Step 4: Prove the solution is still feasible
Step 5: Prove the solution is still at least as good
Step 6: Result: OPT' is still optimal but closer to Greedy
Step 7: Repeat until OPT = Greedy

Conclusion: Greedy = Optimal
```

### Visual Example: Activity Selection

```
Given Activities with Start-End Times:
Aâ‚: [1, 4]
Aâ‚‚: [3, 5]
Aâ‚ƒ: [5, 7]
Aâ‚„: [6, 9]

Greedy Solution (sort by finish time):
Pick Aâ‚ [1,4] âœ“
Pick Aâ‚ƒ [5,7] âœ“ (earliest finish after Aâ‚)
Total: 2 activities {Aâ‚, Aâ‚ƒ}

Suppose OPT = {Aâ‚‚, Aâ‚ƒ} (also 2 activities)

Exchange Argument:
OPT: [Aâ‚‚[3,5], Aâ‚ƒ[5,7]]
Greedy: [Aâ‚[1,4], Aâ‚ƒ[5,7]]

Position 1: OPT chose Aâ‚‚, Greedy chose Aâ‚

Exchange Aâ‚‚ â†” Aâ‚:
- Aâ‚ finishes at 4 (before Aâ‚‚ finishes at 5) âœ“
- Aâ‚ doesn't conflict with Aâ‚ƒ âœ“
- Remaining problem is identical

Result: OPT' = {Aâ‚, Aâ‚ƒ} = Greedy solution

Since OPT was optimal and OPT' is equally good:
Greedy solution is optimal!
```

### Why Exchange Argument is Powerful

```
It proves:
âœ“ Greedy choice is always safe
âœ“ Greedy doesn't prevent optimal subproblems
âœ“ Iterating greedy recovers optimal solution
âœ“ No need to consider alternative choices
```

---

## ğŸ“ CONCEPT 5: Proof by Induction

### Alternative to Exchange Argument

```
INDUCTION PROOF STRATEGY:

Base Case:
  n = 1 (single element)
  Greedy trivially optimal

Inductive Step:
  Assume: Greedy is optimal for n-1 elements
  Prove: Greedy is optimal for n elements
  
  Method:
  1. Greedy chooses element gâ‚
  2. Remove gâ‚, leaving n-1 elements
  3. By induction, greedy is optimal on remaining
  4. Since gâ‚ was best choice, combined solution is optimal
  5. Therefore greedy is optimal for n elements

Conclusion:
  By induction, greedy is optimal for all n
```

### Why Both Work

```
Exchange Argument:        Induction:
â†“                         â†“
Shows optimal can be      Shows optimal
transformed into greedy   must be greedy

Both valid, choose based on problem structure
```

---

## ğŸ“Š Visual: Greedy Decision Tree

```
At each step, greedy picks THE BEST option:

Level 1:        â”Œâ”€ Choice 1 (BEST)
                â”œâ”€ Choice 2
                â”œâ”€ Choice 3
                â””â”€ Choice 4

Level 2:        â”Œâ”€ Choice 1.1 (BEST)
                â””â”€ Choice 1.2

Level 3:        â””â”€ Choice 1.1.1 (BEST)

Result: Path down BEST choices at each level
        (Not necessarily globally optimal)

However, IF problem satisfies greedy choice property:
Result: Path IS globally optimal!
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 1

| Concept | Key Point |
|---------|-----------|
| **Greedy Essence** | Make locally best choice, commit forever |
| **When It Works** | Must have optimal substructure + greedy choice property |
| **Must Prove** | Cannot assume greedy is optimal - must prove for each problem |
| **Exchange Argument** | Shows any optimal can be transformed to greedy |
| **Induction Proof** | Shows greedy must be optimal by cases |
| **Template** | Sort by criterion â†’ iterate â†’ select greedily â†’ return |

---

---

## ğŸ“… DAY 2: ACTIVITY SELECTION & INTERVAL PROBLEMS

### â±ï¸ Duration: 90 minutes

---

## ğŸ“ CONCEPT 1: The Activity Selection Problem

### Problem Statement

```
ACTIVITY SELECTION PROBLEM:

Input:
  - n activities
  - Each activity i has:
    - Start time: sáµ¢
    - End time: fáµ¢ (finish time)
  - Constraint: Must use same resource (classroom, room, etc.)

Output:
  - Select MAXIMUM NUMBER of non-overlapping activities
  - Activities don't overlap if: fáµ¢ â‰¤ sâ±¼ (one ends before other starts)

Question:
  Which activities should we select?
```

### Example

```
Activities:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Act ID â”‚ Start â”‚ Finishâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1    â”‚   1   â”‚   4   â”‚
â”‚   2    â”‚   3   â”‚   5   â”‚
â”‚   3    â”‚   0   â”‚   6   â”‚
â”‚   4    â”‚   5   â”‚   7   â”‚
â”‚   5    â”‚   3   â”‚   8   â”‚
â”‚   6    â”‚   5   â”‚   9   â”‚
â”‚   7    â”‚   6   â”‚  10   â”‚
â”‚   8    â”‚   8   â”‚  11   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline Visualization:
Time:    0  1  2  3  4  5  6  7  8  9  10 11
Act 1:   â”œâ”€â”€â”€â”€â”€â”¤
Act 2:        â”œâ”€â”€â”€â”€â”€â”¤
Act 3:   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Act 4:                 â”œâ”€â”€â”€â”€â”€â”¤
Act 5:        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Act 6:                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Act 7:                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Act 8:                          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Goal: Find the maximum set of non-overlapping activities
```

---

## ğŸ“ CONCEPT 2: Naive Approach (Wrong!)

### Why Naive Fails

```
Attempt 1: Pick by earliest start time

Time:    0  1  2  3  4  5  6  7  8  9  10 11
Act 3:   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â† Earliest start: 0
Act 2:        â”œâ”€â”€â”€â”€â”€â”¤                        Can't pick (overlaps)
Act 1:   â”œâ”€â”€â”€â”€â”€â”¤                             Can't pick (overlaps)
Act 4:                 â”œâ”€â”€â”€â”€â”€â”¤                Pick (no overlap)
Act 8:                          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Pick (no overlap)

Result: 3 activities {3, 4, 8}

Attempt 2: Pick by shortest duration

Act 1: [1,4] = 3 units (short)
Act 2: [3,5] = 2 units (SHORTER)
Act 4: [5,7] = 2 units
...

This is messy and doesn't guarantee optimal.

Both approaches fail on some examples.

WHY? They don't consider the fundamental constraint:
    "I want to leave maximum time for remaining activities"
```

---

## ğŸ“ CONCEPT 3: Greedy Solution (Correct!)

### The Key Insight

```
OPTIMAL CRITERION: Sort by EARLIEST FINISH TIME

Why? Because:
  - An activity that finishes early leaves more time
  - More time = more room for additional activities
  - Maximizes remaining time for subproblem

Analogy:
  You have a 4-hour class period
  Multiple activities can fit
  
  Activity A: 2:00 PM - 2:30 PM (finishes earliest)
  Activity B: 2:00 PM - 3:00 PM (finishes later)
  
  If you pick A: remaining time = 2:30 - 4:00 = 1.5 hours
  If you pick B: remaining time = 3:00 - 4:00 = 1 hour
  
  Greedy picks A (leaves more time for others)
```

### Algorithm

```
ACTIVITY_SELECTION(activities):
  1. Sort activities by finish time (earliest first)
  2. selected = {first activity}
  3. last_finish = finish time of first activity
  4. For each remaining activity i:
       If start time of i â‰¥ last_finish:
           Add activity i to selected
           Update last_finish
  5. Return selected

Time Complexity: O(n log n) for sorting
                O(n) for selection
                Total: O(n log n)
```

### Step-by-Step Example

```
Activities sorted by finish time:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Act ID â”‚ Start â”‚ Finishâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1    â”‚   1   â”‚   4   â”‚  â† Sort key = finish time
â”‚   2    â”‚   3   â”‚   5   â”‚
â”‚   4    â”‚   5   â”‚   7   â”‚
â”‚   6    â”‚   5   â”‚   9   â”‚
â”‚   7    â”‚   6   â”‚  10   â”‚
â”‚   5    â”‚   3   â”‚   8   â”‚
â”‚   3    â”‚   0   â”‚   6   â”‚
â”‚   8    â”‚   8   â”‚  11   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

Greedy Selection Process:

Step 1: Select Activity 1 [1, 4]
        Last finish = 4
        Selected = {1}
        
Time:    0  1  2  3  4  5  6  7  8  9  10 11
Act 1:   â”œâ”€â”€â”€â”€â”€â”¤ âœ“ SELECTED

Step 2: Check Activity 2 [3, 5]
        Start = 3, Last finish = 4
        3 â‰¥ 4? NO â†’ Skip (overlaps)

Step 3: Check Activity 4 [5, 7]
        Start = 5, Last finish = 4
        5 â‰¥ 4? YES â†’ Select
        Last finish = 7
        Selected = {1, 4}

Time:    0  1  2  3  4  5  6  7  8  9  10 11
Act 1:   â”œâ”€â”€â”€â”€â”€â”¤ âœ“
Act 4:                 â”œâ”€â”€â”€â”€â”€â”¤ âœ“

Step 4: Check Activity 6 [5, 9]
        Start = 5, Last finish = 7
        5 â‰¥ 7? NO â†’ Skip (overlaps)

Step 5: Check Activity 7 [6, 10]
        Start = 6, Last finish = 7
        6 â‰¥ 7? NO â†’ Skip (overlaps)

Step 6: Check Activity 5 [3, 8]
        Start = 3, Last finish = 7
        3 â‰¥ 7? NO â†’ Skip (overlaps)

Step 7: Check Activity 3 [0, 6]
        Start = 0, Last finish = 7
        0 â‰¥ 7? NO â†’ Skip (overlaps)

Step 8: Check Activity 8 [8, 11]
        Start = 8, Last finish = 7
        8 â‰¥ 7? YES â†’ Select
        Last finish = 11
        Selected = {1, 4, 8}

Time:    0  1  2  3  4  5  6  7  8  9  10 11
Act 1:   â”œâ”€â”€â”€â”€â”€â”¤ âœ“
Act 4:                 â”œâ”€â”€â”€â”€â”€â”¤ âœ“
Act 8:                          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ âœ“

FINAL RESULT: 3 activities {1, 4, 8}
```

---

## ğŸ“ CONCEPT 4: Proof of Correctness

### Exchange Argument for Activity Selection

```
THEOREM: Greedy algorithm for activity selection is optimal.

PROOF (by exchange argument):

Step 1: Let G = greedy solution (sorted by finish time)
        Let O = some optimal solution
        Assume G â‰  O

Step 2: Let k = first position where G and O differ
        G has activities gâ‚, gâ‚‚, ..., gâ‚–, ...
        O has activities oâ‚, oâ‚‚, ..., oâ‚–, ...
        where gâ‚...gâ‚–â‚‹â‚ = oâ‚...oâ‚–â‚‹â‚ but gâ‚– â‰  oâ‚–

Step 3: Since greedy chose gâ‚–, it has earliest finish time
        among all activities compatible with gâ‚...gâ‚–â‚‹â‚
        
        Therefore: finish(gâ‚–) â‰¤ finish(oâ‚–)

Step 4: Replace oâ‚– with gâ‚– in O:
        O' = {oâ‚, ..., oâ‚–â‚‹â‚, gâ‚–, oâ‚–â‚Šâ‚, ...}

Step 5: Is O' valid (no overlaps)?
        - oâ‚...oâ‚–â‚‹â‚ don't overlap (by assumption)
        - gâ‚– doesn't overlap with oâ‚–â‚‹â‚ 
          (because finish(gâ‚–) â‰¤ finish(oâ‚–) < start(oâ‚–â‚Šâ‚))
        - oâ‚–â‚Šâ‚... don't overlap with gâ‚–
          (because finish(gâ‚–) â‰¤ finish(oâ‚–), and oâ‚–â‚Šâ‚ doesn't overlap with oâ‚–)

Step 6: |O'| = |O| (same size, just replaced one activity)
        O was optimal, O' is valid and same size
        Therefore O' is also optimal

Step 7: O' matches G in first k positions
        Can repeat this process until O = G
        
        Therefore: G is optimal

CONCLUSION: Greedy algorithm produces an optimal solution. âœ“
```

### Why Finish Time Works

```
KEY INSIGHT:

Among all compatible choices, picking the one that finishes
EARLIEST is always safe because:

1. It leaves maximum time for remaining activities
2. Whatever we could fit after a later-finishing activity,
   we can also fit after an earlier-finishing one
3. We might fit MORE after an earlier-finishing activity

Visual Proof:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Remaining Time:                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Option A (Greedy):  â”œâ”€â”€â”¤
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ More room

Option B (Not):        â”œâ”€â”€â”€â”€â”€â”€â”¤
                              â””â”€â”€â”€â”€â”˜ Less room

A finishes earlier â†’ leaves more time for remaining activities
```

---

## ğŸ“ CONCEPT 5: Interval Problem Variations

### Variation 1: Minimize Resources Needed

```
PROBLEM: Minimum number of rooms/resources for all activities

Example:
Activities: [1,4], [2,5], [3,6], [7,9]

Visual:
Time: 1  2  3  4  5  6  7  8  9
      â”œâ”€â”€â”€â”€â”€â”¤
         â”œâ”€â”€â”€â”€â”€â”¤
            â”œâ”€â”€â”€â”€â”€â”¤
                     â”œâ”€â”€â”€â”€â”€â”¤

How many rooms needed?
At time 3: Activities [1,4], [2,5], [3,6] all running
â†’ Need 3 rooms

Solution Approach (NOT greedy by finish time):
  Use sweep line algorithm:
  1. Create events: +1 for start, -1 for end
  2. Sort events by time
  3. Track running count of activities
  4. Maximum count = rooms needed

Why not greedy by finish time?
  Because we're not selecting a subset,
  we're finding resource constraints
```

### Variation 2: Maximum Weight Selection

```
PROBLEM: Select non-overlapping activities with maximum TOTAL WEIGHT

Example:
Activity 1 [1,4]: weight = 100
Activity 2 [3,5]: weight = 50
Activity 3 [5,7]: weight = 80
Activity 4 [6,9]: weight = 70

Greedy by finish time:
  Pick 1 [1,4] weight 100
  Pick 3 [5,7] weight 80
  Total: 180

But what about:
  Pick 2 [3,5] weight 50
  Pick 4 [6,9] weight 70
  Total: 120

Greedy gives 180, others give less...
Is it always optimal?

Actually, NO! Greedy doesn't work here.

Counter-example:
Activity 1 [1,2]: weight = 100
Activity 2 [2,10]: weight = 200

Greedy by finish time picks 1 (finishes earlier):
  Result: 100

Optimal is to pick 2:
  Result: 200

Why doesn't greedy work?
  Because greedy choice property fails
  Picking earliest finish doesn't guarantee
  you leave room for high-weight activities

Solution: Use DYNAMIC PROGRAMMING instead of greedy
```

---

## ğŸ“Š Visual: Interval Problem Decision Tree

```
WHICH ALGORITHM TO USE FOR INTERVAL PROBLEMS?

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ What are we optimizing?             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                     â”‚
      â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUNT       â”‚    â”‚ WEIGHT/VALUE    â”‚
â”‚ (Select max â”‚    â”‚ (Maximize total â”‚
â”‚  number)    â”‚    â”‚  weight)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                     â”‚
      â–¼                     â–¼
  GREEDY âœ“            DYNAMIC PROG âœ“
  Sort by             (Greedy fails)
  finish time
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 2

| Concept | Key Point |
|---------|-----------|
| **Activity Selection** | Choose by earliest finish time |
| **Why Finish Time?** | Leaves maximum time for remaining |
| **Exchange Proof** | Shows early-finish choice always safe |
| **Greedy Works** | Because optimal substructure + greedy choice property |
| **Variations** | Different problems need different approaches |
| **Weight Selection** | Greedy FAILS (use DP instead) |

---

---

## ğŸ“… DAY 3: HUFFMAN CODING & OPTIMAL TREES

### â±ï¸ Duration: 90 minutes

---

## ğŸ“ CONCEPT 1: Prefix Codes and Huffman Coding

### Why Prefix Codes Matter

```
PROBLEM: Encode text efficiently

Example: Encoding the word "HELLO"

ASCII encoding (standard):
H = 01001000
E = 01000101
L = 01001100
O = 01001111

Total: 5 characters Ã— 8 bits = 40 bits

But can we do better?

Key Insight: Some letters appear more frequently!
In English text:
  E appears very frequently
  Q appears very rarely

GREEDY IDEA:
  Give short codes to frequent letters
  Give long codes to rare letters

Example Custom Code:
H = 01
E = 0
L = 10
O = 11

Text "HELLO" = 01 + 0 + 10 + 10 + 11
             = 0 1 0 10 10 11
             
But wait... ambiguity!
  "010" could be:
    - H + E        = 01 + 0
    - E + H + E    = 0 + 1 + 0

SOLUTION: Use PREFIX CODES
(No code is a prefix of another code)

Example:
H = 00
E = 01
L = 10
O = 11

Now "00 01 10 10 11" = "HELLO" unambiguously!

Better example:
For frequently used letters, shorter codes:

E (frequent) = 0       (1 bit)
A (frequent) = 10      (2 bits)
X (rare)     = 11000   (5 bits)
Z (rare)     = 11001   (5 bits)
...

HUFFMAN CODING: Finds optimal prefix code
                 (minimum total bits for text)
```

### Visual: Prefix Code Property

```
PREFIX CODE CONSTRAINT: No code is prefix of another

Valid:
â”œâ”€ 0
â”œâ”€ 10
â”œâ”€ 110
â”œâ”€ 111

Invalid (10 is prefix of 101):
â”œâ”€ 0
â”œâ”€ 10  â† This is a prefix...
â”œâ”€ 101 â† ...of this!

Visual Tree (Valid):
        root
        /  \
       0    1
      /    / \
     A    1   1
         / \ / \
        0  1 0  1
        B  C D  E

Reading codes:
  A: follow left  â†’ 0
  B: follow right, left, left â†’ 10
  C: follow right, left, right â†’ 101
  D: follow right, right, left â†’ 110
  E: follow right, right, right â†’ 111

No code is a prefix of another âœ“
```

---

## ğŸ“ CONCEPT 2: Huffman Coding Algorithm

### The Key Greedy Insight

```
GREEDY CHOICE:
"Build the tree BOTTOM-UP"
"Combine the TWO LEAST-FREQUENT characters first"

Why?
- Least frequent characters get longest codes
- Most frequent characters get shortest codes
- Overall bits minimized

Analogy:
You have 4 items with weights:
A = 5 kg
B = 7 kg
C = 8 kg
D = 10 kg

Want to arrange them in tree to minimize depth for heavy items

Put heavy items near root (short path)
Put light items far from root (long path)

So combine light items first!
```

### Algorithm Step-by-Step

```
HUFFMAN_CODING(characters, frequencies):

Input: Set of characters with their frequencies

1. CREATE LEAF NODE for each character
   Initial forest: {A:5, B:7, C:8, D:10}

2. WHILE more than 1 tree in forest:
     a) SELECT the two trees with minimum frequency
     b) CREATE a new internal node
        Frequency = sum of two selected frequencies
     c) MAKE first tree left child, second tree right child
     d) REMOVE the two trees from forest
     e) ADD new tree to forest

3. ROOT of remaining tree = Huffman tree

4. ASSIGN codes:
     Left child = 0
     Right child = 1
     Path from root to leaf = code for that character
```

### Example Walkthrough

```
Characters and frequencies:
A: 5
B: 7
C: 8
D: 10

Step 1: Create leaf nodes
â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”
â”‚ A â”‚  â”‚ B â”‚  â”‚ C â”‚  â”‚ D  â”‚
â”‚ 5 â”‚  â”‚ 7 â”‚  â”‚ 8 â”‚  â”‚ 10 â”‚
â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜

Step 2: Combine two smallest (A:5, B:7)
                â”Œâ”€â”€â”€â”€â”
                â”‚ 12 â”‚
               /    \
            A(5)    B(7)
            
Remaining forest:
            â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”
            â”‚ 12 â”‚  â”‚ C â”‚  â”‚ D  â”‚
            â”‚AB â”‚  â”‚ 8 â”‚  â”‚ 10 â”‚
            â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜

Step 3: Combine two smallest (C:8, AB:12)
                    â”Œâ”€â”€â”€â”€â”
                    â”‚ 20 â”‚
                   /    \
                C(8)    AB(12)
                        /    \
                      A(5)   B(7)
                      
Remaining forest:
            â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”
            â”‚ 20 â”‚  â”‚ D  â”‚
            â”‚CAB â”‚  â”‚ 10 â”‚
            â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜

Step 4: Combine two smallest (D:10, CAB:20)
                        â”Œâ”€â”€â”€â”€â”
                        â”‚ 30 â”‚
                       /    \
                    D(10)   CAB(20)
                            /     \
                        C(8)      AB(12)
                                 /     \
                              A(5)    B(7)

Remaining forest: Just one tree!

HUFFMAN TREE:
                        root
                        /  \
                      0    1
                     /      \
                    D        *
                           /   \
                          0     1
                         /       \
                        C        *
                               /   \
                              0     1
                             /       \
                            A         B

Final Codes:
D = 0       (1 bit)
C = 10      (2 bits)
A = 110     (3 bits)
B = 111     (3 bits)

Total bits for text "AABCDDD":
A(2) + A(2) + B(3) + C(2) + D(1) + D(1) + D(1)
= 2 + 2 + 3 + 2 + 1 + 1 + 1
= 12 bits

With fixed 3-bit codes: 7 Ã— 3 = 21 bits
Huffman saves: 21 - 12 = 9 bits! âœ“
```

---

## ğŸ“ CONCEPT 3: Why Huffman is Optimal

### Correctness Proof (Exchange Argument)

```
THEOREM: Huffman coding produces optimal prefix code

PROOF:

Lemma 1: Optimal tree has least-frequent characters at deepest levels
Proof: If not, swap them upward. Same or shorter total cost.

Lemma 2: In optimal tree, two least-frequent characters
        are siblings at deepest level
Proof: By Lemma 1, they're deep. If not siblings,
       swap one with parent's sibling. Same or shorter cost.

Main Proof:
Let G = Greedy (Huffman) solution
Let O = Some optimal solution

Case 1: G = O
        Then G is optimal. âœ“

Case 2: G â‰  O
        By Lemma 2, O has two least-frequent chars as siblings
        Huffman chooses them as siblings (Lemma 2)
        
        Remove these two characters from both trees.
        Subproblem: {remaining chars + combined char}
        
        Both G and O must solve subproblem optimally
        (otherwise we could improve them)
        
        Therefore: G_subproblem = O_subproblem
        Therefore: G = O (contradiction)
        
        So Case 2 is impossible.

CONCLUSION: G = O, so Huffman is optimal. âœ“
```

### Visual Proof Intuition

```
KEY INSIGHT: If we combine the two least-frequent characters,
             we DON'T harm optimality.

Why?
- They appear least, so their code length affects total least
- By combining them first, we force them deeper in tree
- Longer codes for low-frequency chars = good trade-off

Intuitive Trade-off:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If we use long codes for rare letters      â”‚
â”‚ We can use short codes for common letters  â”‚
â”‚ Net effect: OVERALL SHORTER                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ CONCEPT 4: Optimality of Prefix Codes

### Why Huffman Outperforms Other Schemes

```
COMPARISON:

Fixed-length code (e.g., ASCII):
A = 00
B = 01
C = 10
D = 11
(All 2 bits regardless of frequency)

Variable-length (Non-prefix):
A = 0
B = 01     â† Problem: B is prefix of 011, etc.
C = 011
D = 0111

Variable-length (Huffman):
D = 0      (Most frequent = shortest)
C = 10
B = 110
A = 111    (Least frequent = longest)

For text heavy in D, lighter in A:
Fixed:     40 bits (always)
Huffman:   Better depending on distribution
```

---

## ğŸ“Š Visual: Huffman Algorithm Decision Tree

```
HUFFMAN CODING ALGORITHM:

Input: Character frequencies
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create leaf for each char   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ While > 1 tree:     â”‚
    â”‚  1. Pick 2 minimum  â”‚
    â”‚  2. Combine them    â”‚
    â”‚  3. Insert back     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Root = Huffman  â”‚
         â”‚ tree            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Assign codes:    â”‚
        â”‚ Left = 0         â”‚
        â”‚ Right = 1        â”‚
        â”‚ Path = code      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 3

| Concept | Key Point |
|---------|-----------|
| **Prefix Code** | No code is prefix of another |
| **Huffman Idea** | Combine least-frequent first |
| **Greedy Choice** | Always safe for optimal substructure |
| **Why Optimal** | Least-frequent chars get longest codes |
| **Exchange Proof** | Swapping doesn't harm optimality |
| **Application** | Data compression, file formats |

---

---

## ğŸ“… DAY 4: FRACTIONAL KNAPSACK & SCHEDULING

### â±ï¸ Duration: 90 minutes

---

## ğŸ“ CONCEPT 1: The Knapsack Problem

### Two Variations: 0/1 vs Fractional

```
KNAPSACK PROBLEM (General):
- Knapsack capacity: W (e.g., 100 kg)
- n items, each with:
  - Weight: wáµ¢
  - Value: váµ¢
- Goal: Maximize total value while staying within capacity

VARIATION 1: 0/1 Knapsack (Discrete)
- Can either TAKE the entire item or LEAVE it
- Cannot take partial item
- Example: selecting art pieces (can't take half a painting)

VARIATION 2: Fractional Knapsack (Continuous)
- Can take ANY FRACTION of an item
- Example: sand, gold, grain (can take half a bag)

KEY DIFFERENCE:
- 0/1: DYNAMIC PROGRAMMING required
- Fractional: GREEDY algorithm works! âœ“
```

### Example Comparison

```
Capacity: W = 10 kg

Items:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID  â”‚ Weight â”‚ Value â”‚ Value/Wt   â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1  â”‚   6    â”‚  30   â”‚    5.0     â”‚
â”‚  2  â”‚   3    â”‚  14   â”‚   4.67     â”‚
â”‚  3  â”‚   4    â”‚  16   â”‚    4.0     â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

0/1 KNAPSACK (choose whole items):
Option 1: Take items 1 + 2 = 9 kg, value = 44
Option 2: Take items 1 + 3 = 10 kg, value = 46 âœ“ OPTIMAL
Option 3: Take items 2 + 3 = 7 kg, value = 30

GREEDY by value/weight fails here!
Greedy would pick: 1 (5.0) + 2 (4.67) = 9 kg, value = 44
But optimal is 1 + 3 = value 46

FRACTIONAL KNAPSACK (take fractions):
Greedy by value/weight:
1. Take item 1 (value/wt = 5.0): full = 6 kg, value = 30
2. Remaining capacity: 4 kg
   Take item 2 (value/wt = 4.67): 
   Can't fit all 3 kg, so take 4/3 kg worth
   Fraction = 4/3 Ã· 3 = 4/9 of item 2
   Value = (4/9) Ã— 14 = 6.22

Total: 30 + 6.22 = 36.22 value in 10 kg âœ“ OPTIMAL

GREEDY WORKS FOR FRACTIONAL!
```

---

## ğŸ“ CONCEPT 2: Fractional Knapsack Algorithm

### The Greedy Approach

```
FRACTIONAL_KNAPSACK(items, capacity):

1. Calculate value/weight ratio for each item

2. Sort items by ratio in DESCENDING order
   (highest value per unit weight first)

3. For each item in sorted order:
   IF weight of item â‰¤ remaining capacity:
       Take entire item
       Reduce remaining capacity
   ELSE:
       Take fractional part
       remaining_value = (remaining capacity / weight) Ã— value
       Add to total
       Break (knapsack full)

4. Return total value

Time Complexity: O(n log n) for sorting
                O(n) for selection
                Total: O(n log n)
```

### Step-by-Step Example

```
Items:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID  â”‚ Weight â”‚ Value â”‚ Value/Wt   â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  A  â”‚   5    â”‚  60   â”‚   12.0     â”‚ â† Highest ratio
â”‚  B  â”‚   3    â”‚  30   â”‚   10.0     â”‚
â”‚  C  â”‚   4    â”‚  20   â”‚    5.0     â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Capacity = 10 kg

Step 1: Sort by value/weight (descending)
        A (12.0), B (10.0), C (5.0)

Step 2: Take item A
        Weight: 5 kg (â‰¤ capacity 10)
        Take all of A
        Total value: 60
        Remaining capacity: 5 kg
        
Step 3: Take item B
        Weight: 3 kg (â‰¤ capacity 5)
        Take all of B
        Total value: 60 + 30 = 90
        Remaining capacity: 2 kg

Step 4: Take item C
        Weight: 4 kg (> capacity 2)
        Can't take all
        Take fraction: 2/4 = 0.5 of C
        Value from C: 0.5 Ã— 20 = 10
        Total value: 90 + 10 = 100
        Remaining capacity: 0 kg (full!)

RESULT:
- Take 5 kg of A (entire item)
- Take 3 kg of B (entire item)
- Take 2 kg of C (half the item)
- Total weight: 10 kg
- Total value: 100

Visual:
Knapsack [5 kg A | 3 kg B | 2 kg C]
         â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¤
         |  60  | 30  | 10| = 100 value
         â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜
```

---

## ğŸ“ CONCEPT 3: Why Greedy Works for Fractional

### Exchange Argument

```
THEOREM: Greedy by value/weight ratio is optimal for fractional knapsack

PROOF (exchange argument):

Let G = Greedy solution (by ratio)
Let O = Some optimal solution

Suppose G â‰  O:

Case 1: Different items selected
        G includes item i, O doesn't
        i has highest ratio among non-selected in O
        
        O includes item j instead
        Since i's ratio > j's ratio: value(i)/weight(i) > value(j)/weight(j)
        
        Replace j with i in O:
        Same weight, higher value â†’ O' is better than O
        Contradiction (O was optimal)

Case 2: Same items, different fractions
        Let i be first item where they differ
        G takes fraction x of i
        O takes fraction y of i, where x > y
        
        O must have more of some lower-ratio item j
        Swap portion of j for portion of i in O:
        - Remove 1 unit of j: lose value(j)/weight(j)
        - Add 1 unit of i: gain value(i)/weight(i)
        - Since i's ratio is higher: net gain!
        - O' is better than O
        Contradiction
        
Therefore: G = O (greedy is optimal)

KEY: Fractional allows perfect swapping at any granularity
     0/1 doesn't (can't partially swap items)
```

### Visual: Why Fractional Allows Greedy

```
FRACTIONAL KNAPSACK:

Can swap at any granularity:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Swap small amount of item B  â”‚
â”‚ â†“                            â”‚
â”‚ For small amount of item A   â”‚
â”‚ (A has higher ratio)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Even if current solution has mostly B,
can improve by replacing with A.

0/1 KNAPSACK:

Can only swap whole items:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If solution has item B       â”‚
â”‚ And we want item A           â”‚
â”‚ But weights don't match      â”‚
â”‚ â†’ Can't swap!                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example: A = 5 kg, B = 3 kg
If O has B (3 kg) and G wants A (5 kg),
can't just swap (different weights)

May need to remove B and C to fit A.
This creates complex trade-offs
â†’ DP needed
```

---

## ğŸ“ CONCEPT 4: Job Sequencing with Deadlines

### Problem Definition

```
JOB SEQUENCING WITH DEADLINES:

Input:
- n jobs
- Each job i has:
  - Profit: páµ¢ (value if completed)
  - Deadline: dáµ¢ (must finish by this time)
  - Duration: 1 time unit (all jobs same length)

Constraint:
- Can do 1 job per time unit
- Must finish job before deadline

Output:
- Schedule that maximizes total profit

Example:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job ID â”‚ Profit â”‚ Deadline â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1    â”‚  100   â”‚    2     â”‚
â”‚   2    â”‚   80   â”‚    3     â”‚
â”‚   3    â”‚   60   â”‚    2     â”‚
â”‚   4    â”‚   50   â”‚    4     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Possible schedules:
Schedule 1: Job 1, Job 2, Job 4
            Finish times: 1, 2, 3
            Profit: 100 + 80 + 50 = 230

Schedule 2: Job 1, Job 3, Job 2
            Finish times: 1, 2, 3
            Profit: 100 + 60 + 80 = 240 âœ“ Better

Goal: Find scheduling that maximizes profit
```

---

## ğŸ“ CONCEPT 5: Greedy Solution for Job Sequencing

### Algorithm

```
JOB_SEQUENCING(jobs):

1. Sort jobs by PROFIT in DESCENDING order
   (highest profit first)

2. Initialize: time_slots = [empty, empty, ..., empty] for 1 to max_deadline

3. For each job in sorted order:
   a) Find latest available slot at or before job's deadline
   b) If found:
       Schedule job in that slot
       Add profit to total
   c) Else:
       Skip this job (can't meet deadline)

4. Return total profit and schedule

Time Complexity: O(nÂ²) worst case (finding slots)
                Can optimize to O(n log n) with disjoint set
```

### Step-by-Step Example

```
Jobs sorted by profit (descending):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job ID â”‚ Profit â”‚ Deadline â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1    â”‚  100   â”‚    2     â”‚ â† Highest profit
â”‚   2    â”‚   80   â”‚    3     â”‚
â”‚   4    â”‚   50   â”‚    4     â”‚
â”‚   3    â”‚   60   â”‚    2     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Time slots (1 to 4):
Slot: [ _ , _ , _ , _ ]
       1   2   3   4

Step 1: Try Job 1 (profit 100, deadline 2)
        Available slot at or before time 2?
        Time 2 is available âœ“
        Schedule: [ _ , 1, _ , _ ]
        Profit: 100

Step 2: Try Job 2 (profit 80, deadline 3)
        Available slot at or before time 3?
        Time 3 is available âœ“
        Schedule: [ _ , 1, 2, _ ]
        Profit: 100 + 80 = 180

Step 3: Try Job 4 (profit 50, deadline 4)
        Available slot at or before time 4?
        Time 4 is available âœ“
        Schedule: [ _ , 1, 2, 4]
        Profit: 100 + 80 + 50 = 230

Step 4: Try Job 3 (profit 60, deadline 2)
        Available slot at or before time 2?
        Time 2 has Job 1 âœ—
        Time 1 is available âœ“
        Schedule: [3, 1, 2, 4]
        Profit: 230 + 60 = 290

FINAL SCHEDULE:
Slot 1: Job 3 (profit 60)
Slot 2: Job 1 (profit 100)
Slot 3: Job 2 (profit 80)
Slot 4: Job 4 (profit 50)

Total Profit: 290
All deadlines met: âœ“
```

---

## ğŸ“ CONCEPT 6: Why Greedy Works Here

### Intuitive Explanation

```
KEY INSIGHT:
"Schedule highest-profit jobs first,
 fit them into latest available slots"

Why latest slots?
- Leaves earlier slots for future jobs
- Maximum flexibility for remaining jobs

Why profit order?
- If a job can't fit (deadline too early),
  better to skip low-profit job than high-profit
- High-profit jobs are "worth the effort"
- Even if we can't fit all, we get maximum profit

Visual Intuition:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Greedy picks high-profit jobs   â”‚
â”‚ Places them in latest valid     â”‚
â”‚ position to leave room for rest â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Compare: Early scheduling
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If we place high-profit early   â”‚
â”‚ We might block later slots      â”‚
â”‚ Can't fit other profitable jobs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Visual Comparison: Greedy Applications

```
WHEN TO USE GREEDY:

Fractional Knapsack:     âœ“ Greedy works
Activity Selection:       âœ“ Greedy works
Job Sequencing:          âœ“ Greedy works
Huffman Coding:          âœ“ Greedy works

0/1 Knapsack:            âœ— Greedy fails â†’ DP
Longest Path:            âœ— Greedy fails â†’ DP
Maximum Spanning Tree:   âœ“ Greedy works

KEY: Must prove for each problem!
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 4

| Concept | Key Point |
|---------|-----------|
| **Fractional Knapsack** | Greedy by value/weight ratio works |
| **0/1 Knapsack** | Greedy fails, need dynamic programming |
| **Job Sequencing** | Greedy by profit with latest-slot placement |
| **Exchange Proof** | Shows greedy choice always safe |
| **Key Difference** | Fractional allows continuous swapping |
| **Why Sort** | Makes greedy choice obvious and optimal |

---

---

## ğŸ“‹ WEEK 12 SUMMARY & MASTERY CHECKLIST

### ğŸ¯ Concepts Covered

1. âœ… **Greedy Fundamentals**
   - Locally optimal choice at each step
   - Requires optimal substructure + greedy choice property
   - Exchange argument proves correctness

2. âœ… **Activity Selection**
   - Sort by earliest finish time
   - Greedy "stays ahead" of optimal
   - Maximum non-overlapping intervals

3. âœ… **Huffman Coding**
   - Combine least-frequent characters first
   - Build optimal prefix codes
   - Proves optimality by exchange argument

4. âœ… **Fractional Knapsack**
   - Sort by value/weight ratio
   - Take items greedily until full
   - Continuous fractions allow perfect swapping

5. âœ… **Job Sequencing**
   - Sort by profit descending
   - Schedule in latest available slot
   - Maximizes profit within deadlines

---

### ğŸ“Š Problem-Solving Patterns Identified

| Pattern | Key Characteristic | Greedy Criterion |
|---------|-------------------|------------------|
| **Activity** | Max count, fixed slots | Earliest finish |
| **Huffman** | Minimize code length | Least frequent first |
| **Knapsack (Frac)** | Max value, weight limit | Max value/weight |
| **Job Sequencing** | Max profit, time slots | Max profit |

---

### âœ… MASTERY CHECKLIST FOR WEEK 12

- [ ] Can explain greedy algorithm concept to someone else
- [ ] Understand when greedy WORKS vs when it FAILS
- [ ] Can construct exchange argument for any greedy problem
- [ ] Know activity selection algorithm perfectly (can implement from scratch)
- [ ] Understand Huffman coding tree construction
- [ ] Can explain why combining least-frequent is optimal
- [ ] Know fractional knapsack algorithm (can implement)
- [ ] Understand difference between 0/1 and fractional knapsack
- [ ] Can solve job sequencing without looking up algorithm
- [ ] Can design greedy solution for new interval-based problem
- [ ] Can recognize when a problem requires DP instead of greedy
- [ ] Can prove correctness using exchange argument
- [ ] Can prove correctness using induction

---

### ğŸ”‘ TOP 5 THINGS TO REMEMBER

1. **Not all locally optimal choices lead to global optimum**
2. **Must prove greedy is correct for each specific problem**
3. **Exchange argument is the KEY proof technique**
4. **Sorting is critical - choose right criterion**
5. **Greedy works when problem has greedy choice property + optimal substructure**

---

**End of Week 12: Greedy Algorithms & Proofs**

*Comprehensive concept explanations with no code*  
*Visual diagrams, examples, and intuitive proofs*  
*Ready for implementation in any language*
