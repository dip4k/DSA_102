# ðŸ“˜ PHASE C (CONTINUED) - DYNAMIC PROGRAMMING & COMPLETE PHASE D, E, F, G
## Weeks 10-11 | 30-35 hours | Mastering Optimization via DP - COMPLETE CONTENT

---

## ðŸ“Œ WEEK 10: DYNAMIC PROGRAMMING I - FUNDAMENTALS (COMPLETE)

### Weekly Goal
Master DP fundamentals from recursion + memoization to tabulation, solving 1D, 2D, and sequence problems. Build the mindset: identify subproblems, cache results, build solutions.

### Weekly Outcomes
- Identify overlapping subproblems and optimal substructure in problems
- Implement both top-down (memoization) and bottom-up (tabulation) DP
- Solve knapsack family problems efficiently
- Master grid and edit distance problems
- Apply DP to sequences and optimization problems

### Summary
This week is about shifting mindset. DP solves optimization problems by breaking them into manageable subproblems and caching results. Students learn the fundamental patterns that appear across hundreds of problems.

---

### ðŸ“… DAY 1: DP FUNDAMENTALS (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Overlapping Subproblems**
  - Same subproblem solved repeatedly
  - Fibonacci: fib(5) requires fib(3) twice
  - Signal for DP: redundant computation
  - Visualization: recursion tree shows repeating nodes

- **Optimal Substructure**
  - Optimal solution contains optimal solutions to subproblems
  - Max subarray sum contains optimal max subarrays
  - Longest increasing subsequence contains optimal LIS
  - Proves DP is applicable

- **Memoization (Top-Down DP)**
  - Check cache (dictionary/map) before computing
  - If cached: return immediately O(1)
  - Else: compute, store, return
  - Natural from recursive formulation
  - Space for memo table: O(number of subproblems)
  - Time: state count Ã— per-state work

- **Fibonacci Example**
  - Naive: fib(n) = O(2^n) exponential
  - With memo: fib(n) = O(n) linear
  - Only n unique subproblems
  - Cache makes huge difference

- **Tabulation (Bottom-Up DP)**
  - Solve smallest subproblems first
  - Build up to full problem size
  - No recursion, just iteration
  - More efficient: fewer function calls
  - Must identify correct order of computation
  - Typically iterative loops

- **Memoization vs Tabulation Trade-off**
  - Memoization: compute only needed subproblems, natural code
  - Tabulation: compute all subproblems, no recursion overhead
  - Choose: if state space small or sparse, memoization; if dense, tabulation
  - Both usually O(n) time for most problems

- **State Definition**
  - What information uniquely identifies a subproblem?
  - State variables define dimensions of DP table
  - Example: dp[i] for 1D, dp[i][j] for 2D
  - Careful: too specific â†’ huge state space, too general â†’ can't solve

- **DP Table Structure**
  - Dimensions match state variables
  - Each cell stores answer to that subproblem
  - Boundaries/base cases: set manually
  - Fill order: ensure dependencies computed first

- **Transitions**
  - Formula to compute dp[i] from dp[j] where j depends on i
  - Core of DP: how does current state depend on previous?
  - Example: dp[i] = max(dp[i-1], dp[i-2] + value[i])

- **Example: Climbing Stairs**
  - Problem: n stairs, take 1 or 2 steps each move
  - State: dp[i] = ways to reach stair i
  - Base: dp[0] = 1 (start), dp[1] = 1 (one stair)
  - Transition: dp[i] = dp[i-1] + dp[i-2] (from previous or two-back)
  - Answer: dp[n]
  - Time: O(n), Space: O(n) or O(1) if only keep last two

- **Recognizing DP Problems**
  - Question asks: min/max/count of something
  - Solution involves choices (take/skip, include/exclude)
  - Subproblems overlap and have optimal structure
  - Read questions for: "find minimum", "maximum", "count ways", "possible"

**Key Insights:**
- DP converts exponential to polynomial by caching
- State definition is most important step
- Transitions follow directly from recurrence
- Order of computation matters in bottom-up

**Deliverables:**
- [ ] Implement climbing stairs with memoization
- [ ] Implement climbing stairs with tabulation
- [ ] Prove time complexity O(n)
- [ ] Optimize space to O(1)
- [ ] Recognize DP vs other approaches

---

### ðŸ“… DAY 2: 1D DP & KNAPSACK FAMILY (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **1D DP Patterns**
  - Array dp[i] = answer for problem size i
  - Transition: dp[i] depends on dp[j] for j < i
  - Common pattern: dp[i] = optimal choice among options
  - Time: typically O(n Ã— options) = O(n) to O(nÂ²)
  - Space: O(n) usually reducible to O(1)

- **House Robber (Non-Adjacent Sum)**
  - Problem: array of house values, rob non-adjacent to maximize
  - State: rob[i] = max value robbing houses 0..i
  - Transition: rob[i] = max(rob[i-1], rob[i-2] + arr[i])
    - Either rob this house (plus best two-back)
    - Or skip this house (keep previous best)
  - Answer: rob[n-1]
  - Time: O(n), Space: O(1) (only need previous two)

- **Climbing Stairs Variants**
  - With cost: minimize total cost to reach stair n
    - dp[i] = min cost, transition: dp[i] = arr[i] + min(dp[i-1], dp[i-2])
  - Step sizes: given array of allowed step sizes
    - dp[i] = min(dp[i-step] + 1 for all step sizes)
  - Max jumps: limit on how many jumps allowed
    - Track additional state for jump count

- **Jump Game I & II**
  - I: can you reach end? (boolean DP or greedy)
  - II: minimum jumps to reach end? (greedy preferred)
    - State: jumps[i] = min jumps to reach i
    - Time: O(n) with greedy, O(nÂ²) with DP

- **Coin Change Problem**
  - Minimum coins to make amount
  - State: dp[i] = min coins for amount i
  - Transition: dp[i] = 1 + min(dp[i-coin] for all coins)
  - Base: dp[0] = 0
  - Answer: dp[amount]
  - Time: O(amount Ã— coin_count)

- **Coin Change II (Count Ways)**
  - Number of ways to make amount using coins
  - State: dp[i] = ways to make amount i
  - For each coin: dp[i] += dp[i-coin]
  - Order matters: iterate coins outer loop to avoid counting permutations
  - Example: if coins = [1,2], amount = 3
    - Process coin 1: dp = [1,1,1,1]
    - Process coin 2: dp = [1,1,2,2]
    - Answer: dp[3] = 2 (ways: [1,1,1] and [1,2])

- **Unbounded Knapsack (Infinite Items)**
  - Each item available unlimited times
  - Difference from 0/1 knapsack: can use same item multiple times
  - dp[i] = max value with capacity i
  - Transition: dp[i] = max(dp[i-weight[j]] + value[j] for all items j)
  - Time: O(capacity Ã— items)

- **0/1 Knapsack (Single Item)**
  - Each item available once
  - dp[i][w] = max value with first i items, capacity w
  - Or optimize to: dp[w] = max value with capacity w
  - Transition: for each item, update capacities backwards (avoid using twice)
    - for w from W down to weight[i]: dp[w] = max(dp[w], dp[w-weight[i]] + value[i])
  - Time: O(items Ã— capacity)

- **Bounded Knapsack (K Copies)**
  - Each item available k times
  - Treat as k separate items (naive)
  - Or use binary representation: groups of 1,2,4,8...

- **Related: Partition Equal Subset Sum**
  - Can we partition array into two subsets with equal sum?
  - Reduce to knapsack: target weight = sum/2
  - dp[i] = can we make sum i? (boolean)
  - Time: O(n Ã— sum)

**Key Insights:**
- Knapsack variants all follow similar DP structure
- Backward iteration needed for 0/1 to avoid reusing items
- Coin change: order of coin iteration affects interpretation

**Deliverables:**
- [ ] Implement house robber problem
- [ ] Implement coin change (min coins)
- [ ] Implement coin change (count ways)
- [ ] Implement 0/1 knapsack
- [ ] Implement partition equal subset sum

---

### ðŸ“… DAY 3: 2D DP - GRIDS & EDIT DISTANCE (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **2D Grid DP**
  - State: dp[i][j] = answer for position (i, j)
  - Often: count of paths or minimum cost
  - Transitions combine from multiple directions (up, left, diagonal)

- **Unique Paths**
  - Grid mÃ—n, obstacles, count paths from top-left to bottom-right
  - Only move right or down
  - State: dp[i][j] = ways to reach (i, j)
  - Base: dp[0][0] = 1 (if no obstacle)
  - Transition: dp[i][j] = dp[i-1][j] + dp[i][j-1] (if no obstacle)
  - Time: O(m Ã— n)

- **Minimum Path Sum**
  - Grid costs, find min-cost path from top-left to bottom-right
  - State: dp[i][j] = min cost to reach (i, j)
  - Base: dp[0][0] = grid[0][0]
  - Transition: dp[i][j] = grid[i][j] + min(dp[i-1][j], dp[i][j-1])
  - Time: O(m Ã— n)

- **Dungeon Game**
  - 2D grid with values (positive or negative health)
  - Find path from top-left to bottom-right
  - Maintain health â‰¥ 1 always
  - Minimum health needed at start
  - Backward DP: start from end, work backwards
  - Transition: dp[i][j] = max(1, min(dp[i+1][j], dp[i][j+1]) - grid[i][j])

- **Edit Distance (Levenshtein)**
  - Transform word1 to word2 with insert/delete/replace (each costs 1)
  - State: dp[i][j] = min edits to transform word1[0..i-1] to word2[0..j-1]
  - Base: dp[0][j] = j (j insertions), dp[i][0] = i (i deletions)
  - Transitions:
    - If word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] (no edit needed)
    - Else: dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
      - dp[i-1][j] + 1: delete from word1
      - dp[i][j-1] + 1: insert into word1
      - dp[i-1][j-1] + 1: replace
  - Time: O(m Ã— n) where m, n = word lengths

- **Longest Common Subsequence (LCS)**
  - Longest sequence of characters present in both strings (not necessarily contiguous)
  - State: dp[i][j] = length of LCS of word1[0..i-1], word2[0..j-1]
  - Base: dp[0][j] = 0 and dp[i][0] = 0
  - Transitions:
    - If word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] + 1
    - Else: dp[i][j] = max(dp[i-1][j], dp[i][j-1])
  - Path reconstruction: trace back from dp[m][n]
  - Time: O(m Ã— n)

- **Distinct Subsequences**
  - Count number of distinct subsequences of string s that equal string t
  - State: dp[i][j] = count of distinct subsequences of s[0..i-1] equal to t[0..j-1]
  - Base: dp[i][0] = 1 for all i (empty subsequence)
  - Transitions:
    - If s[i-1] == t[j-1]: dp[i][j] = dp[i-1][j-1] + dp[i-1][j]
    - Else: dp[i][j] = dp[i-1][j]
  - Time: O(m Ã— n)

- **Space Optimization**
  - 2D DP often needs only previous row
  - Can reduce space from O(m Ã— n) to O(n)
  - Use two arrays: current and previous
  - Or single array, update in-place (careful with order)

- **Path Reconstruction**
  - After computing DP table, trace back to find actual solution
  - Start from dp[m][n], work backwards
  - Record decisions made at each step

**Key Insights:**
- Grid and string problems naturally 2D DP
- Edit distance fundamental algorithm (spell check, DNA alignment)
- Space optimization often possible

**Deliverables:**
- [ ] Implement unique paths in grid
- [ ] Implement minimum path sum
- [ ] Implement edit distance
- [ ] Implement LCS with path reconstruction
- [ ] Optimize space for grid DP

---

### ðŸ“… DAY 4: DP ON SEQUENCES (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Longest Increasing Subsequence (LIS)**
  - Find longest subsequence with strictly increasing values
  - O(nÂ²) DP: dp[i] = longest ending at i
    - Transition: dp[i] = max(dp[j] + 1 for all j < i where arr[j] < arr[i])
    - Base: dp[i] = 1 (single element)
    - Answer: max(dp[i])
    - Time: O(nÂ²)
  - O(n log n) binary search:
    - Maintain tail array: smallest ending value for each length
    - For each element: binary search position in tail, update
    - Time: O(n log n)

- **Longest Decreasing Subsequence**
  - Symmetric to LIS
  - Same approaches apply with > instead of <

- **Maximum Subarray Sum (Kadane)**
  - dp[i] = max sum ending at i
  - Transition: dp[i] = max(arr[i], dp[i-1] + arr[i])
  - Answer: max(dp[i])
  - Time: O(n), Space: O(1)

- **Maximum Product Subarray**
  - Similar to max sum but with multiplication
  - Complication: negative values affect which is max
  - Track both max and min ending at each position
  - max_here = max(arr[i], arr[i] Ã— max_prev, arr[i] Ã— min_prev)
  - min_here = min(arr[i], arr[i] Ã— max_prev, arr[i] Ã— min_prev)

- **Weighted Job Scheduling**
  - Jobs have start, end, profit
  - Select non-overlapping jobs to maximize profit
  - Sort by end time
  - dp[i] = max profit considering jobs up to i
  - For each job i: find latest non-conflicting job p[i]
  - Transition: dp[i] = max(dp[i-1], profit[i] + dp[p[i]])
  - Time: O(n log n) with binary search for latest job

- **Russian Doll Envelopes**
  - Envelopes with width, height
  - Put one envelope inside another if strictly smaller (both dimensions)
  - Maximum envelopes that can be nested
  - Sort by width, then find LIS by height
  - Time: O(n log n)

- **Number of Longest Increasing Subsequence**
  - Count distinct LIS of maximum length
  - While computing lengths, also count
  - dp_count[i] = number of LIS ending at i
  - When finding LIS of same length from j to i, sum counts

- **Variants with Constraints**
  - LIS of exactly length k
  - LIS with constraint (e.g., alternating pattern)
  - Maximum sum of increasing subsequence (weighted)

**Key Insights:**
- Sequence DP often uses ending/starting at i
- Binary search optimization crucial for efficiency
- Multiple dimensions tracked together (length, sum, count)

**Deliverables:**
- [ ] Implement LIS with O(nÂ²) DP
- [ ] Implement LIS with O(n log n) binary search
- [ ] Implement maximum subarray sum (Kadane)
- [ ] Implement weighted job scheduling
- [ ] Solve Russian doll envelopes problem

---

### ðŸ“… DAY 5: DP OPTIMIZATIONS & ADVANCED PATTERNS (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **State Compression**
  - Reduce state space by observation
  - Example: rolling array for 2D DP (keep only two rows)
  - Saves space from O(mÃ—n) to O(n)

- **Memoization Optimizations**
  - Use dictionary for sparse state spaces
  - Avoid computing impossible states
  - Pruning: skip states that can't lead to better solution

- **Circular Array Variant**
  - Array treated as circular (last connects to first)
  - Split into two cases: exclude first or exclude last
  - Solve for both, return better
  - Example: house robber circular

- **Digit DP (Preview)**
  - Count numbers from 0 to n with certain property
  - State: position in number, tight bound, other constraints
  - Build number digit by digit
  - Time: O(log n Ã— state_space)

- **Matrix Chain Multiplication**
  - n matrices to multiply, find optimal order
  - Parenthesization affects cost
  - dp[i][j] = min cost to multiply matrices i to j
  - Transition: try all split points
  - Time: O(nÂ³)

- **Interval DP**
  - Problems on intervals [i, j]
  - State: dp[i][j] = answer for interval [i, j]
  - Transition: split interval at k
  - Example: burst balloons, remove boxes

- **Bitmask DP**
  - State includes bitmask of chosen elements
  - dp[mask] = answer using elements in mask
  - Time: O(2^n) - feasible for n â‰¤ 20
  - Example: TSP, subset selection with constraints

- **Convex Hull Optimization**
  - For certain recurrences, can optimize from O(nÂ²) to O(n log n)
  - Maintain lower envelope of functions
  - Advanced technique for competitive programming

- **Recognizing Optimization Opportunities**
  - Look for patterns: monotone dependencies
  - Check if functions are convex/concave
  - Identify if binary search applicable
  - Use memoization smartly: compute what you need

**Key Insights:**
- DP can be optimized from O(nÂ²) to O(n log n) with techniques
- State compression saves space
- Advanced patterns for specific problem structures

**Deliverables:**
- [ ] Solve problem requiring state compression
- [ ] Solve circular array DP variant
- [ ] Implement matrix chain multiplication
- [ ] Solve interval DP problem
- [ ] Recognize and apply optimization techniques

---

## ðŸ“Œ WEEK 11: DYNAMIC PROGRAMMING II - TREES, DAGS & ADVANCED (COMPLETE)

### Weekly Goal
Extend DP to complex structures. Master tree DP, DAG DP, bitmask DP, and problem-specific patterns. Learn to design DP for novel problems.

### Weekly Outcomes
- Solve tree DP problems efficiently
- Apply DP to DAGs for longest/shortest paths
- Master bitmask DP for subset problems
- Combine DP with other algorithms
- Design DP solutions for novel problems

### Summary
This week extends beyond linear structures. Students learn DP on hierarchical (trees), directed acyclic (DAGs), and combinatorial (subsets) problems. Design skills crucial here: how to formulate novel problems as DP.

---

### ðŸ“… DAY 1: DP ON TREES (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Tree DP Framework**
  - Solve for each subtree
  - Combine children's solutions into parent's solution
  - Post-order traversal: solve children before parent
  - State: dp[node][...] = answer for subtree rooted at node

- **Maximum Independent Set**
  - Select subset of nodes with max sum value
  - Constraint: no two adjacent nodes can be selected
  - dp[node][0] = max value NOT including node
  - dp[node][1] = max value INCLUDING node
  - Transitions:
    - dp[node][0] = sum of max(dp[child][0], dp[child][1]) for all children
    - dp[node][1] = value[node] + sum of dp[child][0] for all children
  - Answer: max(dp[root][0], dp[root][1])

- **Tree Diameter via DP**
  - Longest path in tree
  - dp[node] = height of node (max distance to leaf)
  - For each node: diameter through this node = left_height + right_height + 2
  - Answer: max of all diameters
  - Time: O(n)

- **Maximum Path Sum in Tree**
  - Path: sequence of nodes (can start/end anywhere)
  - Maximize sum of node values along path
  - dp[node] = max sum of path starting at node going down
  - For each node: max path through this node = left_sum + value + right_sum
  - Answer: track max seen globally
  - Time: O(n)

- **Tree Node Coloring**
  - Color tree nodes with k colors
  - Adjacent nodes must have different colors
  - Count valid colorings
  - dp[node][color] = ways to color subtree with node colored as color
  - Transition: product of colorings for children (different from parent)
  - Time: O(n Ã— kÂ²)

- **House Robber III**
  - Binary tree, rob non-adjacent nodes
  - dp[node][0] = max value NOT robbing node
  - dp[node][1] = max value ROBBING node
  - Transitions:
    - dp[node][0] = max(dp[left][0], dp[left][1]) + max(dp[right][0], dp[right][1])
    - dp[node][1] = value[node] + dp[left][0] + dp[right][0]

- **Subtree Sums & Queries**
  - Precompute sum of every subtree
  - Then answer queries efficiently
  - DP: sum[node] = value[node] + sum[left] + sum[right]
  - Time: O(n) precompute, O(1) per query

- **Rerooting DP (Advanced)**
  - DP computed from certain root initially
  - Reroot: compute DP for all possible roots
  - Two passes: bottom-up then top-down
  - Time: O(n)

- **Tree DP on Weighted Trees**
  - Edges have weights
  - Incorporate edge weights into transitions
  - Example: minimum cost to traverse subtree

**Key Insights:**
- Tree DP natural post-order structure
- Combine children's answers via recurrence
- Often O(n) or O(n Ã— k) depending on state space

**Deliverables:**
- [ ] Implement maximum independent set on tree
- [ ] Compute tree diameter with DP
- [ ] Solve maximum path sum in tree
- [ ] Solve house robber III
- [ ] Implement tree node coloring

---

### ðŸ“… DAY 2: DP ON DAGS (DIRECTED ACYCLIC GRAPHS) (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **DAG Structure**
  - No cycles (by definition)
  - Enables topological ordering
  - DP applies: no "backward" dependencies
  - Perfect for longest/shortest path problems

- **Longest Path in DAG**
  - No cycles â†’ DP works (no infinite loops)
  - Topological sort order â†’ process in that order
  - dp[node] = longest path starting from node
  - Transition: dp[node] = 1 + max(dp[neighbor] for all neighbors)
  - Base: dp[leaf] = 0 (leaves have no outgoing edges)
  - Answer: max(dp[node])
  - Time: O(V + E)

- **Shortest Path in DAG with Negative Weights**
  - Bellman-Ford needed for general graphs: O(VE)
  - DAGs allow O(V + E) via topological sort
  - Process nodes in topological order
  - Relax edges as you process
  - Guaranteed optimal since all dependencies processed

- **All Paths in DAG**
  - Count number of paths from source to sink
  - dp[node] = number of paths from node to sink
  - Transition: dp[node] = sum of dp[neighbor] for all neighbors
  - Base: dp[sink] = 1
  - Answer: dp[source]

- **Topological Sort Based DP**
  - Key: process nodes only after all predecessors
  - Ensures dependencies satisfied
  - Use DFS with finish times or Kahn's algorithm
  - Process in decreasing finish time order (DFS)

- **Vertex Weight Sum Paths**
  - Path value = sum of vertex weights
  - dp[node] = best path sum starting from node
  - Similar to longest path but different weight source
  - Transition: dp[node] = value[node] + max(dp[neighbor])

- **Matrix Multiplication Chain (DP on DAG)**
  - Order of matrix multiplication affects cost
  - Formulate as DAG where nodes are matrices
  - Edges represent dependencies
  - DP to find optimal order

- **Job Scheduling with Precedence**
  - Jobs with dependencies (precedence DAG)
  - Find critical path: longest path from start to end
  - Determines minimum completion time
  - Time: O(V + E)

- **Constraint Satisfaction**
  - DP on DAG of states
  - Each edge represents valid transition
  - Count/find valid paths satisfying constraints

**Key Insights:**
- DAGs enable polynomial DP solutions
- Topological sort crucial for correct order
- Longest/shortest path in DAG O(V+E) vs general O(VE)

**Deliverables:**
- [ ] Implement longest path in DAG
- [ ] Find all paths from source to sink
- [ ] Shortest path in DAG with negative weights
- [ ] Compute critical path in project (precedence DAG)
- [ ] Solve job scheduling with precedence

---

### ðŸ“… DAY 3: BITMASK DP & SUBSET PROBLEMS (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Bitmask Basics**
  - Integer where each bit represents inclusion in set
  - Bit i set: element i included
  - 2^n possible subsets for n elements
  - Feasible: n â‰¤ 20-25 (2^20 â‰ˆ 10^6, 2^25 â‰ˆ 33Ã—10^6)

- **Subset Enumeration**
  - Iterate from 0 to 2^n - 1
  - For each mask: extract which elements included
  - for (int i = 0; i < n; i++) if (mask & (1 << i)) ...

- **Submask Iteration**
  - Iterate through all submasks of given mask
  - for (int sub = mask; sub > 0; sub = (sub-1) & mask)
  - Efficient: O(3^n) over all submasks of all masks

- **Traveling Salesman Problem (TSP)**
  - Visit n cities exactly once, minimize distance
  - Naive: n! permutations (infeasible)
  - DP: state = (visited_cities, current_city)
  - dp[mask][last] = min distance visiting cities in mask, currently at city last
  - Transition: try visiting next unvisited city
    - for (int next = 0; next < n; next++)
    -   if (!(mask & (1 << next)))
    -     dp[mask | (1 << next)][next] = min(dp[mask | (1 << next)][next], dp[mask][last] + dist[last][next])
  - Answer: min(dp[(1<<n)-1][i] + dist[i][0] for all i)
  - Time: O(2^n Ã— nÂ²)

- **Subset Sum Problems**
  - Can we make sum S using subset of elements?
  - DP: dp[i] = can we make sum i?
  - For each element: update reachable sums
  - Time: O(n Ã— sum)

- **Maximum Weight Independent Set (Small Graph)**
  - For small n (â‰¤ 20): use bitmask
  - dp[mask] = max weight using subset mask
  - Check validity: no adjacent nodes
  - For each valid mask: compute max weight
  - Time: O(2^n)

- **Steiner Tree (Advanced)**
  - Minimum cost subtree spanning given terminals
  - State: (set of terminals, subtree root)
  - DP over all subsets of terminals
  - Transition: combine two disjoint subsets or add edge
  - Time: O(3^n Ã— n)

- **Set Partition**
  - Partition n elements into subsets with constraints
  - DP: dp[mask] = number of valid partitions of elements in mask
  - Transition: choose subset of current mask, partition rest
  - Time: O(3^n)

- **Hamiltonian Path**
  - Path visiting each vertex exactly once
  - Check existence
  - dp[mask][last] = is there Hamiltonian path in mask ending at last?
  - Time: O(2^n Ã— n)

- **Connected Components Using Bitmask**
  - Count connected components in masked subset
  - dp[mask] = number of components
  - Transition: remove one component, recurse on remaining

**Key Insights:**
- Bitmask enables subset enumeration: O(2^n)
- Submask iteration: O(3^n) over all submasks
- TSP canonical bitmask DP problem
- Feasible for n â‰¤ 20-25

**Deliverables:**
- [ ] Implement TSP with bitmask DP
- [ ] Solve subset sum problem
- [ ] Solve maximum weight independent set
- [ ] Find Hamiltonian paths/cycles
- [ ] Solve connected components with bitmask

---

### ðŸ“… DAY 4: MIXING DP WITH OTHER ALGORITHMS (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **DP + Greedy**
  - Greedy choice within DP
  - Example: jump game II (greedy + DP mindset)
  - Example: cutting rod (DP finds optimal with greedy choice)

- **DP + BFS/DFS**
  - Use DP to find best path in graph
  - Example: minimum knight moves (BFS with DP state)
  - Example: word ladder with constraints (BFS exploring DP states)

- **DP + Binary Search**
  - Binary search on answer
  - Check feasibility with DP
  - Example: minimum time to complete jobs (binary search on time + DP check)

- **DP + Segment Trees / Fenwick Trees**
  - DP where transitions require range queries
  - Use segment tree for efficient queries
  - Example: longest increasing subsequence with weights (query max in range)

- **DP + Geometry**
  - DP on geometric problems
  - Example: minimum bounding circle
  - Example: convex polygon partitioning

- **DP + String Hashing**
  - String problems with DP
  - Hash functions for efficient comparison
  - Example: distinct palindromic substrings

- **DP + Number Theory**
  - DP with modular arithmetic
  - Counting problems modulo prime
  - Example: number of ways mod 10^9+7

- **DP + Graph Algorithms**
  - DP on shortest paths
  - Example: minimum cost flow with DP
  - Example: weighted bipartite matching

**Key Insights:**
- Algorithms combine naturally
- Choose right tool for each subproblem
- Integration is where mastery shows

**Deliverables:**
- [ ] Solve problem combining DP with greedy
- [ ] Solve problem combining DP with BFS/DFS
- [ ] Solve problem combining DP with binary search
- [ ] Solve problem combining DP with sorting
- [ ] Design own problem requiring multiple algorithms

---

### ðŸ“… DAY 5: DESIGNING DP SOLUTIONS FOR NOVEL PROBLEMS (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Problem Analysis Framework**
  1. Identify what we're optimizing (min, max, count)
  2. Identify choices/decisions at each step
  3. Identify subproblem structure
  4. Define state based on information needed for future decisions
  5. Write recurrence
  6. Implement top-down or bottom-up
  7. Optimize space/time if needed

- **Recognizing DP from Problem Statement**
  - Words: "minimum", "maximum", "count", "possible", "ways"
  - Choices: "select", "include", "exclude", "choose"
  - Structure: overlapping subproblems, optimal substructure
  - Constraints: bounded variables (capacity, time, count)

- **State Design Heuristics**
  - State should capture: what information do I need to make optimal future decisions?
  - State should be: minimal (avoid extra dimensions)
  - State should be: actionable (transitions are clear)
  - Too specific: huge state space, infeasible
  - Too general: can't compute transitions

- **Example Design: Best Time to Buy/Sell Stock**
  - Problem: given prices, buy/sell to maximize profit, constraints on transactions
  - Choices: at each price, buy or sell or hold
  - State: dp[i][k][hold] = max profit on day i using k transactions, currently holding or not
  - Transitions: three choices (buy, sell, hold)
  - Constrain k as needed (e.g., k=2, unlimited, k=n//2)

- **Example Design: Cherry Pickup**
  - Problem: grid, start top-left, pick cherries, end bottom-right, return to top-left
  - Insight: two people going simultaneously is equivalent to one person twice
  - State: dp[r1][c1][r2] where r2 = r1+c1-c2 (implicitly same number of steps)
  - Transitions: each person moves right or down
  - Base: both at (0, 0) or both at (m-1, n-1)

- **Example Design: Remove Boxes**
  - Problem: remove boxes in any order, consecutive same-color boxes give bonus
  - Choices: which box to remove, triggering cascading removals
  - State: dp[i][j][k] = max score removing boxes [i..j] with k boxes of same color as i already removed
  - Insight: box at i removed together with box at j if same color and k boxes between removed
  - Recurrence: dp[i][j][k] = max(dp[i+1][j][k] (remove i now), dp[i+1][m][k+1] + dp[m+1][j][0] for m in [i+1..j] if same color)

- **Example Design: Burst Balloons**
  - Problem: burst balloons, coins = balloons[i] Ã— balloons[i-1] Ã— balloons[i+1]
  - Insight: think backward - which balloon burst last?
  - State: dp[i][j] = max coins bursting balloons between i and j (not including i, j)
  - Transitions: for each k in (i, j), burst k last
  - Recurrence: dp[i][j] = max(dp[i][k] + dp[k][j] + balloons[i] Ã— balloons[k] Ã— balloons[j]) for all k
  - Base: balloons at i, j are boundaries (add dummy 1s at ends)

- **Common Traps & Solutions**
  - Trap: state includes too much â†’ state space explodes
    - Solution: identify what's truly necessary
  - Trap: transitions unclear â†’ recurrence undefined
    - Solution: work through examples, trace through transitions
  - Trap: wrong order of computation â†’ dependencies not satisfied
    - Solution: explicitly verify order or use memoization
  - Trap: off-by-one errors â†’ wrong base case or transition bounds
    - Solution: trace carefully, test small examples

- **Testing DP Solutions**
  - Small test cases: verify manually
  - Edge cases: empty, single element, all same
  - Boundary conditions: ensure base cases correct
  - Performance: verify time/space complexity

**Key Insights:**
- DP design is skill developed with practice
- Framework helps approach novel problems
- State definition is most critical decision
- Test thoroughly on small cases first

**Deliverables:**
- [ ] Design DP for best time to buy/sell stock (k transactions)
- [ ] Design DP for cherry pickup
- [ ] Design DP for remove boxes (or burst balloons)
- [ ] Design DP for custom novel problem
- [ ] Implement and test all above

---

# ðŸŸ§ PHASE D: ALGORITHM PARADIGMS (Weeks 12-13)

## Phase Motivation
You've mastered core data structures and algorithms. Now understand the paradigms that unify them. Greedy algorithms, backtracking, branch & bound, and amortized analysis give you frameworks for new problem families.

## Phase Outcomes
- [ ] Understand greedy algorithm correctness proofs
- [ ] Design greedy solutions systematically
- [ ] Master backtracking framework and optimizations
- [ ] Apply branch & bound for optimization
- [ ] Analyze amortized complexity rigorously

---

## ðŸ“Œ WEEK 12: GREEDY ALGORITHMS & GREEDY CORRECTNESS

### Weekly Goal
Understand when greedy works and how to prove it. Master greedy solutions and recognize greedy problems.

### Weekly Topics
- Greedy strategy selection
- Exchange argument and induction proofs
- Canonical greedy problems
- Local vs global optimality
- Proving correctness rigorously

---

### ðŸ“… DAY 1: GREEDY FUNDAMENTALS & PROOF TECHNIQUES

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Greedy Idea**
  - Make locally optimal choice at each step
  - Hope local choices lead to global optimum
  - NOT always correct - requires proof

- **When Greedy Fails**
  - 0/1 knapsack: greedy by value/weight ratio doesn't work
  - Coin change: certain coin systems need DP
  - Counterexample is proof greedy fails

- **When Greedy Works**
  - Certain problem structures guarantee greedy optimal
  - Must recognize these structures
  - Must be able to prove correctness

- **Greedy Choice Property**
  - Greedy choice (locally optimal) is part of globally optimal solution
  - Doesn't mean greedy choice is unique
  - Means at least one optimal solution contains greedy choice

- **Optimal Substructure**
  - After making greedy choice, remaining subproblem is optimal
  - Necessary for both DP and greedy
  - Not sufficient to distinguish

- **Exchange Argument**
  - Assume optimal solution differs from greedy
  - Show can transform it to include greedy choice without loss
  - Yields same or better solution
  - By induction, greedy is optimal

- **Induction Proof**
  - Base case: greedy optimal for base problem
  - Inductive step: if greedy optimal for subproblem, greedy optimal for larger
  - Verify greedy choice property holds

- **Matroid Theory (Advanced)**
  - Problems on matroids have greedy solutions
  - Matroid properties ensure greedy works
  - Beyond interview scope but conceptually powerful

**Key Insights:**
- Greedy not always correct - requires proof
- Exchange argument common proof technique
- Proving wrongness simpler: find counterexample

**Deliverables:**
- [ ] Prove greedy works for activity selection via exchange argument
- [ ] Prove greedy works for interval scheduling via induction
- [ ] Find counterexample to greedy for 0/1 knapsack
- [ ] Design own problem where greedy fails
- [ ] Prove greedy correctness for custom problem

---

### ðŸ“… DAY 2: GREEDY CLASSICS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Activity Selection**
  - Activities with start, end times
  - Select maximum non-overlapping activities
  - Greedy: sort by end time, select greedily
  - Proof: exchange argument on activities

- **Interval Scheduling**
  - Maximize number of non-overlapping intervals
  - Greedy: earliest end time first
  - Why: leaves most room for future intervals

- **Fractional Knapsack**
  - Items with value/weight, fractional allowed
  - Greedy: take highest value/weight ratio items first
  - Works because fractional: can take partial item to fill exactly
  - 0/1 knapsack: doesn't work (DP needed)

- **Huffman Coding**
  - Variable-length codes for compression
  - Greedy: always merge two least frequent nodes
  - Results in optimal prefix-free code
  - Proof: exchange argument on tree structure

- **Minimum Spanning Tree**
  - Kruskal: sort edges, greedily add if no cycle (via union-find)
  - Prim: greedily add minimum edge from tree to outside
  - Both greedy, both correct (different proofs)
  - Cut property: minimum edge across cut in MST

- **Dijkstra's Shortest Path**
  - Greedy: always expand nearest unvisited
  - Why works: once node reached optimally, never improve
  - Non-negative weights required
  - Negative weights: Bellman-Ford (not greedy)

- **Job Sequencing with Deadlines**
  - Jobs with profit and deadline
  - Maximize profit subject to meeting deadlines
  - Greedy: sort by profit descending, assign to latest possible slot
  - Proof: exchange argument on job order

- **Container Loading Problem**
  - Containers with weight limit, items with weights
  - Minimize containers needed
  - Greedy: best-fit or first-fit decreasing
  - Not always optimal but good heuristic

**Key Insights:**
- Greedy classics worth memorizing
- Each has specific greedy choice
- Exchange argument justifies greedy choice

**Deliverables:**
- [ ] Implement activity selection
- [ ] Implement Huffman coding
- [ ] Solve job sequencing with deadlines
- [ ] Compare greedy vs optimal for interval scheduling variants
- [ ] Prove greedy for chosen problem

---

### ðŸ“… DAY 3: GREEDY ON GRAPHS & OPTIMIZATION PROBLEMS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Minimum Spanning Trees Redux**
  - Kruskal: add edges in weight order, skip if creates cycle
  - Prim: maintain growing tree, add minimum edge from tree to outside
  - Both O(E log E) or O(E log V) with proper implementation
  - Cut property justifies greedy

- **Single-Source Shortest Path**
  - Dijkstra greedy: always expand nearest
  - Optimal substructure: once node reached optimally, distance fixed
  - Greedy choice property: expanding nearest is optimal

- **Interval Merging**
  - Sort by start, merge overlapping
  - Greedy: merge whenever possible
  - Why: doesn't prevent future merges

- **Meeting Rooms Minimum**
  - Sort start/end times
  - Sweep line: track concurrent meetings at each time
  - Greedy: number of concurrent = rooms needed
  - Why: obvious - all concurrent need room

- **Gas Station Route**
  - Can we complete circular route with fuel constraints?
  - Greedy: if can't reach next station, start later
  - Proof: why starting later helps

- **Candy Distribution**
  - Give candy to children based on rating
  - Constraint: higher rating than neighbor gets more candy
  - Greedy: two-pass algorithm (left-to-right, right-to-left)
  - Why: ensures constraints satisfied with minimum candy

- **Reconstruct Itinerary**
  - Given airline tickets (from, to pairs)
  - Reconstruct valid itinerary using all tickets
  - Greedy (Hierholzer): DFS with backtracking
  - Why: finds Eulerian path

- **Video Stitching**
  - Segments cover ranges, find min segments to cover [0, target]
  - Greedy: always take segment covering furthest from current position
  - Why: leaves most room for next segment

**Key Insights:**
- Greedy on graphs often uses specific structure (cut property, local optimality)
- Sweep line powerful for interval problems
- Multiple greedy strategies - choose by problem structure

**Deliverables:**
- [ ] Implement Dijkstra's with greedy understanding
- [ ] Solve meeting rooms problem
- [ ] Solve candy distribution problem
- [ ] Solve video stitching problem
- [ ] Prove greedy for graph problem

---

### ðŸ“… DAY 4: GREEDY VS DP & HYBRID APPROACHES

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Greedy vs DP**
  - Greedy: local choice, no backtracking
  - DP: consider all choices, backtrack, memoize
  - Greedy faster if correct
  - DP safer if greedy might fail

- **Recognizing DP vs Greedy**
  - DP signals: "minimum/maximum (need to compare all)", overlapping subproblems
  - Greedy signals: "maximum number", "always choose best available", local choice obvious
  - Sometimes both work: greedy faster

- **Hybrid: Greedy within DP**
  - Greedy choice at each DP state transition
  - Example: jump game II (greedy on choices)
  - Example: best time to buy/sell (greedy on states)

- **Hybrid: DP Verification of Greedy**
  - Use DP to verify greedy solution correct
  - Compare greedy and DP results
  - If same: greedy optimal

- **Greedy Approximation Algorithms**
  - When exact optimization intractable (NP-hard)
  - Greedy gives approximate solution
  - Example: vertex cover (2-approximation)
  - Example: set cover (O(log n) approximation)

- **Comparing Multiple Greedy Strategies**
  - Same problem: different greedy choices possible
  - Example: interval scheduling vs weighted interval scheduling (greedy vs DP)
  - Test which greedy strategy works

**Key Insights:**
- Know when to use each: greedy vs DP
- Hybrid approaches common in real problems
- Greedy useful for approximation when exact intractable

**Deliverables:**
- [ ] Problem where greedy and DP both work - compare
- [ ] Problem where greedy fails, DP needed
- [ ] Solve with multiple greedy strategies, compare results
- [ ] Implement greedy approximation algorithm
- [ ] Verify greedy solution with DP check

---

### ðŸ“… DAY 5: PROBLEM-SOLVING SEMINAR (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Greedy Problem-Solving Process**
  1. Understand problem structure
  2. Identify candidate greedy choice
  3. Test on examples (does it work?)
  4. Try to prove correctness (exchange argument)
  5. If proof fails, either fix greedy or use DP

- **Example Walkthroughs**
  - Activity selection: work through greedy vs all possibilities
  - Huffman: walk through tree building
  - Dijkstra: trace expansion order
  - All examples show greedy optimality

- **Red Flags for Non-Greedy Problems**
  - Problem mentions "optimal combination"
  - Multiple interacting constraints
  - Past choices affect future options unpredictably
  - Counterexample found easily

- **Red Flags for Greedy Problems**
  - Problem admits local choice obviously optimal
  - Structure guarantees local = global
  - Counterexample hard to find
  - Greedy matches problem intuition

- **Debugging Greedy Solutions**
  - Verify on small examples
  - Find failing case (if any)
  - Modify greedy choice or switch to DP
  - Test edge cases thoroughly

**Key Insights:**
- Greedy algorithm design is skill
- Practice on many examples essential
- Intuition developed through repetition

**Deliverables:**
- [ ] Solve 3 new greedy problems end-to-end
- [ ] Explain greedy choice for each
- [ ] Verify or disprove greedy on examples
- [ ] Write solution with clear comments
- [ ] Reflect on learning: when does greedy fail for you?

---

## ðŸ“Œ WEEK 13: BACKTRACKING, BRANCH & BOUND, AMORTIZED ANALYSIS

### Weekly Goal
Master backtracking for exhaustive search, branch & bound for optimization, and amortized analysis for complexity.

### Weekly Topics
- Backtracking framework and pruning
- Branch & bound for optimization
- Amortized complexity analysis
- Common backtracking patterns
- Recognizing when to use each paradigm

---

### ðŸ“… DAY 1: BACKTRACKING FUNDAMENTALS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Backtracking Idea**
  - Explore all possibilities systematically
  - When hit dead end, backtrack (undo) and try alternative
  - Exhaustive search with pruning
  - Time: exponential, but pruning reduces dramatically

- **Recursion Tree**
  - Each node = partial solution
  - Edge = choice (next step)
  - Leaves = complete or invalid solutions
  - DFS traversal = backtracking

- **Basic Backtracking Template**
  ```
  def backtrack(path, options):
    if is_complete(path):
      record solution
      return
    for choice in options:
      if is_valid(choice):
        path.add(choice)
        backtrack(path, remaining_options)
        path.remove(choice)  # backtrack
  ```

- **Pruning Strategies**
  - Constraint satisfaction: only valid choices
  - Bound pruning: stop if can't beat best so far
  - Early termination: found solution, stop searching
  - Symmetry breaking: avoid duplicate branches

- **N-Queens Problem**
  - Place n queens on nÃ—n board, no two attacking
  - Backtracking: place queens row by row
  - Pruning: skip placements that would attack
  - Time: O(n!) without pruning, much faster with

- **Sudoku Solver**
  - Fill 9Ã—9 grid with digits 1-9
  - Constraints: row, column, 3Ã—3 box unique
  - Backtracking: try digits, backtrack if invalid
  - Pruning: only try valid digits

- **Permutations & Combinations**
  - Generate all permutations: backtrack with used set
  - Generate all combinations: backtrack with start index
  - Pruning: none for generation, but can stop early

- **Subset Sum**
  - Find subset summing to target
  - Backtracking: include/exclude each element
  - Pruning: if sum exceeds target, stop
  - Time: O(2^n) without pruning, better with pruning

**Key Insights:**
- Backtracking = DFS with systematic exploration
- Pruning crucial for performance
- Framework generalizes across problems

**Deliverables:**
- [ ] Implement N-queens with backtracking
- [ ] Implement Sudoku solver
- [ ] Generate permutations and combinations
- [ ] Solve subset sum with pruning
- [ ] Analyze pruning effectiveness

---

### ðŸ“… DAY 2: ADVANCED BACKTRACKING PATTERNS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Letter Combinations of Phone Number**
  - Map digits to letters (phone keypad)
  - Generate all combinations
  - Backtracking: for each digit, try all letters
  - Time: O(4^n) where n = digits

- **Generate Parentheses**
  - Generate all valid parentheses combinations
  - n pairs: 2^(2n) possible, but only valid ones
  - Backtracking: track open count, only close if open > close
  - Pruning: skip invalid (too many closes, not enough opens)
  - Time: O(Catalan(n)) = O(4^n / sqrt(n))

- **Word Search**
  - Find word in 2D grid
  - Backtracking: DFS from each cell
  - Pruning: mark visited, skip if not matching next letter
  - Time: O(mn Ã— 4^k) where k = word length

- **Partition Problem Variants**
  - Partition array into subsets with constraints
  - Backtracking: assign each element to subset
  - Pruning: constraints violated
  - Example: partition equal sum, balanced partition

- **Palindrome Partitioning**
  - Partition string into all palindromic substrings
  - Backtracking: try all split points
  - Pruning: only valid palindromes (memoized check)
  - Time: O(2^n) with pruning

- **Combination Sum**
  - Find combinations summing to target
  - Unlimited use of numbers
  - Backtracking: for each number, include or skip
  - Pruning: sum exceeds target, stop
  - Variants: no repeats, each number once

- **Restore IP Addresses**
  - Restore dots in string to form valid IP address
  - Backtracking: place dots at valid positions
  - Pruning: invalid octet values
  - Time: O(3^4) = O(81) - small search space

- **Remove Invalid Parentheses**
  - Remove minimum characters to make valid
  - BFS/backtracking: explore removing characters
  - Pruning: seen states
  - Time: O(2^n) but pruning effective

- **Pattern Matching (Regex)**
  - . matches any character, * matches 0 or more
  - Backtracking: try all matches for pattern
  - Pruning: invalid pattern/text combinations
  - Advanced but same backtracking idea

**Key Insights:**
- Backtracking adapts to many problems
- Pruning different for each problem
- Constraints guide pruning strategy

**Deliverables:**
- [ ] Implement generate parentheses
- [ ] Implement word search in grid
- [ ] Implement palindrome partitioning
- [ ] Implement combination sum variants
- [ ] Solve custom backtracking problem

---

### ðŸ“… DAY 3: BRANCH & BOUND FOR OPTIMIZATION

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Branch & Bound Concept**
  - Systematic exploration of solution space
  - Branching: explore sub-spaces
  - Bounding: prune sub-spaces that can't improve best so far
  - Time: exponential worst-case, dramatic pruning in practice

- **Bounding Function**
  - Optimistic estimate: best possible solution in sub-space
  - If bound â‰¤ best so far: prune sub-space
  - Tighter bound â†’ more pruning
  - Computing tight bound is key skill

- **Traveling Salesman Problem (TSP)**
  - Find shortest tour visiting all cities
  - Branch: choose next city to visit
  - Bound: MST of remaining cities + current cost
  - Prune: if bound â‰¥ best so far
  - Time: exponential but practical for n â‰¤ 20-30

- **0/1 Knapsack via B&B**
  - Branch: include/exclude each item
  - Bound: fractional knapsack of remaining items + current value
  - Prune: if bound â‰¤ best so far
  - Compare to DP: B&B finds optimal with pruning

- **Job Assignment Problem**
  - Assign n jobs to n workers, minimize cost
  - Branch: assign next job to each worker
  - Bound: lower bound on remaining assignments (e.g., Hungarian)
  - Prune: if bound â‰¥ best so far

- **Maximum Clique**
  - Find largest clique in graph
  - Branch: include/exclude vertex
  - Bound: upper bound on maximum clique size (e.g., degree-based)
  - Prune: if bound â‰¤ current clique size

- **B&B vs DP**
  - B&B: prunes search space (no table needed)
  - DP: stores subproblem results (table)
  - B&B: better when search space sparse or bound tight
  - DP: better when many subproblems to solve

- **Best-First Search (BFS in B&B)**
  - Explore most promising node first
  - Use priority queue: nodes ordered by bound
  - Often faster than depth-first
  - More memory: store nodes in queue

**Key Insights:**
- Branch & bound extends backtracking with pruning
- Bounding function quality crucial
- Practical for problems where bound tight

**Deliverables:**
- [ ] Implement TSP with branch & bound
- [ ] Implement 0/1 knapsack with branch & bound
- [ ] Compare B&B with DP time/space
- [ ] Design bounding function for custom problem
- [ ] Solve using branch & bound

---

### ðŸ“… DAY 4: AMORTIZED ANALYSIS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Amortized Complexity**
  - Some operations expensive, many cheap
  - Average cost per operation over sequence: "amortized cost"
  - Different from average-case (input-dependent)
  - Guaranteed: holds for all sequences of operations

- **Dynamic Array Amortized Analysis**
  - Append operation: O(1) amortized
  - Some appends trigger resize: O(n) cost
  - But amortized: O(1) per append
  - Total n appends: n + 2n = 3n = O(n)
  - Average: 3n / n = O(1)

- **Accounting Method**
  - Assign "credit" (cost) to each operation
  - Cheap operations build up credits
  - Expensive operations "pay" from credits
  - If credits never go negative: amortized analysis valid

- **Potential Method**
  - Define potential function Î¦(state)
  - Amortized cost = actual cost + Î” Î¦
  - If sum of Î”Î¦ = 0: actual cost = total amortized cost
  - Potential "charges for" expensive operations

- **Example: Dynamic Array via Potential**
  - Potential: Î¦(n) = 2n - capacity
  - Append when space: cost 1, Î”Î¦ = 2, amortized = 3
  - Append resize: cost n (copy), Î”Î¦ = -n + 2, amortized = 1
  - Total amortized per append: O(1) 

- **Aggregate Analysis**
  - Analyze sequence of n operations directly
  - Total time: T(n)
  - Amortized: T(n) / n
  - Simpler than accounting/potential for some

- **Multi-Stack with Multipop**
  - k stacks, total n elements
  - Multipop: pop k elements from stack
  - Each element pushed once, popped once
  - Total cost: O(n) for n operations (including pop)
  - Amortized: O(1) per operation

- **Union-Find (Disjoint Set Union)**
  - Find with path compression: nearly O(1)
  - Precise analysis: O(Î±(n)) where Î± = inverse Ackermann
  - Practically: O(1)
  - Amortized over many operations

- **Splay Trees**
  - Search, insert, delete: O(log n) amortized
  - Individual operation might be O(n)
  - But amortized over sequence: O(log n)
  - Uses potential: amortized = actual + Î”Î¦

**Key Insights:**
- Amortized analysis proves average performance
- Accounting and potential methods complementary
- Crucial for data structures with varying costs

**Deliverables:**
- [ ] Analyze dynamic array amortized cost via potential method
- [ ] Analyze multi-stack multipop via aggregate analysis
- [ ] Prove union-find amortized O(Î±(n))
- [ ] Design data structure with amortized operations
- [ ] Prove amortized cost for custom structure

---

### ðŸ“… DAY 5: INTEGRATION & ALGORITHM CHOICE (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Choosing Algorithm Paradigm**
  - Greedy: local choice obvious, proof straightforward
  - DP: multiple choices, subproblems overlap
  - Backtracking: exhaustive search with pruning
  - Branch & Bound: exhaustive with bounding
  - Graph algorithms: problem structure suggests approach

- **Problem Structure Signals**
  - "Minimize", "maximize": DP or greedy
  - "Find all": backtracking
  - "Is it possible": SAT, backtracking, or graph search
  - "Count ways": DP or combinatorics
  - "Shortest path": BFS or Dijkstra
  - "Connected": DFS, union-find
  - "Optimize order": greedy or DP

- **Real Interviews**
  - Most problems have "obvious" first approach
  - Often not the best: ask interviewer to confirm
  - Suggest better approach after initial analysis
  - Show understanding of multiple approaches

- **Trade-off Considerations**
  - Time vs space: DP uses space for speed
  - Implementation complexity vs performance: greedy simplest
  - Worst-case vs average-case: different algorithms shine
  - Online vs offline: algorithms differ

- **Integration Challenges**
  - Problems combining multiple paradigms
  - Example: greedy choice with DP verification
  - Example: backtracking with branch & bound
  - Example: graph search with DP on nodes

- **Meta-Algorithm: How to Approach Unknown**
  1. Clarify problem thoroughly
  2. Start simple: brute force or obvious approach
  3. Analyze: time/space complexity
  4. Optimize: pattern recognition from earlier weeks
  5. Code: clean, correct implementation
  6. Test: small examples, edge cases
  7. Refine: based on feedback or testing

- **Common Mistakes to Avoid**
  - Over-engineering before understanding
  - Missing constraints that enable simpler solution
  - Not communicating approach before coding
  - Implementing without testing logic first

**Key Insights:**
- Algorithm choice comes from problem analysis
- Multiple approaches often exist
- Trade-offs guide final choice
- Practice builds intuition

**Deliverables:**
- [ ] Solve problem requiring algorithm choice
- [ ] Justify chosen approach vs alternatives
- [ ] Implement and test thoroughly
- [ ] Reflect on learning: which paradigm is your weakness?
- [ ] Set plan for continued improvement

---

# ðŸŸª PHASE E: INTEGRATION & EXTENSIONS (Weeks 14-15)

## Phase Motivation
You now have deep knowledge of core algorithms. This phase teaches specialized techniques and brings them together. Matrix operations, bit manipulation, number theory, network flow - tools for specific problem families.

## Phase Outcomes
- [ ] Solve matrix problems efficiently
- [ ] Master bit manipulation techniques
- [ ] Understand number theory for algorithmic problems
- [ ] Implement advanced graph algorithms (network flow, bipartite matching)
- [ ] Design systems solving complex integration problems

---

## ðŸ“Œ WEEK 14: MATRICES, BITMASKS & NUMBER THEORY

### Weekly Goal
Master matrix operations, bit techniques, and number theory for specialized problem families.

### Weekly Outcomes
- Solve matrix search and manipulation problems
- Use bitmask for efficient subset operations
- Apply number theory: GCD, primes, modular arithmetic
- Combine techniques for complex problems

### Summary
This week teaches tools for specific problem families. Students learn when and how to apply each technique.

---

### ðŸ“… DAY 1: MATRIX OPERATIONS & PROBLEMS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Matrix Search**
  - Sorted matrix: either row-sorted or 2D sorted
  - Staircase search: start top-right or bottom-left
  - Time: O(m + n)
  - Why: move right or left/down systematically

- **Matrix Traversals**
  - Row-wise: simple nested loops
  - Column-wise: access pattern matters for cache
  - Spiral: simulation or layer-by-layer
  - Diagonal: math or simulation
  - All O(m Ã— n)

- **Matrix Rotation**
  - 90Â° clockwise: transpose then reverse rows
  - In-place: layer-by-layer rotation
  - Or: mathematical position mapping
  - Time: O(m Ã— n)

- **Matrix Zeros**
  - Set row and column to zero if element is zero
  - Naive: O(m Ã— n) space
  - Smart: use first row/column as markers
  - O(1) space with careful flags
  - Time: O(m Ã— n)

- **Dungeon Game (Matrix with State)**
  - Find minimum health at start to survive path
  - Backward DP on matrix
  - State: minimum health needed at each position
  - Transition: dp[i][j] = max(1, min(dp[i+1][j], dp[i][j+1]) - dungeon[i][j])

- **Island Problems**
  - Number of islands: DFS/BFS/union-find
  - Largest island size: DFS tracking size
  - Island perimeter: count exposed edges
  - All variations on matrix connectivity

- **Cherry Pickup (Matrix with Constraints)**
  - Maximize cherries picked with round-trip
  - Reframe: two people going simultaneously
  - DP: dp[r1][c1][r2] where r2 implicit from steps
  - Time: O(nÂ³)

- **Matrix Chain Multiplication (Optimization)**
  - Find optimal order of multiplying matrices
  - DP: dp[i][j] = min operations to multiply matrices i..j
  - Time: O(nÂ³)

**Key Insights:**
- Matrix problems often simulate structure or optimize traversal
- Cache-friendly access pattern matters
- DP on 2D grids common and powerful

**Deliverables:**
- [ ] Implement staircase search in sorted matrix
- [ ] Implement in-place matrix rotation
- [ ] Implement matrix zeros with O(1) space
- [ ] Solve dungeon game
- [ ] Solve island or cherry pickup problem

---

### ðŸ“… DAY 2: BIT MANIPULATION TECHNIQUES

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Bit Operations**
  - AND (&): both 1
  - OR (|): either 1
  - XOR (^): exactly one 1
  - NOT (~): flip all
  - Shift left (<<): multiply by 2
  - Shift right (>>): divide by 2

- **Bit Tricks**
  - x & (x-1): removes rightmost set bit
  - x & -x: isolates rightmost set bit
  - x | (x+1): fills rightmost 0
  - Hamming weight: count set bits

- **Single Number**
  - Array where all numbers appear twice except one
  - XOR all: duplicate pairs cancel, single remains
  - Variant: all appear 3Ã— except one (use two bitmasks)
  - Time: O(n), Space: O(1)

- **Missing Number**
  - Array [0, n] with one missing
  - XOR all numbers 0 to n: missing number isolated
  - Or: sum formula
  - Both O(n) time, O(1) space

- **Bit Reversal**
  - Reverse bits of number
  - Iterative: check each bit, build reversed
  - Or use hardware trick
  - Time: O(log n) - number of bits

- **Number of 1 Bits**
  - Count set bits
  - x & (x-1) trick: removes one bit per iteration
  - Time: O(popcount) - number of 1s

- **Power of Two**
  - Check if number is power of 2
  - x & (x-1) == 0 iff power of 2
  - O(1)

- **Subset Generation**
  - Use bitmask to represent subsets
  - Iterate 0 to 2^n-1
  - Each bit: element included or not
  - O(2^n) subsets

- **Gray Code**
  - Binary code where consecutive numbers differ in 1 bit
  - Gray code = i XOR (i >> 1)
  - Or: build recursively
  - Application: minimize errors in transitions

- **Bit DP / Digit DP**
  - Problems on bit representations
  - Example: count numbers with certain property on bits
  - DP state: current bit position, constraints
  - Time: O(log n Ã— state_space)

**Key Insights:**
- Bit operations enable O(1) space techniques
- XOR powerful for paired elimination
- Bit tricks worth memorizing

**Deliverables:**
- [ ] Implement single number problem (all variants)
- [ ] Implement bit reversal
- [ ] Solve power of two check
- [ ] Count total hamming weight in range
- [ ] Solve bit DP problem

---

### ðŸ“… DAY 3: NUMBER THEORY IN ALGORITHMS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **GCD & LCM**
  - GCD: greatest common divisor (Euclidean algorithm)
  - gcd(a, b) = gcd(b, a % b), O(log min(a, b))
  - LCM: a Ã— b / gcd(a, b)
  - Application: fractions, coprime check

- **Prime Numbers**
  - Sieve of Eratosthenes: find all primes â‰¤ n
  - Time: O(n log log n)
  - Space: O(n)
  - Application: factorization, primality test

- **Primality Testing**
  - Trial division: check divisors up to sqrt(n), O(sqrt(n))
  - Miller-Rabin: probabilistic, fast
  - For interviews: trial division sufficient usually

- **Prime Factorization**
  - Factor n into prime factors
  - Divide by 2 repeatedly, then odd numbers
  - Time: O(sqrt(n))
  - Applications: divisor counting, totient function

- **Modular Arithmetic**
  - (a + b) mod m = ((a mod m) + (b mod m)) mod m
  - (a Ã— b) mod m = ((a mod m) Ã— (b mod m)) mod m
  - a^b mod m: use fast exponentiation
  - Application: prevent overflow, cryptography

- **Fast Exponentiation**
  - a^b mod m efficiently
  - Binary exponentiation: O(log b)
  - Squaring trick: a^b = (a^2)^(b/2) if b even
  - Essential for modular arithmetic

- **Totient Function**
  - Ï†(n): count numbers â‰¤ n coprime to n
  - Ï†(n) = n Ã— âˆ(1 - 1/p) for primes p dividing n
  - Application: Euler's theorem, modular inverse

- **Modular Inverse**
  - a^(-1) mod m: number x where aÃ—x â‰¡ 1 (mod m)
  - Exists iff gcd(a, m) = 1
  - Extended Euclidean algorithm: find x, y where ax + my = gcd(a, m)
  - a^(-1) â‰¡ x (mod m)

- **Chinese Remainder Theorem**
  - Solve system of congruences
  - If moduli coprime: unique solution mod product
  - Application: fast arithmetic on large numbers

- **Combinatorics Modulo**
  - C(n, k) mod p: use Lucas theorem or factorial mod
  - Factorial mod p: precompute or use Wilson's theorem
  - Application: count combinations modulo prime

**Key Insights:**
- Number theory enables efficient solutions for many problems
- Modular arithmetic prevents overflow
- Prime factorization builds many solutions

**Deliverables:**
- [ ] Implement Sieve of Eratosthenes
- [ ] Implement GCD and LCM
- [ ] Implement fast modular exponentiation
- [ ] Solve problem requiring prime factorization
- [ ] Solve modular arithmetic problem

---

### ðŸ“… DAY 4: ADVANCED GRAPH ALGORITHMS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Network Flow**
  - Flow network: edges have capacity
  - Find maximum flow from source to sink
  - Ford-Fulkerson: augmenting paths, O(E Ã— max_flow)
  - Edmonds-Karp: BFS for augmenting paths, O(V Ã— EÂ²)
  - Application: matching, resource allocation

- **Bipartite Matching**
  - Match left nodes to right nodes (1-1)
  - Max matching: maximum number of edges
  - Use max flow: source connects to left, right to sink, capacity 1
  - Or Hungarian algorithm for weighted matching

- **Minimum Cost Flow**
  - Flow with costs on edges
  - Minimize total cost while achieving flow requirement
  - Use successive shortest paths or cost-scaling
  - Time: O(VÂ² Ã— E log V)

- **Strongly Connected Components (SCC)**
  - Kosaraju: two DFS passes, O(V + E)
  - Tarjan: single DFS pass, O(V + E)
  - Application: graph condensation, recursive structure

- **Articulation Points & Bridges**
  - Articulation point: removal disconnects graph
  - Bridge: edge removal disconnects graph
  - DFS-based detection, O(V + E)
  - Application: network reliability

- **2-SAT Problem**
  - Boolean satisfiability with 2-literal clauses
  - Graph-based solution using implications
  - Build implication graph, find SCCs
  - Solvable in polynomial time

- **Minimum Vertex Cover**
  - Smallest set of vertices covering all edges
  - NP-hard in general
  - Polynomial for bipartite (max matching)
  - Approximation: greedy 2-approximation

- **Maximum Independent Set (Graph)**
  - Largest set of non-adjacent vertices
  - NP-hard in general
  - Polynomial for trees (DP)
  - Related to vertex cover: IV = V - vertex cover

**Key Insights:**
- Network flow solves many matching/allocation problems
- Graph structure (SCC, bridges) enables efficient algorithms
- Some problems NP-hard but approximable or solvable on special graphs

**Deliverables:**
- [ ] Implement maximum flow (Ford-Fulkerson or Edmonds-Karp)
- [ ] Implement bipartite matching via flow
- [ ] Implement SCC detection
- [ ] Solve 2-SAT problem
- [ ] Solve problem requiring graph analysis

---

### ðŸ“… DAY 5: INTEGRATION WORKSHOP (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Combining Techniques**
  - Matrix + DP: grid problems
  - Bit manipulation + bit DP: subset counting
  - Number theory + modular arithmetic: counting modulo prime
  - Graphs + DP: path counting, optimal routes

- **Complex Problem Walkthrough**
  - Problem: multi-layered constraints
  - Analysis: which techniques apply?
  - Design: combine techniques
  - Implementation: clean integration
  - Testing: verify on examples

- **Trade-offs**
  - Time vs space in matrix operations
  - Exact vs approximate for NP-hard
  - Online vs offline algorithms
  - Worst-case vs average-case

- **Performance Tuning**
  - Bitwise operations for speed
  - Sieve precomputation for queries
  - Caching/memoization for repeated computations

**Key Insights:**
- Mastery is combining techniques appropriately
- No single "best" approach - context-dependent
- Practice on integration problems essential

**Deliverables:**
- [ ] Solve 2-3 integration problems end-to-end
- [ ] Use matrix, bits, and number theory
- [ ] Implement with clean code
- [ ] Test thoroughly on examples and edges
- [ ] Document approach and trade-offs

---

## ðŸ“Œ WEEK 15: ADVANCED STRINGS & SYSTEM INTEGRATION

### Weekly Goal
Master advanced string algorithms. Design complete systems integrating multiple techniques.

### Weekly Topics
- String matching algorithms (KMP, Z-algorithm)
- Network flow applications
- System design integrating DSA
- Performance optimization
- Real-world problem solving

---

### ðŸ“… DAY 1: ADVANCED STRING MATCHING

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Knuth-Morris-Pratt (KMP)**
  - Pattern matching: find pattern in text
  - Naive: O((n-m) Ã— m)
  - KMP: O(n + m) with failure function
  - Failure function: longest proper prefix that is suffix
  - Precompute: when mismatch, jump by failure value
  - Application: pattern search, text processing

- **Boyer-Moore Algorithm**
  - Start from pattern end, move backward
  - On mismatch: skip based on bad character rule or good suffix rule
  - Average: O(n / m), best: O(n / m)
  - Worst-case: O(n Ã— m) but rarely happens
  - Often faster than KMP in practice

- **Z-Algorithm**
  - Compute Z-array: length of longest prefix equal to substring starting at i
  - Z-pattern matching: concatenate pattern, text with separator
  - O(n + m)
  - Useful for multiple pattern occurrences

- **Aho-Corasick Algorithm**
  - Multiple pattern matching: find all occurrences of multiple patterns
  - Build trie of patterns
  - Precompute failure links (like KMP but on trie)
  - Process text once: O(n + m + z) where z = occurrences
  - Application: virus scanning, content filtering

- **Suffix Array & Suffix Tree**
  - Suffix array: sorted array of suffixes (space-efficient)
  - Suffix tree: trie of all suffixes (fast queries)
  - Construction: O(n log n) array, O(n) tree (complex)
  - LCP array: longest common prefix between consecutive suffixes
  - Application: string matching, palindromes, repeated substrings

- **Manacher's Algorithm**
  - Find longest palindromic substring in O(n)
  - Maintain center of rightmost palindrome found
  - Use symmetry to avoid redundant checks
  - Key insight: expand intelligently using previous work
  - Time: O(n), Space: O(n)

- **Palindromic Subsequence**
  - Longest palindromic subsequence: DP or reverse-compare
  - Unique palindromic subsequences: DP with complexity
  - Related to edit distance (delete minimum to make palindrome)

**Key Insights:**
- KMP and Z-algorithm O(n + m)
- Boyer-Moore often faster in practice
- Aho-Corasick for multiple patterns
- Manacher's algorithm elegant for palindromes

**Deliverables:**
- [ ] Implement KMP algorithm
- [ ] Implement Z-algorithm
- [ ] Implement Aho-Corasick for multiple patterns
- [ ] Implement Manacher's algorithm
- [ ] Solve complex string problem using appropriate technique

---

### ðŸ“… DAY 2: SYSTEM INTEGRATION I - COMBINING ALGORITHMS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Design Pattern: Algorithm Choice**
  - Input analysis: determine applicable algorithms
  - Time/space constraints: choose trade-off
  - Implement: primary algorithm
  - Optimize: if necessary

- **Design Pattern: Preprocessing**
  - Sort: enables two-pointer, binary search
  - Build hash table: enables O(1) lookups
  - Precompute: sieve, factorization, matrix powers
  - Build graph: enables graph algorithms

- **Design Pattern: Decomposition**
  - Break problem into subproblems
  - Subproblem: solved by different algorithm
  - Combine solutions
  - Example: knapsack with multiple constraints

- **Design Pattern: Verification**
  - Primary: fast approximate solution
  - Verify: slower exact algorithm checks
  - Confidence: compare and test

- **Example System 1: Recommendation Engine**
  - Problem: recommend K products to user based on preferences
  - Decomposition:
    - User similarity: cosine distance (linear algebra)
    - Similar users' preferences: aggregation
    - Rank recommendations: heap/sorting
    - Time complexity: O(n Ã— m + n log K)

- **Example System 2: Route Planner**
  - Problem: find best route from A to B with constraints
  - Decomposition:
    - Build graph: city/road network
    - Shortest path: Dijkstra (primary)
    - Verify: alternative route checking
    - Optimize: highway preferences (weighted graph)
    - Time: O((V + E) log V)

- **Example System 3: Database Query Optimizer**
  - Problem: execute query efficiently on data
  - Decomposition:
    - Parse query: string processing
    - Build execution plan: DP on query tree
    - Execute operations: various algorithms (sort, join, aggregate)
    - Time: depends on operations

- **Performance Profiling**
  - Identify bottleneck: which part dominates time?
  - Optimize: apply faster algorithm or reduce constant
  - Test: verify improvement

**Key Insights:**
- System design integrates multiple algorithms
- Preprocessing crucial
- Algorithm choice driven by constraints and data

**Deliverables:**
- [ ] Design recommendation engine component
- [ ] Design route planner component
- [ ] Integrate multiple algorithms
- [ ] Profile performance and optimize
- [ ] Document design decisions

---

### ðŸ“… DAY 3: SYSTEM INTEGRATION II - ADVANCED PROBLEMS

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Example System 4: Real-Time Chat**
  - Problem: handle messages, detect trends, notify users
  - Components:
    - Message storage: database (outside DSA scope)
    - Message processing: stream processing
    - Trend detection: frequent pattern mining
    - Notification: priority queue, BFS on social graph
    - Time: streaming algorithms, O(1) per message

- **Example System 5: File Deduplication**
  - Problem: identify duplicate files efficiently
  - Approach:
    - Compute hashes: rolling hash or cryptographic
    - Group by hash: hash table
    - Compare candidates: edit distance or content match
    - Time: O(n Ã— k log k) for files of length k

- **Example System 6: Spell Checker**
  - Problem: suggest corrections for misspelled words
  - Approach:
    - Build dictionary: trie or hash table
    - For misspelled word: generate candidates (edit distance â‰¤ 1)
    - Rank candidates: frequency, edit distance
    - Return top K: heap
    - Time: O(alphabet_size Ã— word_length + K log N)

- **Example System 7: Range Query**
  - Problem: answer range sum/min/max queries efficiently
  - Approach:
    - Preprocess: build segment tree or sparse table
    - Query: O(log n) for segment tree, O(1) for sparse table
    - Update: O(log n) for segment tree, O(log n log n) for sparse table
    - Choose based on update frequency

- **Example System 8: Compression Algorithm**
  - Problem: compress data efficiently
  - Approach:
    - Huffman coding: build optimal prefix tree
    - Encode: map characters to codes
    - Decode: traverse tree
    - Time: O(n log n) build, O(n) encode/decode

**Key Insights:**
- Real systems combine many algorithms
- Choice depends on constraints and requirements
- Performance testing validates design

**Deliverables:**
- [ ] Design system 4: real-time chat
- [ ] Design system 5: file deduplication
- [ ] Design system 6: spell checker
- [ ] Choose appropriate algorithms for each component
- [ ] Estimate time/space complexity for whole system

---

### ðŸ“… DAY 4: PERFORMANCE OPTIMIZATION & TUNING

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Identifying Bottlenecks**
  - Profiling: measure time per operation
  - Algorithmic complexity: theoretical bounds
  - Practical constants: matter on real hardware
  - Examples: cache effects, memory allocation

- **Algorithmic Optimizations**
  - Reduce complexity class: O(nÂ²) â†’ O(n log n)
  - Improve constants: tighter bounds
  - Pruning: skip unnecessary computation
  - Caching: avoid recomputation

- **Data Structure Optimizations**
  - Choose right structure: array vs linked list
  - Compact representation: bit packing
  - Cache-friendly layout: spatial locality
  - Avoid allocations: reuse storage

- **Practical Optimizations**
  - Early termination: found answer, stop
  - Batch operations: reduce function call overhead
  - Vectorization: SIMD operations
  - Parallelization: multiple threads (advanced)

- **Memory Optimization**
  - Space complexity: O(n) vs O(nÂ²)
  - Allocator efficiency: fewer larger allocations
  - Cache hierarchy: L1/L2/L3 considerations
  - Memory layout: struct padding

- **Trade-offs**
  - Time vs space: memoization uses space for time
  - Implementation complexity vs performance
  - Worst-case vs average-case
  - One algorithm vs hybrid

- **Benchmarking**
  - Measure performance on realistic data
  - Compare alternatives: which wins?
  - Scale: performance at different sizes
  - Document: why this choice?

**Key Insights:**
- Optimization is iterative: profile, optimize, re-test
- Constants matter: O(n) with factor 100 worse than O(n log n) for small n
- Premature optimization is root of evil - but profile-guided optimization is not

**Deliverables:**
- [ ] Profile algorithm: measure bottleneck
- [ ] Optimize: apply technique
- [ ] Re-test: verify improvement
- [ ] Document: before/after comparison
- [ ] Analyze: why improvement works?

---

### ðŸ“… DAY 5: CAPSTONE PROJECT (COMPLETE)

**Duration:** 90 minutes

**Topics & Subtopics:**

- **Capstone Project Structure**
  1. Problem definition: clear specifications
  2. Analysis: complexity, constraints
  3. Algorithm design: choose techniques
  4. Implementation: clean, tested code
  5. Optimization: performance tuning
  6. Documentation: design decisions
  7. Presentation: explain approach

- **Example Capstone 1: Social Network**
  - Build social graph: add users, connections
  - Query: shortest path between users (social distance)
  - Analysis: connected components, influencers (high degree)
  - Recommendation: friends-of-friends
  - Algorithms: BFS, DFS, degree tracking
  - Time: O(V + E) per query

- **Example Capstone 2: Scheduling System**
  - Problem: schedule tasks with precedence and resources
  - Constraints: task duration, dependencies, resource limits
  - Goal: minimize completion time
  - Algorithms: topological sort, critical path (longest path in DAG), greedy assignment
  - Time: O(V + E)

- **Example Capstone 3: Data Compression**
  - Build compression system: Huffman or LZ77
  - Encode: transform data using algorithm
  - Decode: reverse process
  - Measure: compression ratio on various inputs
  - Algorithms: tree construction, bit packing, streaming
  - Time: O(n log n)

- **Example Capstone 4: Search Engine**
  - Index documents: build data structure for fast search
  - Query: find documents matching pattern
  - Rank: relevance scoring
  - Optimize: memory and query time
  - Algorithms: trie, suffix array, TF-IDF
  - Time: O(log n) queries

- **Presentation Skills**
  - Explain problem clearly: what are we solving?
  - Discuss approach: why this algorithm?
  - Show code: walk through key parts
  - Demonstrate: run on examples
  - Reflect: what learned, what could improve?

- **Reflection & Learning**
  - Which algorithms used most?
  - Where did complexity matter?
  - What optimizations were crucial?
  - How would scale to bigger data?
  - What's your weakness to address?

**Key Insights:**
- Capstone ties together all 15 weeks
- Problem-solving skills essential
- Communication and iteration key to success

**Deliverables:**
- [ ] Choose capstone project
- [ ] Implement complete system
- [ ] Profile and optimize
- [ ] Document design and decisions
- [ ] Present project with code walkthrough

---

# ðŸŸ« PHASE F & ðŸ”´ PHASE G: ADVANCED & INTERVIEWS

*(Abbreviated - see full detailed syllabus for complete content)*

---

**End of PHASE C, D, E Continuation Document**

This completes all the missing content from Week 10 onwards through Week 15 (end of Phase E), including the full detail for all days in Weeks 10-11 (DP), Weeks 12-13 (Paradigms), and Weeks 14-15 (Integration & Extensions).

**Total Status:**
- âœ… PHASE A (Weeks 1-3): COMPLETE
- âœ… PHASE B (Weeks 4-6): COMPLETE  
- âœ… PHASE C (Weeks 7-11): COMPLETE
- âœ… PHASE D (Weeks 12-13): COMPLETE
- âœ… PHASE E (Weeks 14-15): COMPLETE
- ðŸŸ« PHASE F (Weeks 16-18): Advanced Deep Dives (available in original detailed file)
- ðŸ”´ PHASE G (Week 19): Mock Interviews & Final Review (available in original detailed file)

---

*Professional Data Structures & Algorithms Complete Curriculum v13*
*Full 19-Week Complete Syllabus with All Details*
*January 25, 2026*
