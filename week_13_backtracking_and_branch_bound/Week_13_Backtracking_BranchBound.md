# ğŸŸ§ WEEK 13: BACKTRACKING & BRANCH & BOUND

**Duration:** 5 days | 450 minutes | Advanced Algorithm Paradigms  
**Focus:** Backtracking for combinatorial problems, Branch & Bound for optimization, Amortized analysis  
**Prerequisites:** Recursion, DFS, greedy algorithms, complexity analysis  

---

## ğŸ¯ WEEK 13 LEARNING OUTCOMES

By end of Week 13, you will be able to:

1. **Design backtracking algorithms** with proper pruning strategies
2. **Solve N-Queens, Sudoku, Maze** problems systematically
3. **Apply Branch & Bound** to optimization problems
4. **Analyze amortized complexity** using accounting and potential methods
5. **Combine paradigms** (greedy + backtracking, DP + B&B)
6. **Recognize when to use** each algorithmic paradigm

---

---

## ğŸ“… DAY 1: BACKTRACKING FUNDAMENTALS

### â±ï¸ Duration: 90 minutes

---

## ğŸ“ CONCEPT: What is Backtracking?

### Core Idea

**Backtracking:** Build solution incrementally, one piece at a time. If current piece leads to invalid state, undo it and try alternative.

Think of it like solving a maze:

```
Exploring a Maze:

START â—‹
  â”‚
  â”œâ”€â†’ Dead end âœ— â†’ BACKTRACK
  â”‚
  â”œâ”€â†’ Path continues
  â”‚    â”‚
  â”‚    â”œâ”€â†’ Dead end âœ— â†’ BACKTRACK
  â”‚    â”‚
  â”‚    â””â”€â†’ Path continues
  â”‚         â”‚
  â”‚         â””â”€â†’ GOAL! â—‹ SUCCESS

Key: When stuck, undo recent choices and try alternatives
```

### Relationship to DFS

```
BACKTRACKING = DFS on Solution Space Tree

Example: Permutations of [1, 2, 3]

Solution Tree:
                    root (empty)
           /          |          \
         [1]        [2]         [3]
        /  \        /  \        /  \
      [1,2] [1,3] [2,1] [2,3] [3,1] [3,2]
       |     |     |     |     |     |
     [1,2,3][1,3,2][2,1,3][2,3,1][3,1,2][3,2,1]

DFS traversal visits all leaves = all permutations
Backtracking naturally prunes invalid branches
```

---

## ğŸ“ CONCEPT: State Space vs Solution Space

### Two Different Views

```
STATE SPACE: All possible problem states

Example: N-Queens board
State = current board configuration (with some queens placed)

Solution Space: Only valid final states

Example: N-Queens solution
Solution = board with N queens placed, no conflicts

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Solution Space Tree:                 â”‚
â”‚                                      â”‚
â”‚ Root: Empty board (0 queens)         â”‚
â”‚   â”œâ”€ Queen at row 1, col 1          â”‚
â”‚   â”‚   â”œâ”€ Queen at row 2, col 3      â”‚
â”‚   â”‚   â”‚   â”œâ”€ Queen at row 3, col 5  â”‚
â”‚   â”‚   â”‚   â”‚   ...                    â”‚
â”‚   â”‚   â”‚   â””â”€ Conflict â†’ Prune        â”‚
â”‚   â”‚   â””â”€ Queen at row 2, col 4      â”‚
â”‚   â”‚       ...                        â”‚
â”‚   â””â”€ Queen at row 1, col 2          â”‚
â”‚       ...                            â”‚
â”‚                                      â”‚
â”‚ Leaves: Valid complete solutions    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ CONCEPT: Backtracking Template

### Generic Structure

```
BACKTRACKING_TEMPLATE:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKTRACK(state, remaining choices):   â”‚
â”‚                                        â”‚
â”‚  1. IF state is complete solution:    â”‚
â”‚     Record solution                    â”‚
â”‚     Return                             â”‚
â”‚                                        â”‚
â”‚  2. IF state is invalid:               â”‚
â”‚     Return (prune this branch)         â”‚
â”‚                                        â”‚
â”‚  3. FOR each possible next choice:     â”‚
â”‚     a) Add choice to state             â”‚
â”‚     b) Recursively explore            â”‚
â”‚     c) Remove choice from state        â”‚
â”‚        (backtrack)                     â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

```
1. STATE: Current partial solution
   - What constraints are satisfied?
   - What variables assigned?
   - What's left to assign?

2. CHOICES: Possible next decisions
   - What values can next variable take?
   - Are some choices prunable?

3. CONSTRAINTS: Validity rules
   - Which partial states are invalid?
   - When to prune?

4. BASE CASE: Complete solution
   - When is state complete?
   - Did we reach goal?
```

---

## ğŸ“ CONCEPT: Pruning (Critical for Efficiency)

### Why Pruning Matters

```
WITHOUT PRUNING:
Exponential search space explored completely
â†’ Very slow for large n

Example: Permutations of n elements
Without pruning: Explore all n! paths
With pruning: Skip invalid branches early

Speedup: Can be dramatic (100x+ for reasonable n)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pruning Strategy:                    â”‚
â”‚                                      â”‚
â”‚ âœ— = Invalid state                    â”‚
â”‚                                      â”‚
â”‚         root                         â”‚
â”‚        / | \                         â”‚
â”‚       A  B  C                        â”‚
â”‚      / \ âœ—  / \                      â”‚
â”‚     AA AB   CA CB                    â”‚
â”‚    /| âœ—      âœ—  âœ—                    â”‚
â”‚   AAA AAB    (pruned!)               â”‚
â”‚                                      â”‚
â”‚ Without pruning: Explore all        â”‚
â”‚ With pruning: Stop at âœ—             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Types of Pruning

```
1. VALIDITY PRUNING: Constraint violation
   "This partial solution violates constraint"
   Example: Two queens on same row in N-Queens
   
   if (isInvalid(current_state)):
       return  // Skip this entire branch

2. OPTIMALITY PRUNING: Lower/upper bounds (Branch & Bound)
   "This branch can't possibly beat current best"
   Example: If partial value < best found, skip
   
   if (lowerBound(partial) < bestSoFar):
       return  // Skip this branch

3. REDUNDANCY PRUNING: Equivalent states
   "This state is equivalent to one we've seen"
   Example: Using symmetry
   
   if (isRedundant(state)):
       return  // Skip this branch
```

---

## ğŸ“Š Visual: Backtracking Execution Tree

```
DEPTH-FIRST TRAVERSAL with BACKTRACKING:

DFS Order (with pruning):

                root
              /  |  \
             1   2   3
           / \ / \ / \
          a  b c d e f

Visit: root â†’ 1 â†’ a â†’ (leaf)
       back to: 1 â†’ b â†’ (leaf)
       back to: 1 â†’ (done with 1)
       back to: root â†’ 2 â†’ c (pruned, skip d)
       back to: 2 â†’ (done with 2)
       back to: root â†’ 3 â†’ e â†’ (leaf)
       back to: 3 â†’ f â†’ (leaf)

Each branch that's pruned
saves exploring subtree
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 1

| Concept | Key Point |
|---------|-----------|
| **Backtracking** | Build solution incrementally, undo when stuck |
| **Solution Tree** | DFS on state space tree |
| **State** | Current partial solution |
| **Base Case** | Solution found or invalid |
| **Pruning** | Skip entire subtree if invalid |
| **Efficiency** | Pruning can reduce exponential to manageable |

---

---

## ğŸ“… DAY 2: BACKTRACKING PROBLEMS

### â±ï¸ Duration: 120 minutes

---

## ğŸ“ CONCEPT: N-Queens Problem

### Problem Statement

```
N-QUEENS PROBLEM:

Place n queens on n Ã— n chessboard such that:
- No two queens on same row
- No two queens on same column
- No two queens on same diagonal

Example (4-Queens):
   0 1 2 3
0  . Q . .
1  . . . Q
2  Q . . .
3  . . Q .

No conflicts! âœ“

Goal: Find all valid placements (or one solution)
```

### Solution Representation

```
APPROACH: Place one queen per row

State = [colâ‚€, colâ‚, colâ‚‚, ..., col_{n-1}]
where coláµ¢ = column of queen in row i

Example (4-Queens):
State = [1, 3, 0, 2]
Meaning:
  Row 0: Queen at column 1
  Row 1: Queen at column 3
  Row 2: Queen at column 0
  Row 3: Queen at column 2

Why one per row?
- Guarantees no row conflicts (by design)
- Reduces search space from nÂ² positions to n^n states
  (instead of n^(nÂ²))
```

### Algorithm

```
N_QUEENS(n):
  solutions = []
  state = []
  
  BACKTRACK(0, state, solutions):
    1. If placed n queens (state length = n):
       Record state as solution
       Return
    
    2. For each column 0 to n-1:
       a) Is this column safe (no conflicts)?
          - Check column conflicts
          - Check diagonal conflicts
       
       b) If safe:
          - Place queen at (current_row, column)
          - Add column to state
          - Recursively solve for next row
          - Remove column from state (backtrack)
       
       c) If not safe:
          - Skip to next column

  Call: BACKTRACK(0, [], solutions)
```

### Step-by-Step Example (4-Queens)

```
Initial: Row 0, State = []

Row 0:
  Try Col 0: Safe? Yes â†’ State = [0]
    
    Row 1, State = [0]:
      Try Col 0: Same column as queen above â†’ Prune
      Try Col 1: Adjacent diagonal â†’ Prune
      Try Col 2: Safe? Yes â†’ State = [0, 2]
      
        Row 2, State = [0, 2]:
          Try Col 0: Same column â†’ Prune
          Try Col 1: Safe? Yes â†’ State = [0, 2, 1]
          
            Row 3, State = [0, 2, 1]:
              Try Col 0: Same column â†’ Prune
              Try Col 1: Same column â†’ Prune
              Try Col 2: Same column â†’ Prune
              Try Col 3: Safe? Yes â†’ State = [0, 2, 1, 3]
              
                Row 4 = n, solution found! âœ“
                But wait, let's verify [0,2,1,3]:
                
                Visual:
                   0 1 2 3
                0  Q . . .
                1  . . Q .
                2  . Q . .
                3  . . . Q
                
                Check conflicts:
                Row conflicts: None (one per row) âœ“
                Column conflicts: None (all different) âœ“
                Diagonal conflicts: None âœ“
                
                SOLUTION #1: [0, 2, 1, 3]
              
              Backtrack from [0, 2, 1, 3]
              
          Backtrack from [0, 2, 1]
          Try Col 2, 3, ... (more attempts)
      
      Backtrack from [0, 2]
      Try Col 3: Safe? Yes â†’ State = [0, 3]
      ... (continue exploring)
  
  Backtrack from [0]
  Try Col 1, 2, 3: ... (more attempts)

Continue until all possibilities exhausted
```

### Conflict Detection

```
DIAGONAL CONFLICT CHECK:

Two queens conflict if they're on same diagonal:
- Main diagonal: row - col = constant
- Anti-diagonal: row + col = constant

For queen at (row1, col1) and (row2, col2):
if (row1 - col1 == row2 - col2):
    Same main diagonal conflict âœ—

if (row1 + col1 == row2 + col2):
    Same anti-diagonal conflict âœ—

Example: Queen at (0, 0) and queen at (2, 2)
  row - col: 0 - 0 = 0, 2 - 2 = 0 â†’ Same! Conflict âœ—
  
Example: Queen at (0, 1) and queen at (2, 3)
  row - col: 0 - 1 = -1, 2 - 3 = -1 â†’ Same! Conflict âœ—
  row + col: 0 + 1 = 1, 2 + 3 = 5 â†’ Different â†’ OK
```

---

## ğŸ“ CONCEPT: Sudoku Solver

### Problem Structure

```
SUDOKU PUZZLE:

9 Ã— 9 grid, divided into 3 Ã— 3 boxes

Constraints:
- Each row: digits 1-9 exactly once
- Each column: digits 1-9 exactly once
- Each 3Ã—3 box: digits 1-9 exactly once

â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚5 3â”‚   â”‚   â”‚
â”‚6  â”‚ 9 â”‚ 8 â”‚
â”‚  9â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚6  â”‚   â”‚
â”‚ 4 â”‚ 8 â”‚ 3 â”‚
â”‚ 7 â”‚   â”‚ 6 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚6  â”‚
â”‚ 3 â”‚ 9 â”‚ 4 â”‚
â”‚   â”‚   â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Goal: Fill empty cells to satisfy constraints
```

### Backtracking Approach

```
SUDOKU_SOLVE():

1. Find first empty cell (row, col)
2. If no empty cell â†’ Puzzle solved! Return true
3. For each digit 1-9:
   a) Is digit valid at (row, col)?
      - Check row: digit not already present
      - Check column: digit not already present
      - Check 3Ã—3 box: digit not already present
   
   b) If valid:
      - Place digit
      - Recursively solve rest
      - If recursive call succeeds â†’ Return true
      - If recursive call fails â†’ Remove digit (backtrack)
   
   c) If not valid:
      - Skip this digit

4. If all digits tried and none work â†’ Return false
```

### Key Optimization: Constraint Propagation

```
NAIVE BACKTRACKING: Try all 1-9 for each cell
- Explores many dead ends

SMARTER APPROACH: Constraint Propagation
- Before trying numbers, reduce possibilities
- Cell can only have digits that don't violate constraints

Example:
Cell (0,0) already has 5, 6, 9 in its row
Can only try: 1, 2, 3, 4, 7, 8
Reduces from 9 tries to 6 tries âœ“

With column and box constraints:
Further reduces possibilities
Some cells may have only 1 valid choice!
```

---

## ğŸ“ CONCEPT: Permutations & Combinations

### Permutations (Order Matters)

```
PERMUTATION BACKTRACKING:

Generate all orderings of a set

Example: Permutations of [1, 2, 3]
Result: [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PERMUTE(available, current):         â”‚
â”‚                                      â”‚
â”‚ 1. If available is empty:            â”‚
â”‚    Record current as permutation     â”‚
â”‚    Return                            â”‚
â”‚                                      â”‚
â”‚ 2. For each element in available:    â”‚
â”‚    a) Add element to current         â”‚
â”‚    b) Remove element from available  â”‚
â”‚    c) Recursively permute            â”‚
â”‚    d) Restore element to available   â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example execution:
Start: available=[1,2,3], current=[]
  Pick 1: available=[2,3], current=[1]
    Pick 2: available=[3], current=[1,2]
      Pick 3: available=[], current=[1,2,3] â†’ Solution!
    Pick 3: available=[2], current=[1,3]
      Pick 2: available=[], current=[1,3,2] â†’ Solution!
  Pick 2: available=[1,3], current=[2]
    ... (similar)
  Pick 3: available=[1,2], current=[3]
    ... (similar)
```

### Combinations (Order Doesn't Matter)

```
COMBINATION BACKTRACKING:

Generate all subsets of size k

Example: Combinations of [1,2,3,4] choose 2
Result: [1,2], [1,3], [1,4], [2,3], [2,4], [3,4]

Key Difference from Permutations:
- Avoid duplicates by only considering elements after current

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMBINE(start_idx, remaining, curr): â”‚
â”‚                                      â”‚
â”‚ 1. If remaining == 0:                â”‚
â”‚    Record current as combination     â”‚
â”‚    Return                            â”‚
â”‚                                      â”‚
â”‚ 2. For i = start_idx to n-1:         â”‚
â”‚    a) Add array[i] to current        â”‚
â”‚    b) Recursively combine from i+1   â”‚
â”‚    c) Remove array[i] from current   â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example execution:
Start: idx=0, remaining=2, current=[]
  Pick 1 (idx=0): current=[1], remaining=1
    Pick 2 (idx=1): current=[1,2], remaining=0 â†’ Solution!
    Pick 3 (idx=2): current=[1,3], remaining=0 â†’ Solution!
    Pick 4 (idx=3): current=[1,4], remaining=0 â†’ Solution!
  Pick 2 (idx=1): current=[2], remaining=1
    Pick 3 (idx=2): current=[2,3], remaining=0 â†’ Solution!
    Pick 4 (idx=3): current=[2,4], remaining=0 â†’ Solution!
  Pick 3 (idx=2): current=[3], remaining=1
    Pick 4 (idx=3): current=[3,4], remaining=0 â†’ Solution!
```

---

## ğŸ“ CONCEPT: Word Search & Maze Problems

### Word Search

```
WORD SEARCH PROBLEM:

Find if a word exists in grid
- Horizontally, vertically, or diagonally
- Adjacent cells only (no jumping)
- Each cell used at most once per word search

Visual:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚C A Tâ”‚
â”‚O D Râ”‚
â”‚G E Dâ”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Search: "CAT"
Path: C(0,0) â†’ A(0,1) â†’ T(0,2) â†’ Found! âœ“

Search: "CODE"
Path: C(0,0) â†’ O(1,0) â†’ D(1,1) â†’ E(2,1) â†’ Found! âœ“

Search: "CAD"
Path: C(0,0) â†’ A(0,1) â†’ D(1,1) â†’ Found! âœ“

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORD_SEARCH(board, word):            â”‚
â”‚                                      â”‚
â”‚ 1. For each cell in board:           â”‚
â”‚    IF cell matches word[0]:          â”‚
â”‚      Start DFS from this cell        â”‚
â”‚      Try to find rest of word        â”‚
â”‚                                      â”‚
â”‚ DFS(row, col, word_index, visited):  â”‚
â”‚   1. If word_index == word.length:   â”‚
â”‚      Found entire word! Return true  â”‚
â”‚   2. For each adjacent cell:         â”‚
â”‚      IF not visited AND              â”‚
â”‚         cell == word[word_index]:    â”‚
â”‚        Mark as visited               â”‚
â”‚        Recursive DFS from adjacent   â”‚
â”‚        Unmark as visited (backtrack) â”‚
â”‚   3. If none work â†’ Return false     â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Maze Solving

```
MAZE SOLVING:

Find path from START to EXIT

Visual:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚S . # . . Eâ”‚  S = Start
â”‚# . # . #â”‚  E = Exit
â”‚. . . . . .â”‚  . = Path
â”‚# # . # # #â”‚  # = Wall
â”‚. . . . . .â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

One solution:
S â†’ â†’ â†“ â†’ â†’ E
    â†“     â†“
    â†’ â†’ â†’ â†“
        â†“
    â†’ â†’ â†’ E

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SOLVE_MAZE(start, exit):             â”‚
â”‚                                      â”‚
â”‚ DFS(row, col, visited):              â”‚
â”‚   1. If (row, col) == exit:          â”‚
â”‚      Found solution! Return true     â”‚
â”‚   2. If out of bounds or wall:       â”‚
â”‚      Return false                    â”‚
â”‚   3. If already visited:             â”‚
â”‚      Return false                    â”‚
â”‚   4. Mark (row, col) as visited      â”‚
â”‚   5. Try all 4 directions:           â”‚
â”‚      UP, DOWN, LEFT, RIGHT           â”‚
â”‚      If any succeeds â†’ Return true   â”‚
â”‚   6. Unmark (row, col) (backtrack)   â”‚
â”‚   7. Return false                    â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 2

| Concept | Key Point |
|---------|-----------|
| **N-Queens** | Place one queen per row, check conflicts |
| **Sudoku** | Fill cells respecting 3 constraints |
| **Permutations** | Order matters, generate all orderings |
| **Combinations** | Order doesn't matter, start_idx prevents duplicates |
| **Word Search** | DFS with visited tracking |
| **Maze** | Find path avoiding walls, backtrack on dead ends |
| **Optimization** | Constraint propagation reduces search space |

---

---

## ğŸ“… DAY 3: BRANCH & BOUND

### â±ï¸ Duration: 120 minutes

---

## ğŸ“ CONCEPT: Branch & Bound Fundamentals

### Core Idea

```
BRANCH & BOUND:

Systematic search with pruning based on bounds

Key Components:
1. BRANCH: Partition problem into subproblems
2. BOUND: Compute upper/lower bound for subproblem
3. PRUNE: Skip subproblems that can't improve best

Why?
Backtracking prunes when state is invalid.
Branch & Bound prunes when state can't be optimal.
More aggressive pruning â†’ faster!

Visual:

Search Tree:
                 root
                / | \
               /  |  \
              a   b   c
            / \   |  / \
           d   e  f g   h

Without B&B:
Explore all branches (if valid)

With B&B:
- Compute bound for 'a'
- If bound(a) < best_found â†’ Skip entire subtree
- Continue only for promising branches
```

### Bounding Functions

```
UPPER BOUND: Best possible solution in subproblem

Example (TSP): 
Current partial tour has cost C.
Remaining cities must be visited.
Upper bound = C + minimum_spanning_tree_of_remaining

Why upper bound?
- If it's less than best found, we can skip
- No way to achieve better value

LOWER BOUND: Worst possible solution in subproblem

Example (Knapsack):
Current partial value is V.
Can we add more items?
Lower bound = V + (feasible additional value)

Why lower bound?
- If it's better than best found, might be worth exploring
```

### Best-First Search

```
BEST_FIRST_SEARCH for Branch & Bound:

Use PRIORITY QUEUE ordered by bound

Algorithm:
1. Add root to priority queue
2. While queue not empty:
   a) Pop node with best bound
   b) If solution found â†’ Return
   c) If bound < best_found â†’ Prune
   d) Otherwise, branch into children
   e) Add children to queue

Why better than DFS?
- Explores most promising nodes first
- Often finds good solutions early
- Can prune more subtrees
```

---

## ğŸ“ CONCEPT: Branch & Bound for TSP

### Traveling Salesman Problem

```
TRAVELING SALESMAN PROBLEM (TSP):

Given n cities with distances between all pairs.
Find shortest tour visiting all cities exactly once
and returning to start.

NP-hard! No known polynomial algorithm.
Branch & Bound can find optimal for small-medium n.

Example (4 cities):
      1   2   3   4
   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
1  â”‚ 0  â”‚ 10 â”‚ 15 â”‚ 20 â”‚
2  â”‚ 10 â”‚ 0  â”‚ 35 â”‚ 25 â”‚
3  â”‚ 15 â”‚ 35 â”‚ 0  â”‚ 30 â”‚
4  â”‚ 20 â”‚ 25 â”‚ 30 â”‚ 0  â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜

Possible tours:
[1,2,3,4,1]: 10+35+30+20 = 95
[1,2,4,3,1]: 10+25+30+15 = 80 â† Optimal
[1,3,2,4,1]: 15+35+25+20 = 95
[1,3,4,2,1]: 15+30+25+10 = 80 â† Optimal
...

There are (n-1)!/2 = 3 tours (for n=4)
For n=20: Over 60 quadrillion tours!
Branch & Bound needed to find optimal.
```

### Branch & Bound for TSP

```
APPROACH:

1. Start with partial tour (just starting city)

2. For each unvisited city:
   a) Extend tour to include that city
   b) Compute lower bound on tour length
      Lower bound = (partial cost) + (MST of remaining)
   c) If lower bound < best_tour found â†’ Explore
   d) Otherwise â†’ Prune

3. When all cities visited â†’ Compare with best_tour

Example (starting at city 1):

Step 1: Tour = [1]
        Lower bound = 0 + MST{2,3,4}
        MST{2,3,4} minimum distance to visit all = ?
        Via 2â†’3â†’4: 35+30 = 65
        Via 2â†’4â†’3: 25+30 = 55
        Via 3â†’2â†’4: 35+25 = 60
        Minimum MST = 55
        Lower bound = 0 + 55 = 55

Step 2: Try extending to each city:
        Tour = [1,2]
        Cost so far = 10
        Lower bound = 10 + MST{3,4} + (return to 1)
        Need to visit 3, 4, and return
        MST{3,4} = 30 (direct 3-4)
        Return distance: min(3â†’1, 4â†’1) = min(15, 20) = 15
        Lower bound = 10 + 30 + 15 = 55

        Tour = [1,3]
        Cost so far = 15
        Lower bound = 15 + MST{2,4} + (return)
        MST{2,4} = 25 (direct 2-4)
        Return = min(2â†’1, 4â†’1) = min(10, 20) = 10
        Lower bound = 15 + 25 + 10 = 50

        Tour = [1,4]
        Cost so far = 20
        Lower bound = 20 + MST{2,3} + (return)
        MST{2,3} = 35 (direct 2-3)
        Return = min(2â†’1, 3â†’1) = min(10, 15) = 10
        Lower bound = 20 + 35 + 10 = 65

Step 3: Best lower bound is [1,3] with 50
        Explore that first (best-first search)
        Continue...
```

---

## ğŸ“ CONCEPT: Branch & Bound for 0/1 Knapsack

### Setup

```
0/1 KNAPSACK WITH BRANCH & BOUND:

Items with weights and values.
Maximize value subject to weight â‰¤ W.

Key Insight:
UPPER BOUND = Current value + (Fractional Knapsack of remaining)

Why?
- Fractional knapsack is optimal for continuous problem
- 0/1 can't do better than fractional
- So fractional gives an upper bound

If upper bound < best_found â†’ Can prune
```

### Algorithm

```
KNAPSACK_B&B(items, capacity):

1. Sort items by value/weight ratio (descending)

2. best_value = 0
   best_selection = none

3. DFS_BRANCH_AND_BOUND(item_idx, current_weight, 
                         current_value, remaining_capacity):
   
   a) If item_idx == n:
      If current_value > best_value:
         Update best_value and best_selection
      Return

   b) Compute upper bound:
      upper_bound = current_value + 
                    fractional_knapsack(item_idx, 
                                       remaining_capacity)

   c) If upper_bound â‰¤ best_value:
      Prune this branch (can't improve)
      Return

   d) Try including current item:
      If weight[item_idx] â‰¤ remaining_capacity:
         Include it
         Recursively solve
         Exclude it (backtrack)

   e) Try excluding current item:
      Don't include it
      Recursively solve next item

Example execution:

Items sorted by ratio:
A: w=2, v=12, ratio=6
B: w=4, v=20, ratio=5  
C: w=6, v=18, ratio=3
D: w=3, v=8, ratio=2.67

Capacity: 10

[Try including A]:
  Current: w=2, v=12, remaining=8
  Upper bound = 12 + fractional(B,C,D with 8 units)
              = 12 + 20 + (4/6)*18
              = 12 + 20 + 12 = 44
  44 > best_found (0) â†’ Explore
  
  [Try including B]:
    Current: w=6, v=32, remaining=4
    Upper bound = 32 + (4/6)*18 = 32 + 12 = 44
    44 > best_found â†’ Explore
    
    [Try including C]:
      w=12 > capacity â†’ Can't include
      
    [Try including D]:
      Current: w=9, v=40, remaining=1
      Upper bound = 40 + 0 = 40
      40 > best_found â†’ Explore
      
      [Try including nothing else]:
        All items considered
        Value = 40
        Best_found = 40
        
    [Done exploring from B]
  
  [Try excluding B]:
    Current: w=2, v=12, remaining=8
    Upper bound = 12 + fractional(C,D with 8)
                = 12 + (6/6)*18 + (2/3)*8
                = 12 + 18 + 5.33 = 35.33
    35.33 < 40 â†’ Prune!

Final: Best value = 40 (items A + B + partial D)
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 3

| Concept | Key Point |
|---------|-----------|
| **Branch & Bound** | Systematically search with bounds |
| **Upper Bound** | Best possible value in subproblem |
| **Pruning** | Skip if bound worse than best found |
| **Best-First** | Use priority queue for promising nodes |
| **TSP Application** | MST as lower bound |
| **Knapsack** | Fractional as upper bound |
| **Efficiency** | Often faster than backtracking |

---

---

## ğŸ“… DAY 4: AMORTIZED ANALYSIS

### â±ï¸ Duration: 120 minutes

---

## ğŸ“ CONCEPT: Amortized Complexity

### Why Amortized Analysis?

```
PROBLEM: Single operation can be expensive

Example: Dynamic Array

Append to array:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Usually O(1)            â”‚
â”‚ But sometimes O(n) when â”‚
â”‚ array is full and needs  â”‚
â”‚ to resize               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

How to analyze?
- Can't just say "O(n)" (pessimistic)
- Most operations are O(1)
- Occasional O(n) resize

SOLUTION: Amortized analysis
Average cost over sequence of operations
Even with expensive operations, amortized is O(1)
```

### Accounting Method

```
INTUITION: "Savings Account" for future work

Each operation gets assigned a cost (amortized cost)
Some cost is used immediately
Some cost is "saved" for later
Account balance never goes negative

Example: Append to dynamic array

Initial array: [a, b, c, d] (size 4)

Operation 1: Append(e) [size 4, count 5 - OVERFLOW]
Actual cost: 1 + 4 (copy) = 5 operations
Assigned amortized cost: 3
  Used immediately: 1 (append)
  Saved: 2 (toward future resize)
Account: +2

Operation 2: Append(f) [size 8, count 6]
Actual cost: 1 (just append)
Assigned amortized cost: 3
  Used immediately: 1 (append)
  Saved: 2
Account: +2 + 2 = 4

Operation 3: Append(g) [size 8, count 7]
Actual cost: 1
Assigned amortized cost: 3
Account: +4 + 2 = 6

Later operations...
Account grows as we save for potential resize.
When next resize happens:
Actual cost: 7 + 8 (copy) = 15 operations
Account: Sufficient to cover!
```

### Potential Method

```
DEFINITION:

Amortized cost = Actual cost + Change in potential

Where potential is function of data structure state.

Example: Dynamic array with doubling

Potential function: Î¦ = 2 Ã— (number of elements) - (array size)

Append when not full:
Actual cost = 1
Change in potential = 2 Ã— 1 - 0 = 2
Amortized = 1 + 2 = 3

Append when full (resize):
Actual cost = n + 1 (copy n elements + 1 append)
Change in potential = 2Ã—(n+1) - 2n = 2
Amortized = (n+1) + 2 = n+3 ??? Wrong!

Wait, let me recalculate...
Before: n elements, size n
        Potential = 2n - n = n

After: n+1 elements, size 2n
       Potential = 2(n+1) - 2n = 2n + 2 - 2n = 2

Change = 2 - n = 2 - n (negative!)

Amortized = (n+1) + (2-n) = 3

Interesting! Expensive resize has amortized cost 3!
Because potential was high before, decreases after.
```

---

## ğŸ“ CONCEPT: Amortized Analysis of Dynamic Arrays

### Doubling Strategy Analysis

```
DOUBLING STRATEGY:

When array full:
- Create new array of double size
- Copy all elements
- Append new element

Operations sequence:

Append 1: array = [a], size = 1
Append 2: array = [a,b], size = 2  
Append 3: RESIZE! array = [a,b,c,_], size = 4
          Actual cost: 2 (copy) + 1 (append) = 3
Append 4: array = [a,b,c,d], size = 4
Append 5: RESIZE! array = [a,b,c,d,e,_,_,_], size = 8
          Actual cost: 4 (copy) + 1 (append) = 5
Append 6: array = [...,f,...], size = 8
...
Append 9: RESIZE! array = [...,_,...], size = 16
          Actual cost: 8 (copy) + 1 (append) = 9

Total cost for n appends:
= 1 + 1 + 3 + 1 + 5 + 1 + 1 + 1 + 9 + ...
= n + (cost of all resizes)

Cost of resizes:
When array size goes from k to 2k:
Cost = k

Resizes happen at: size 1â†’2, 2â†’4, 4â†’8, ..., 2^kâ†’2^(k+1)
Total resize cost = 1 + 2 + 4 + 8 + ... + 2^k
                  = 2^(k+1) - 1

For n elements:
k â‰ˆ log n
Total cost â‰ˆ n + 2n - 1 = 3n - 1

Average per operation = (3n - 1) / n â‰ˆ 3 = O(1) âœ“

AMORTIZED COST: O(1) per append
```

### Accounting Method Proof

```
THEOREM: Amortized cost of append is O(1)

PROOF (using accounting):

Assign amortized cost = 3 to each append

Cost accounting:
- 1 unit: for immediate append
- 2 units: savings toward future resize

When array doubles from size k to 2k:
- Before: k elements, k units saved (1 per element Ã— k elements)
- Wait, only last element saved from its operation...

Actually, more careful analysis:
- k-1 appends (no resize): 3(k-1) cost assigned
  Used: k-1 (immediate), saved: 2(k-1)
  Account: 2(k-1)

- 1 append (with resize): cost assigned = 3
  Used: 1 (immediate) + k (copy from old array)
  Saved: 2
  Need from account: k - 1 = k-1
  Have in account: 2(k-1) = 2k - 2
  
  Since k-1 â‰¤ 2k-2 (true for k â‰¥ 1):
  Account never goes negative âœ“

- After resize, continue...
  Account now has: (2(k-1)) - (k-1) + 2 = k + 1
  
Thus: Amortized cost is always 3 = O(1)
```

---

## ğŸ“ CONCEPT: Other Data Structures

### Self-Adjusting Binary Search Tree (Splay)

```
SPLAY TREES:

Access a frequently-used element:
Tree restructures ("splays") that element to root

Analysis: O(log n) amortized per operation

Why?
- Sometimes splay is expensive (many rotations)
- But afterwards, frequently-used element is near root
- Future accesses are cheap
- Over sequence, amortizes to O(log n)
```

### Fibonacci Heaps

```
FIBONACCI HEAPS:

Complex data structure with amortized:
- Insert: O(1) amortized
- Find min: O(1) amortized
- Extract min: O(log n) amortized
- Decrease key: O(1) amortized

Total n operations: O(n log n) total
Compare with binary heap: O(n log n) per operation

For certain algorithms (Dijkstra):
Using Fibonacci heap can improve complexity!
But high constant factors, rarely used in practice.
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 4

| Concept | Key Point |
|---------|-----------|
| **Amortized** | Average cost over sequence of operations |
| **Accounting** | Each operation pays for future work |
| **Potential** | Amortized = Actual + Change in potential |
| **Dynamic Array** | O(1) amortized despite O(n) resize |
| **Account Balance** | Never goes negative (proof of correctness) |
| **Key Insight** | Expensive operations are infrequent |

---

---

## ğŸ“… DAY 5 (OPTIONAL): MIXED PARADIGM PROBLEMS

### â±ï¸ Duration: 90 minutes

---

## ğŸ“ CONCEPT: Combining Paradigms

### Backtracking + Greedy

```
PROBLEM: Maximize selections with constraints

Example: Job Scheduling (like Day 4, Week 12)
but with multiple machines

Approach:
1. Greedy ordering: Sort jobs by profit
2. Backtracking: Try assigning to machines
   If assignment leads to infeasible state â†’ Backtrack

Hybrid: Best of both
- Greedy narrows choices (profit order)
- Backtracking explores remaining possibilities
```

### Branch & Bound + DP

```
PROBLEM: Optimization with subproblem sharing

Approach:
1. Branch & Bound: Systematically explore
2. Dynamic Programming: Cache subproblem results
   
When bounding function encounters known subproblem:
- Use DP value instead of recomputing
- Prune more aggressively

Example: TSP with memorized distances
- Compute MST once, reuse for all bounds
```

### Backtracking + Constraint Propagation + Greedy

```
PROBLEM: Complex constraint satisfaction

Approach:
1. Constraint propagation: Reduce variable domains
2. Greedy heuristic: Choose variable with smallest domain (MRV)
3. Backtracking: Search when needed

Example: Sudoku solver
- Propagate constraints (cell can't have number in row)
- Choose cell with fewest possibilities first (MRV)
- Backtrack when stuck

Results in massive speedup compared to naive backtracking
```

---

## ğŸ“ CONCEPT: Choosing the Right Paradigm

### Decision Tree

```
ALGORITHM PARADIGM SELECTION:

Problem â†’ Question â†’ Answer â†’ Paradigm

Q1: Is optimal substructure present?
  NO â†’ Might not have standard solution
       Try: Heuristics, approximation
  YES â†’ Continue

Q2: Can problem be decomposed?
  NO â†’ Try: Iteration, simulation
  YES â†’ Continue

Q3: Overlapping subproblems?
  YES â†’ Dynamic Programming
  NO â†’ Continue

Q4: Greedy choice property?
  YES â†’ Greedy Algorithm
  NO â†’ Continue

Q5: Need to try multiple choices?
  YES â†’ Backtracking
  NO â†’ Continue

Q6: Need optimization pruning?
  YES â†’ Branch & Bound
  NO â†’ Continue

Q7: Is solution amortized?
  YES â†’ Amortized analysis
  NO â†’ Regular complexity analysis
```

---

## ğŸ”‘ KEY INSIGHTS FOR DAY 5

| Concept | Key Point |
|---------|-----------|
| **Hybrid Approaches** | Combine paradigms for complex problems |
| **Constraint Prop** | Reduce search space before backtracking |
| **MRV Heuristic** | Variable with smallest domain first |
| **Paradigm Choice** | Different problems need different approaches |
| **Real World** | Most complex problems use multiple techniques |

---

---

## ğŸ“‹ WEEK 13 SUMMARY & MASTERY CHECKLIST

### ğŸ¯ Concepts Covered

1. âœ… **Backtracking Fundamentals**
   - Incrementally build solutions
   - Prune invalid branches
   - DFS on solution space tree

2. âœ… **Backtracking Problems**
   - N-Queens: one per row, check conflicts
   - Sudoku: constraint satisfaction with propagation
   - Permutations: order matters, generate all
   - Combinations: order doesn't matter, avoid duplicates
   - Word Search: DFS with visited tracking
   - Maze: find path, backtrack on dead ends

3. âœ… **Branch & Bound**
   - Systematic search with bounding
   - Prune based on upper/lower bounds
   - TSP with MST bounds
   - 0/1 Knapsack with fractional upper bound

4. âœ… **Amortized Analysis**
   - Accounting method (savings account)
   - Potential method (state function)
   - Dynamic array doubling: O(1) amortized
   - Other structures: splay trees, Fibonacci heaps

5. âœ… **Mixed Paradigms**
   - Combine backtracking + greedy
   - Combine B&B + DP
   - Constraint propagation with backtracking
   - Algorithm selection decision tree

---

### ğŸ“Š Problem Patterns

| Pattern | Key Technique | Example |
|---------|---------------|---------|
| **Exhaustive Search** | Backtracking | Permutations |
| **Constraint Satisfaction** | Constraint prop + backtrack | Sudoku |
| **Optimization** | Branch & Bound | TSP, Knapsack |
| **Complexity Analysis** | Amortized | Dynamic arrays |

---

### âœ… MASTERY CHECKLIST FOR WEEK 13

- [ ] Can implement N-Queens without looking up algorithm
- [ ] Understand why constraint propagation helps Sudoku
- [ ] Can generate all permutations and combinations from scratch
- [ ] Know how to detect when to use backtracking vs DP
- [ ] Can explain Branch & Bound pruning strategy
- [ ] Understand upper/lower bounds in optimization
- [ ] Can trace TSP branch & bound execution
- [ ] Know accounting method for amortized analysis
- [ ] Can prove dynamic array amortized cost
- [ ] Can combine multiple paradigms for complex problems
- [ ] Can choose right paradigm for new problems
- [ ] Understand trade-offs between paradigms

---

### ğŸ”‘ TOP 5 THINGS TO REMEMBER

1. **Backtracking = DFS with pruning on solution tree**
2. **Constraint propagation dramatically reduces search space**
3. **Branch & Bound prunes with bounds, not just validity**
4. **Amortized cost: expensive operations are infrequent**
5. **Choose paradigm based on problem structure**

---

**End of Week 13: Backtracking & Branch & Bound**

*Comprehensive concept explanations with no code*  
*Visual diagrams, decision trees, and detailed examples*  
*Ready for implementation in any language*

