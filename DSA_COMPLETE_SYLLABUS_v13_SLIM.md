# üìò DATA STRUCTURES & ALGORITHMS COMPLETE CURRICULUM v13
## Comprehensive 19-Week Professional Syllabus

---

# üéì CURRICULUM OVERVIEW

## Phase Structure & Duration

| Phase | Name | Weeks | Duration | Focus Area |
|-------|------|-------|----------|-----------|
| **A** | üü¶ Foundations & Computational Thinking | 1-3 | 40-45 hrs | Fundamentals, Memory, Complexity |
| **B** | üü© Core Patterns & String Manipulation | 4-6 | 40-45 hrs | Problem-Solving Patterns |
| **C** | üü® Trees, Graphs & Dynamic Programming | 7-11 | 60-70 hrs | Advanced Data Structures |
| **D** | üüß Algorithm Paradigms | 12-13 | 25-30 hrs | Greedy, Backtracking, Analysis |
| **E** | üü™ Integration & Extensions | 14-15 | 25-30 hrs | Specialized Techniques |
| **F** | üü´ Advanced Deep Dives (Optional) | 16-18 | 35-40 hrs | Competitive Programming |
| **G** | üî¥ Mock Interviews & Final Review | 19 | 10-12 hrs | Interview Preparation |
| **TOTAL** | Complete DSA Mastery | 19 weeks | 235-270 hrs | Professional Competency |

---

# üü¶ PHASE A: FOUNDATIONS & COMPUTATIONAL THINKING
## Weeks 1-3 | 40-45 hours | Building Mental Models

### Phase Goals & Outcomes
**Goals:** Establish foundational understanding of how algorithms execute, scale, and use resources.

**Learning Outcomes:**
- Understand memory hierarchy, pointers, and address spaces
- Classify algorithms by time/space complexity using formal notation
- Design and analyze recursive algorithms with confidence
- Master fundamental data structures (arrays, lists, stacks, queues, heaps)
- Implement and compare sorting algorithms and hash tables

---

## üìå WEEK 1: COMPUTATIONAL FUNDAMENTALS, PEAK FINDING & ASYMPTOTICS

### Weekly Goal
Build rock-solid mental models of program execution, memory hierarchy, complexity frameworks, and recursion mechanics.

### Weekly Outcomes
- Understand RAM model assumptions and where they break
- Classify algorithms using Big-O, Big-Œ©, Big-Œò notation
- Design recursive algorithms and analyze recursion trees
- Experience algorithmic thinking through peak finding

### Summary
This week establishes the theoretical foundation for all subsequent work. Students learn how programs actually execute on real machines, understand the mathematical tools for analyzing algorithms, and experience the power of algorithmic design through the canonical peak finding problem.

---

### üìÖ DAY 1: RAM MODEL, VIRTUAL MEMORY & POINTERS | 90 min

**Topics:**
- **Memory Models**
  - RAM model: addressable cells with O(1) random access
  - Real systems: cache hierarchy (L1/L2/L3/DRAM/disk), virtual memory
  - Address space layout: code segment, data segment, heap, stack, kernel space
  - Process isolation and protection via virtual memory

- **Pointers & References**
  - Pointer as address stored in variable
  - Dereferencing: following pointers through memory
  - Address arithmetic and scaling (stride = element size)
  - Null pointers and dangling references

- **Memory Hierarchy & Locality**
  - Cache lines (~64 bytes) and prefetching
  - Temporal locality: repeated access fast
  - Spatial locality: nearby access efficient
  - False sharing on multi-core systems

- **Virtual Memory & TLB**
  - Virtual‚Üíphysical address translation
  - Translation Lookaside Buffer (TLB) as cache
  - Page faults: expensive (100s-1000s cycles)
  - Impact on algorithm performance

- **Stack & Heap**
  - Stack frames: function parameters, local variables, return addresses
  - Stack overflow from deep recursion
  - Heap allocation, fragmentation, allocator overhead
  - Lifetime management (automatic vs manual)

---

### üìÖ DAY 2: ASYMPTOTIC ANALYSIS - BIG-O, BIG-Œ©, BIG-Œò | 90 min

**Topics:**
- **Complexity Notation**
  - Big-O (upper bound): f(n) ‚â§ c¬∑g(n) for large n
  - Big-Œ© (lower bound): f(n) ‚â• c¬∑g(n) for large n
  - Big-Œò (tight bound): f(n) = both O(g(n)) and Œ©(g(n))
  - Intuition and practical interpretation

- **Complexity Classes Hierarchy**
  - O(1): constant time (array access, hash lookup)
  - O(log n): logarithmic (binary search)
  - O(n): linear (array traversal)
  - O(n log n): linearithmic (merge sort, quick sort average)
  - O(n¬≤): quadratic (bubble sort, nested loops)
  - O(n¬≥): cubic (matrix multiplication naive)
  - O(2‚Åø): exponential (subsets, exhaustive search)
  - O(n!): factorial (permutations)

- **Comparing Functions at Scale**
  - Growth rates: 1 < log n < n < n log n < n¬≤ < n¬≥ < 2‚Åø < n!
  - Crossover points: when complexity matters (n=10‚Å∂, n=10‚Åπ)
  - Constants matter on small inputs; asymptotic on large

- **Properties & Rules**
  - Constant factors ignored: O(2n) = O(n)
  - Transitivity: O(f)=O(g) and g=O(h) ‚Üí f=O(h)
  - Sum rule: O(f) + O(g) = O(max(f,g))
  - Product rule: O(f) √ó O(g) = O(f¬∑g)

- **Simple Recurrences**
  - T(n) = T(n/2) + O(1) ‚Üí O(log n) (binary search)
  - T(n) = 2T(n/2) + O(n) ‚Üí O(n log n) (merge sort)
  - T(n) = T(n-1) + O(1) ‚Üí O(n) (linear scan)
  - Recurrence trees for visualization

- **Master Theorem (Intuitive)**
  - Pattern: T(n) = a¬∑T(n/b) + O(n^d)
  - Result depends on comparing a vs b^d
  - Dominance of recursion tree work

---

### üìÖ DAY 3: SPACE COMPLEXITY & MEMORY USAGE | 90 min

**Topics:**
- **Space Analysis**
  - Total space = input + output + auxiliary
  - Input space usually excluded (given, read-only)
  - Focus on auxiliary/working space
  - In-place algorithms: O(1) auxiliary space

- **Stack vs Heap Space**
  - Stack: automatic deallocation, size limited, fast access
  - Heap: manual management (or GC), larger, slower
  - Recursion depth limited by stack (typically ~10^4)
  - Stack overflow from deep or unbounded recursion

- **Space Patterns**
  - O(1): in-place rearrangement (swaps only)
  - O(log n): recursion depth (binary search)
  - O(n): new array/list allocation
  - O(n¬≤): 2D grid, matrix storage

- **Overheads**
  - Pointer size: 8 bytes (64-bit systems)
  - Object headers: type information, GC marks
  - Allocator metadata: block size, free lists
  - Padding/alignment: performance optimization

- **Time-Space Tradeoffs**
  - Memoization: use space to save time (DP)
  - Precomputation: build table once, query fast
  - Caching: store expensive operation results
  - When is space worth trading for time?

- **Memory Hierarchy Impact**
  - Cache hits vs misses: 1-2 cycles vs 100+ cycles
  - Locality trumps asymptotic complexity at small scales
  - Prefetching and sequential access patterns
  - Algorithm choice influenced by memory reality

---

### üìÖ DAY 4: RECURSION I - CALL STACK & PATTERNS | 90 min

**Topics:**
- **Call Stack Mechanics**
  - Activation records: parameters, locals, return address
  - Frame management: pushing on call, popping on return
  - Recursion depth: function call nesting level
  - Stack overflow: exceeding system stack limit

- **Recursive Structure**
  - Base case: termination condition (stopping point)
  - Recursive case: problem reduction (smaller subproblem)
  - Progress guarantee: must approach base case
  - Proof by induction connection

- **Simple Examples**
  - Factorial: n! = n √ó (n-1)!
  - Sum of array: sum[0..n] = arr[n] + sum[0..n-1]
  - Fibonacci: fib(n) = fib(n-1) + fib(n-2)
  - Power: x^n = x √ó x^(n-1)

- **Recursion Trees**
  - Tree structure: nodes represent function calls, edges are calls
  - Depth: height of tree (max recursion depth)
  - Breadth: branching factor (calls per call)
  - Total nodes: sum of all operations

- **Identifying Complexity from Trees**
  - Linear recursion: single chain, O(depth)
  - Binary recursion: two branches, O(2^depth)
  - Balanced recursion: split equally, O(depth √ó breadth^depth)
  - Exponential blowup: naive Fibonacci O(2^n)

- **Common Failure Modes**
  - Missing base case ‚Üí infinite recursion
  - Wrong reduction ‚Üí no progress toward base
  - Stack overflow ‚Üí too deep recursion
  - Off-by-one errors in base case

---

### üìÖ DAY 5: RECURSION II - ADVANCED PATTERNS & MEMOIZATION | 120 min

**Topics:**
- **Recursion Patterns**
  - Linear recursion: f(n-1) only (one call per level)
  - Tree recursion: f(n-1) + f(n-2) (multiple calls per level)
  - Divide-and-conquer: f(n/2) splits (balanced reduction)
  - Mutual recursion: A calls B calls A

- **Tail Recursion**
  - Tail call: recursive call is last operation
  - Compiler optimization: TCO converts to loop
  - Performance: avoids extra stack frames
  - Examples and when it applies

- **Overlapping Subproblems**
  - Same subproblem computed repeatedly
  - Naive Fibonacci: fib(5) calls fib(3) twice
  - Visualization: overlapping portions in recursion tree
  - Signal for optimization via memoization

- **Memoization (Top-Down DP)**
  - Cache/memo table: stores computed results
  - Lookup before computing: return immediately if cached
  - Store after computing: save for future lookups
  - Converts exponential to polynomial time

- **Memoization Example**
  - Fibonacci without: fib(40) takes seconds (2^40 ‚âà 10^12)
  - Fibonacci with memo: fib(40) instant (40 unique calls)
  - Space: O(n) for memo table
  - Time: O(n) instead of O(2^n)

- **Recursion Depth Limits**
  - Default stack limit: ~1-10 MB (system dependent)
  - Typical max depth: 1000-10000
  - Exceeding causes stack overflow (crash)
  - Solutions: iterative approach or explicit stack

- **Converting to Iteration**
  - Use explicit stack instead of call stack
  - Maintain state variables
  - Loop until stack empty
  - Reduces recursion depth risk

---

### üìÖ DAY 6 (OPTIONAL): PEAK FINDING & ALGORITHMIC THINKING | 120 min

**Topics:**
- **1D Peak Finding**
  - Definition: element ‚â• both neighbors (or ‚â• one if at edge)
  - Naive solution: linear scan O(n)
  - Better insight: can we do faster?

- **1D Divide & Conquer Solution**
  - Examine middle element and its neighbor
  - Compare with neighbor: if smaller, peak on that side
  - If larger, could be peak or peak to the other side
  - Recurse on promising side: O(log n) or O(n) depending on careful analysis

- **2D Peaks**
  - Definition: element ‚â• all 4 neighbors
  - Strategy 1: find column maximum, check left/right, recurse
  - Column-max greedy: O(n) to find, compare neighbors O(1)
  - Recurrence: T(n,m) = T(n, m/2) + O(n) ‚Üí O(n log m)
  - Trade-off with O(m log n) row-based approach

- **Algorithmic Meta-Lessons**
  - Exploiting structure leads to faster algorithms
  - Brute force (O(n)) ‚Üí smarter structure (O(log n))
  - Information-theoretic lower bounds
  - Algorithm design iterative process

- **Proof Techniques**
  - Correctness by exhaustion: covers all cases
  - Recursion proof: solved subproblem implies solved original
  - Exchange argument: why greedy choice valid

---

## üìå WEEK 2: LINEAR DATA STRUCTURES & BINARY SEARCH

### Weekly Goal
Master arrays, linked lists, stacks, queues, and binary search. Understand memory layouts, performance tradeoffs, and when to use each structure.

### Weekly Outcomes
- Understand array indexing and cache locality implications
- Build dynamic arrays with amortized O(1) insertion
- Implement linked structures and analyze pointer operations
- Master binary search and extend beyond sorted arrays
- Understand LIFO/FIFO semantics and real-world applications

### Summary
This week covers fundamental data structures and the most important search algorithm. Students learn how memory layout affects performance, build data structures from scratch, and master binary search as both algorithm and pattern for optimization problems.

---

### üìÖ DAY 1: ARRAYS & MEMORY LAYOUT | 90 min

**Topics:**
- **Static Arrays**
  - Contiguous memory: elements stored sequentially
  - Index to address: address = base + i √ó stride (stride = element size)
  - Constant-time access: O(1) lookup
  - Fixed size: cannot resize without allocation

- **Memory Layout & Performance**
  - Row-major (C/C++): [i][j] = base + i√ócols + j
  - Column-major (Fortran): [i][j] = base + j√órows + i
  - Cache line alignment: typically 64 bytes
  - Prefetching: sequential access detected and optimized

- **Cache Implications**
  - Sequential access: highly cache-friendly
  - Random access: defeats prefetching, poor cache
  - SIMD vectorization: works with tight loops on arrays
  - False sharing: multiple threads on same cache line

- **Pros of Arrays**
  - Excellent cache locality
  - O(1) random access
  - SIMD-friendly
  - Minimal memory overhead

- **Cons of Arrays**
  - Fixed size; resizing expensive
  - Insert/delete in middle: O(n) shifts
  - Waste space if under-utilized

- **Multi-Dimensional Arrays**
  - Conceptually 2D/3D; physically linear in memory
  - Memory layout choice affects iteration performance
  - Row-major more cache-friendly for row iterations
  - Nesting implications

---

### üìÖ DAY 2: DYNAMIC ARRAYS & AMORTIZED GROWTH | 90 min

**Topics:**
- **Dynamic Array Model**
  - Logical size: actual elements
  - Capacity: allocated space
  - Growth strategy: when full, allocate more

- **Doubling Strategy**
  - When size == capacity, allocate 2√ó capacity
  - Copy existing elements to new location
  - Allow 2n operations before next resize
  - Amortized O(1) append

- **Amortized Analysis**
  - Amortized cost = total cost / operations
  - Total cost: n operations + 1+2+4+...+n copies = 2n
  - Average: 2n/n = O(1) per operation
  - Occasional expensive operation, many cheap ones

- **Reallocation Effects**
  - Pointer invalidation: all pointers to old array become invalid (critical bug source)
  - Temporary memory spike: may need 2√ó memory during copy
  - Performance spikes: allocation/copy can stall program

- **Resizing Dynamics**
  - Each resize copies all elements: O(n) operation
  - But happens infrequently: log n resizes total
  - Doubling vs linear growth (e.g., 1.5√ó): different constants
  - Load factor: elements / capacity

- **Shrinking**
  - Rarely shrink to avoid thrashing (resize up, down, up, ...)
  - Reserve: pre-allocate capacity for known growth
  - Shrink-to-fit: explicitly compact (trade space for time)

- **Real-World Implications**
  - Vector capacity increases: not always predictable
  - GC pressure from allocation
  - Virtual memory effects: allocation size crosses page boundaries
  - Performance profiling shows latency spikes

---

### üìÖ DAY 3: LINKED LISTS | 90 min

**Topics:**
- **Singly Linked Lists**
  - Node: value + next pointer
  - Head pointer: entry point
  - Traversal: follow pointers from head to tail
  - Null terminator: end of list

- **Doubly Linked Lists**
  - Node: value + next + prev pointers
  - Bidirectional traversal
  - Delete easier: have previous pointer already

- **Operations & Complexity**
  - Insert/delete at head: O(1) (have pointer)
  - Insert/delete at tail: O(1) with tail pointer, O(n) without
  - Insert/delete at arbitrary: O(n) to find position
  - Search: O(n) linear scan
  - Access by index: O(n) traversal

- **Pros**
  - O(1) insert/delete once positioned
  - Dynamic size; no capacity issues
  - Flexible structure; can interleave with other data

- **Cons**
  - No O(1) random access (must traverse)
  - Poor cache locality: pointers scattered in memory
  - Extra memory per node: pointers + padding
  - GC overhead: tracking live nodes

- **Cache Reality Check**
  - Array of n integers: likely in L3 cache
  - Linked list of n nodes: scattered across memory
  - Linked list traversal: cache miss nearly every access (~3000 cycles each)
  - Array traversal: one miss per 8 elements (~240 cycles total)
  - Asymptotics misleading: O(n) array faster than O(1) linked list in practice

- **Specific Use Cases**
  - LRU cache: doubly-linked for efficient eviction
  - Polynomial representation: easy insertion at arbitrary position
  - Graph adjacency (in some implementations)
  - When truly need O(1) insert/delete (and not arrays with reserve)

- **Variants**
  - Circular linked list: tail‚Üíhead, useful for queues
  - Sentinel nodes: dummy nodes simplify edge cases
  - Intrusive lists: embedding node structure in data
  - Skip lists: probabilistic structure with O(log n) operations

---

### üìÖ DAY 4: STACKS, QUEUES & DEQUES | 90 min

**Topics:**
- **Stacks (LIFO)**
  - Push: add to top
  - Pop: remove from top
  - Connection to call stack (real implementation of recursion)
  - Stack frames: function parameters, locals, return address

- **Stack Use Cases**
  - Recursion: call stack tracks execution
  - Expression evaluation: operator precedence, postfix conversion
  - Backtracking: push state, pop to undo
  - DFS: explicit stack instead of recursion
  - Browser back button: history stack

- **Queues (FIFO)**
  - Enqueue: add to back
  - Dequeue: remove from front
  - Real queue semantics: fairness, scheduling

- **Queue Use Cases**
  - BFS: frontier of nodes to explore
  - Job scheduling: FIFO fairness
  - Producer-consumer: buffer between systems
  - Breadth-first algorithms

- **Deques (Double-Ended Queue)**
  - Add/remove from both ends
  - Generalization of stack and queue
  - Bidirectional traversal possible

- **Deque Use Cases**
  - Sliding window maximum/minimum: monotonic deque
  - LRU cache with time tracking
  - Undo/redo: forward and backward history
  - Rotation: element moves to end after processing

- **Implementations**
  - Array-based stack: O(1) push/pop, amortized with growth
  - Array-based queue: circular buffer to avoid shifts
  - List-based: O(1) all operations but cache unfriendly
  - Circular buffer: pointer wraps at end (mod for wraparound)

- **Monotonic Deque Pattern**
  - Maintain deque in increasing/decreasing order
  - Remove elements from front violating monotonicity
  - Each element processed once: O(n) total
  - Application: sliding window max/min

- **Performance**
  - Stack/queue operations: O(1) all cases
  - Monotonic deque: O(1) amortized per element
  - Space: O(n) for stored elements

---

### üìÖ DAY 5: BINARY SEARCH & INVARIANTS | 120 min

**Topics:**
- **Binary Search Precondition**
  - Sorted array: ascending or descending
  - Invariant: target in [low, high] throughout
  - Proof by induction: each step narrows range correctly

- **Implementation**
  - mid = low + (high - low) / 2 (prevents overflow)
  - Compare arr[mid] to target
  - If equal: found, return mid
  - If arr[mid] < target: recurse right half
  - If arr[mid] > target: recurse left half
  - Terminates when low > high: not found

- **Termination & Correctness**
  - Loop invariant: if target exists, it's in [low, high]
  - Each iteration: range halves
  - Terminates: low eventually > high
  - Correctness: by loop invariant, either found or not exists

- **Binary Search Variants**
  - First occurrence: find leftmost index where arr[i] == target
  - Last occurrence: find rightmost index where arr[i] == target
  - Lower bound: first element ‚â• target
  - Upper bound: first element > target
  - Each requires subtle condition changes

- **Off-by-One Error Prevention**
  - Mid calculation: mid = low + (high - low) / 2 safe
  - Termination: when low > high, search complete
  - Boundary conditions: care with first/last element

- **Binary Search on Answer Space**
  - Reframe problem: "Find X such that condition(X)"
  - Condition is monotone: false up to threshold, true after
  - Example: "Minimize capacity to complete all jobs in K hours?"
    - Too small capacity: impossible
    - Large capacity: feasible
    - Find minimum feasible capacity

- **Feasibility Checks**
  - Must be deterministic: yes/no answer
  - Must be efficient: faster than linear search
  - Example: check if N jobs fit in capacity C (greedy packing)

- **Binary Search Optimization Examples**
  - Capacity planning: find min capacity such that feasible
  - Distance placement: find max min-distance between N points
  - Time allocation: find min time to complete all tasks
  - Load balancing: minimize maximum load

- **Floating-Point Binary Search**
  - For real-valued targets (e.g., square root)
  - Epsilon tolerance for convergence
  - Fixed iterations (typically 20-30 for 10^-6 precision)
  - Compare with precision requirement

- **Systems Perspective**
  - Binary search in STL: std::binary_search, lower_bound, upper_bound
  - Using BS to auto-tune parameters
  - Online vs offline versions
  - Adaptive binary search (knows array partially)

---

## üìå WEEK 3: SORTING, HEAPS & HASHING

### Weekly Goal
Understand sorting algorithm trade-offs, heap structures, and hash table implementations. Master core primitives used throughout DSA.

### Weekly Outcomes
- Know when each sort is appropriate (O(n¬≤) vs O(n log n))
- Build and operate heaps
- Implement hash tables with collision resolution
- Understand rolling hash for string pattern matching
- Analyze real-world performance vs asymptotic complexity

### Summary
This week covers three fundamental primitives used across countless algorithms. Students learn sorting algorithms from theory to practice, understand heap structure and operations, and master hash tables both for explicit use and as internal tools.

---

### üìÖ DAY 1: ELEMENTARY SORTS - BUBBLE, SELECTION, INSERTION | 90 min

**Topics:**
- **Bubble Sort**
  - Adjacent swaps: compare neighbors, swap if out of order
  - Larger elements "bubble" up each pass
  - Best case: O(n) if already sorted (with early termination)
  - Worst case: O(n¬≤)
  - Stable: equal elements maintain order

- **Selection Sort**
  - Partition array: sorted (left) | unsorted (right)
  - Find minimum in unsorted, swap to sorted partition
  - Exactly n-1 swaps total
  - O(n¬≤) always (no best case improvement)
  - Not stable: swapping breaks relative order

- **Insertion Sort**
  - Grow sorted prefix by inserting next element
  - Shift elements to make space
  - Insert element at correct position
  - Best case: O(n) if sorted
  - Worst case: O(n¬≤) if reverse sorted
  - Stable and in-place
  - Small constant factors

- **Comparison**
  - Bubble sort: worst of the three, avoid
  - Selection sort: minimal swaps, not adaptive
  - Insertion sort: adaptive, small constants, stable
  - All have O(n¬≤) worst case

- **Adaptive Behavior**
  - "Nearly sorted" input: few elements out of place
  - Insertion sort: O(n) + O(inversions) effectively
  - Bubble/selection: still O(n¬≤) always

- **When Elementary Sorts Win**
  - Small arrays: n < 50, constants dominate
  - Nearly sorted: insertion sort O(n)
  - Hybrid: use insertion on small subarrays in quicksort
  - Hardware limitations: predictable, cache-friendly

- **Stability & In-Place**
  - Stability: equal elements maintain relative order (matters for multi-key sort)
  - In-place: O(1) extra space (or O(log n) with recursion)
  - Trade-offs: stability sometimes requires extra space

---

### üìÖ DAY 2: MERGE SORT & QUICKSORT | 120 min

**Topics:**
- **Merge Sort**
  - Divide array in half recursively
  - Recursively sort both halves
  - Merge two sorted halves in O(n) time
  - Recurrence: T(n) = 2T(n/2) + O(n) ‚Üí O(n log n)

- **Merge Operation**
  - Two pointers on sorted arrays
  - Compare elements, take smaller, advance pointer
  - Merge time: O(m+n) where m, n are half sizes
  - Preserves stability: equal elements from left processed first

- **Merge Sort Characteristics**
  - Guaranteed O(n log n) worst-case (predictable for real-time)
  - Stable: maintains relative order of equal elements
  - Requires O(n) extra space for merge buffers
  - Not in-place; external sorts (disk-based) use merge
  - Top-down (recursive) or bottom-up (iterative) implementation

- **Quicksort**
  - Partition around pivot: smaller left, larger right
  - Recursively sort halves
  - In-place with careful partition
  - Expected O(n log n); worst-case O(n¬≤)

- **Partitioning**
  - Move pivot to correct position (partition position)
  - Rearrange so smaller on left, larger on right
  - Multiple strategies:
    - Hoare partition: two pointers from ends
    - Lomuto partition: one pointer, simpler
  - All O(n) but different constant factors

- **Pivot Selection**
  - Poor pivot choice: leads to O(n¬≤) (sorted input with first element pivot)
  - Randomized pivot: expected O(n log n)
  - Median-of-three: first, middle, last element
  - Better pivot: reduces worst case likelihood

- **Quicksort Characteristics**
  - Not stable: partition breaks relative order
  - In-place: O(log n) extra space for recursion
  - Fast in practice: better cache, fewer data movements
  - Worst-case O(n¬≤) but rare with good pivot strategy

- **3-Way Partition**
  - Handles duplicates efficiently
  - Partition into: < pivot, = pivot, > pivot
  - Don't recurse on middle section
  - Improves performance on many duplicate elements

- **Hybrid Algorithms**
  - Introsort: quicksort ‚Üí switches to heapsort if depth too deep
  - Prevents worst-case O(n¬≤)
  - Used in C++ std::sort
  - Timsort (Python, Java): merge sort + insertion sort hybrid

- **Memory Effects**
  - Merge sort: sequential access, good prefetching, predictable
  - Quicksort: more random access, but in-place
  - Cache efficiency: quicksort often faster despite asymptotics
  - Modern CPUs: quicksort frequently beats merge sort

- **Practical Considerations**
  - Quicksort: faster average case, standard choice
  - Merge sort: stable, guaranteed O(n log n)
  - Introsort: best of both (C++ standard)
  - Timsort: optimized for real data patterns

---

### üìÖ DAY 3: HEAPS, HEAPIFY & HEAP SORT | 90 min

**Topics:**
- **Binary Heap Structure**
  - Complete binary tree in array
  - Parent of index i: (i-1)/2
  - Left child: 2i+1; right child: 2i+2
  - Min-heap: parent ‚â§ children
  - Max-heap: parent ‚â• children

- **Heap Operations**
  - Insert: add to end, bubble up (sift-up)
    - Compare with parent, swap if violates heap property, recurse up
    - Time: O(log n)
  - Extract-min/max: remove root, move last to root, bubble down (sift-down)
    - Compare with children, swap with smaller/larger, recurse down
    - Time: O(log n)
  - Find-min/max: root element, O(1)

- **Heapify (Build Heap)**
  - Bottom-up approach: start from last non-leaf
  - For each node: sift down if needed
  - Process all internal nodes: O(n) total time
  - Surprising result: O(n) not n√óO(log n)
  - Why: most nodes near leaves (sift-down trivial)

- **Heap Sort**
  - Build heap: O(n)
  - Extract-min repeatedly n times: n√óO(log n) = O(n log n)
  - In-place variant: swap root with last, contract array, sift-down
  - Total: O(n log n) worst-case
  - Not stable: extraction order differs from insertion order

- **Priority Queue**
  - Heap-based implementation
  - Operations: insert (enqueue), extract-min/max (dequeue), peek
  - All O(log n) except peek O(1)
  - Applications: Dijkstra, event simulation, task scheduling

- **Min-Heap vs Max-Heap**
  - Min-heap: smallest at root (priority = smallest)
  - Max-heap: largest at root (priority = largest)
  - Convert: negate values or use different comparator
  - Both useful depending on problem

- **Decrease-Key & Delete**
  - Decrease-key: sift element up
  - Delete: decrease to -‚àû, extract-min
  - Fibonacci heaps: O(1) amortized decrease-key (complex)

- **Practical Variants**
  - D-ary heaps: each node has d children
  - Better cache locality than binary
  - Used in some systems (d=4)
  - Leftist heaps: mergeable heaps
  - Skew heaps: amortized version of leftist

---

### üìÖ DAY 4: HASH TABLES I - SEPARATE CHAINING | 90 min

**Topics:**
- **Hash Function Basics**
  - Map key ‚Üí bucket index [0, m-1]
  - Desiderata: uniformity, fast computation, deterministic
  - Integer hash: h(key) = key % m
  - String hash: (a‚ÇÄ√ób‚Å∞ + a‚ÇÅ√ób¬π + ...) % m

- **Hash Function Quality**
  - Uniformity: collisions minimized, even distribution
  - Fast: O(1) computation
  - Deterministic: same input always same output
  - Avalanche effect: small input change ‚Üí big hash change

- **Separate Chaining**
  - Buckets as linked lists (or vectors)
  - Collision: multiple keys hash to same bucket
  - Search: hash key, scan chain for key
  - Expected O(1) operations given good hash and load factor

- **Hash Table Operations**
  - Insert: hash key, append to chain
    - Expected O(1) if hash uniform and load factor reasonable
  - Delete: hash key, remove from chain
    - Expected O(1)
  - Search/lookup: hash key, scan chain
    - Expected O(1), worst-case O(n)

- **Load Factor & Resizing**
  - Load factor: Œ± = n/m (elements / buckets)
  - As Œ± increases, chains get longer
  - Typical: keep Œ± < 1 (less than one element per bucket on average)
  - Too small: waste space
  - Too large: long chains, slow operations

- **Dynamic Resizing**
  - When Œ± exceeds threshold (e.g., Œ± > 2), resize
  - Double number of buckets typically
  - Rehash all elements (may go to different bucket)
  - Amortized O(1) insertion with doubling strategy

- **Collision Frequency**
  - Birthday paradox: collisions occur faster than intuition
  - With m buckets and n elements, expect Œò(n¬≤/m) collisions
  - At Œ±=1: expect ~1 collision pair
  - Managing collisions critical for performance

- **Chaining Trade-offs**
  - Simple implementation
  - Expected O(1) operations
  - Worst-case O(n) if all hash to one bucket (unlikely)
  - Space: n + m (elements + buckets)

- **Real-World Implementations**
  - Python dict: hash table (used to be open addressing, now hybrid)
  - C++ unordered_map: hash table with separate chaining
  - Java HashMap: hash table with chaining (buckets are linked lists)

---

### üìÖ DAY 5: HASH TABLES II - OPEN ADDRESSING & ROLLING HASH | 120 min

**Topics:**
- **Open Addressing Overview**
  - No separate chains; all elements in single table
  - Collision: probe for empty slot
  - Probe sequence: h(key), h(key)+1, h(key)+2, ...
  - Find empty slot; insert there

- **Linear Probing**
  - h_i(key) = (h(key) + i) mod m
  - Simple but suffers primary clustering
  - Cluster: contiguous occupied slots attract more collisions
  - Long probes increase search time

- **Quadratic Probing**
  - h_i(key) = (h(key) + i¬≤) mod m
  - Reduces clustering
  - May not probe all slots if m not prime
  - Secondary clustering still possible

- **Double Hashing**
  - h_i(key) = (h‚ÇÅ(key) + i√óh‚ÇÇ(key)) mod m
  - Two independent hash functions
  - Probes entire table (if h‚ÇÇ coprime to m)
  - Reduces clustering significantly
  - Better distribution than linear/quadratic

- **Load Factor Constraints**
  - Open addressing: typically keep Œ± < 0.5 (half-full)
  - As Œ± ‚Üí 1, expected probes ‚Üí ‚àû
  - Higher space usage than chaining
  - Better cache locality than chaining

- **Deletion in Open Addressing**
  - Can't just remove (breaks probe chains)
  - Tombstones: mark deleted slots
  - Tombstones accumulate: need rehashing periodically
  - Search skips tombstones; insert reuses them

- **Hash Security**
  - Hash flooding: adversary chooses keys hashing same
  - DoS attack: degrades to O(n) operations
  - Mitigation: randomized hash seed per table
  - SipHash: cryptographic hash (Python 3.3+, Ruby)

- **Universal Hashing**
  - Family of hash functions with property:
    - For any two distinct keys, P[h(k‚ÇÅ) = h(k‚ÇÇ)] ‚â§ 1/m
  - Randomized seed: prevents adversarial collisions
  - Guarantees good performance against adversarial input

- **Rolling Hash / Polynomial Hash**
  - Hash string s = (s[0]√ób^(n-1) + s[1]√ób^(n-2) + ... + s[n-1]) mod p
  - Sliding window: hash of substring [i, i+k)
  - Update: remove s[i]√ób^(n-1), multiply by b, add s[i+k]
  - All in O(1) time per slide

- **Rolling Hash Implementation**
  - Modular arithmetic: mod large prime to prevent overflow
  - Precompute powers of b mod p
  - Handle collision: verify full strings match
  - Choose b (typically 256 or 31) and p (typically large prime)

- **Karp-Rabin Algorithm**
  - String matching using rolling hash
  - Hash pattern P and each substring of text T
  - If hashes match: verify (check for collision)
  - Time: O(n+m) expected; O(nm) worst-case (but rare)
  - Applications: substring search, plagiarism detection, DNA sequences

- **Karp-Rabin Multi-Pattern**
  - Hash multiple patterns simultaneously
  - Search for any pattern match
  - O(n + m + k√ólog k) where k = number of patterns
  - Better than KMP for multiple patterns

---

# üü© PHASE B: CORE PATTERNS & STRING MANIPULATION
## Weeks 4-6 | 40-45 hours | Problem-Solving Patterns

### Phase Goals & Outcomes
**Goals:** Master high-frequency problem-solving patterns that apply to hundreds of interview questions.

**Learning Outcomes:**
- Apply two-pointer techniques to array and list problems
- Master sliding window for substring and subarray problems
- Recognize and solve via hash maps and monotonic stacks
- Handle string problems: palindromes, substrings, brackets
- Choose patterns systematically based on problem structure

---

## üìå WEEK 4: CORE PROBLEM-SOLVING PATTERNS I

### Weekly Goal
Master array/sequence patterns: two pointers, sliding windows, divide & conquer, binary search as pattern.

### Weekly Outcomes
- Understand two-pointer mechanics and when to use same vs opposite direction
- Apply sliding window to fixed and variable-size problems
- Use divide & conquer systematically
- Extend binary search to optimization problems

### Summary
This week teaches the most fundamental patterns in array and sequence problems. Students learn how to manipulate pointers and windows, decompose problems via divide & conquer, and frame optimization as binary search. These patterns apply to hundreds of problems.

---

### üìÖ DAY 1: TWO-POINTER PATTERNS | 90 min

**Topics:**
- **Same-Direction Pointers**
  - Both start at beginning, move in same direction
  - Example: removing duplicates in-place
    - Slow pointer: write position for unique elements
    - Fast pointer: scan for new values
    - Invariant: [0, slow) = unique elements
  - Example: merging two sorted arrays
    - Merge result while pointers advance
  - Time: O(n), Space: O(1)

- **Opposite-Direction Pointers (Convergent)**
  - One at start, one at end, converge toward middle
  - Example: container with most water
    - Two lines at indices i and j
    - Area = (j-i) √ó min(h[i], h[j])
    - Greedy: move inward from shorter line
    - Why: moving from longer never improves (width decreases, height doesn't improve)
  - Example: two-sum in sorted array
    - Sum too small: advance left pointer
    - Sum too large: retreat right pointer
  - Time: O(n), Space: O(1)

- **Divergent Approach (Start Middle, Expand)**
  - Start at center, expand outward
  - Example: palindrome expansion
    - Center at each position
    - Expand while characters match
    - Odd-length and even-length centers
  - Example: trapping rain water (advanced)
    - Combine with other techniques

- **Invariants & Correctness**
  - Maintain clear invariant as pointers move
  - Example: "elements before slow are sorted and final"
  - Example: "answer is within current pointers or won't appear"
  - Invariant proves correctness

- **Pattern Recognition**
  - In-place modification: signal for two-pointers
  - No extra space required: typical application
  - Partitioning problems: two-pointers natural
  - Array manipulation: two-pointers often best

---

### üìÖ DAY 2: SLIDING WINDOW (FIXED SIZE) | 90 min

**Topics:**
- **Fixed-Length Windows**
  - Window of size k slides over array
  - At each step: add new element, remove old element
  - Maintain aggregate (sum, min, max, frequency)
  - O(n) time for computing all k-windows

- **Running Sum / Average**
  - Compute sum of all k-length subarrays
  - First window: sum array[0..k-1] in O(k)
  - Each subsequent: sum += arr[i+k] - arr[i] in O(1)
  - Total: O(n) instead of O(nk)

- **Maximum/Minimum in Window**
  - Naive: O(n) for each window ‚Üí O(nk) total
  - Monotonic deque approach: O(n) total
    - Maintain deque of indices in decreasing order of values
    - Add: remove from back while new element larger
    - Remove: pop from front if outside window
    - Front of deque is max at each step

- **Deque Details**
  - Store indices, not values (need to know if outside window)
  - Add: while queue not empty and queue.back() ‚â§ current, pop back
  - Remove: if queue.front() <= i-k, pop front
  - Front is always max/min for current window

- **Frequency-Based Windows**
  - Fixed window size, track character frequencies
  - Example: "permutation in string"
    - Check if pattern is permutation of any substring
    - Compare frequency maps at each window
    - Perfect match: return true
  - Example: "all anagrams of substring"
    - Find all anagrams in text
    - Slide window and compare frequency maps
    - Track starting indices

- **Frequency Comparison**
  - Need both frequency maps match
  - Track frequency of each character
  - Optimize: count distinct characters matched
  - When all distinct characters have correct count, found

- **Edge Cases**
  - Window larger than array: return empty or handle
  - All elements same: window comparison still works
  - Very small arrays: handle boundaries carefully
  - Empty string: handle gracefully

---

### üìÖ DAY 3: SLIDING WINDOW (VARIABLE SIZE) | 120 min

**Topics:**
- **Variable-Size Window Mechanics**
  - Expand right pointer to include more
  - Shrink left pointer when constraint violated
  - Goal: find optimal window (min/max length, best sum, etc.)
  - Time: O(n) if shrink total O(n)

- **Constraint-Based Windows**
  - Invariant: [left, right] maintains property
  - Expand right: try including next element
  - Check constraint: if violated, shrink left
  - Continue until valid, record best

- **Two-Pointer Management**
  - Left pointer: shrink side
  - Right pointer: expand side
  - Both move only forward (never backward)
  - Each position visited once: O(n) total

- **Longest Substring Without Repeating Characters**
  - Track character positions in map
  - Expand right: add character
  - If character seen: update left to after previous occurrence
  - Remove old character from map
  - Length: right - left + 1
  - Time: O(n)

- **At Most k Distinct Characters**
  - Maintain frequency map of current window
  - Count distinct characters
  - If more than k distinct: shrink left until ‚â§ k
  - Track max length seen
  - Time: O(n)

- **At Exactly k Distinct**
  - Compute at-most-k minus at-most-(k-1)
  - Or: track distinct count, expand/shrink appropriately
  - Common interview trick: reduce to simpler problem

- **Minimum Window Substring**
  - Find smallest window containing all pattern characters
  - Expand right until complete
  - Shrink left while complete
  - Track best window found
  - Time: O(n)

- **Window Validity Checking**
  - Clear condition for window validity
  - Efficient checking: frequency maps, counters
  - Early termination: once found, can sometimes stop
  - Optimization: track how many chars have enough count

- **Frequency Map Optimization**
  - Instead of checking full match each step
  - Track count of characters with sufficient frequency
  - When count equals required characters, window valid
  - Faster than full comparison each step

---

### üìÖ DAY 4: DIVIDE & CONQUER PATTERN | 90 min

**Topics:**
- **Divide & Conquer Template**
  - Divide: split problem into subproblems (usually 2, can be more)
  - Conquer: recursively solve subproblems
  - Combine: merge solutions into final answer
  - Complexity: typically O(n log n) for balanced splits

- **General Recurrence**
  - T(n) = a√óT(n/b) + O(n^d)
  - a subproblems of size n/b
  - O(n^d) combine cost per level
  - Result depends on comparing a vs b^d (Master theorem)

- **Merge Sort as Canonical Example**
  - Divide: split into two halves
  - Conquer: recursively sort halves
  - Combine: merge two sorted halves in O(n)
  - Recurrence: T(n) = 2T(n/2) + O(n) = O(n log n)
  - Worst-case guaranteed (unlike quicksort)

- **Counting Inversions**
  - Problem: how many pairs (i, j) where i < j but arr[i] > arr[j]
  - Naive: O(n¬≤) nested loops
  - Divide & conquer:
    - Count inversions in left half
    - Count inversions in right half
    - Count inversions across halves (pairs from left > pairs from right)
    - Merge sort modified to track cross-half inversions
  - Time: O(n log n)

- **Majority Element**
  - Find element appearing > n/2 times
  - Divide & conquer approach:
    - Recursively find majority in left/right
    - Combine: check if left or right majority is overall majority
  - Time: O(n log n)
  - Better: O(n) with voting algorithm or frequency count

- **Closest Pair of Points**
  - 2D problem: find nearest pair distance
  - Naive: O(n¬≤) all pairs
  - Divide & conquer:
    - Sort by x-coordinate once
    - Recursively find closest in left half
    - Recursively find closest in right half
    - Check pairs spanning the dividing line
    - Optimization: only check O(n) pairs near dividing line
  - Complexity: O(n log n)

- **When D&C Works Well**
  - Optimal substructure: problem has independent subproblems
  - Can combine efficiently: combine cost reasonable
  - Balanced splits: nearly equal-sized subproblems
  - Recursive structure natural: problem naturally decomposes

- **When D&C Doesn't Work**
  - Unbalanced splits: leads to O(n¬≤) (like quicksort on sorted)
  - High combining cost: can dominate
  - Better addressed by iteration or DP
  - Unnecessary overhead: simple iteration faster

- **Hybrid Approaches**
  - Combine D&C with other techniques
  - Example: quicksort with median-of-three pivot
  - Example: merge sort switching to insertion for small arrays
  - Example: counting inversions during merge step

---

### üìÖ DAY 5: BINARY SEARCH AS A PATTERN | 120 min

**Topics:**
- **Framing as Binary Search**
  - Reframe problem: "Find X such that condition(X)"
  - Condition must be monotone: false up to threshold, true after
  - Binary search finds the boundary
  - Not limited to sorted arrays

- **Monotonicity Requirement**
  - Condition must split decision space into two regions
  - If feasible(X), then typically feasible(X+1)
  - Or: if infeasible(X), then typically infeasible(X-1)
  - Must be consistent; can't oscillate

- **Capacity Planning Pattern**
  - Problem: minimize maximum load across K machines
  - Search space: [max_task, sum_all_tasks]
  - Condition: "Can we distribute tasks with capacity C?"
  - Check: greedy packing, see if all fit in C
  - Binary search on C to find minimum

- **Distance Maximization Pattern**
  - Problem: "Aggressive cows" - place N cows maximizing minimum distance
  - Search space: [1, max_distance]
  - Condition: "Can we place N cows with minimum distance D?"
  - Check: greedy placement, count if ‚â• N cows fit
  - Binary search on D to find maximum

- **Time Allocation Pattern**
  - Problem: minimize time to deliver all packages with two people
  - Search space: [max_single_time, sum_all_times]
  - Condition: "Can person 1 deliver K packages in time X?"
  - Check: greedily assign packages to person 1
  - Binary search on time X

- **Feasibility Check Implementation**
  - Must be deterministic: true/false answer
  - Must be efficient: faster than linear search
  - Example: greedy packing in O(n)
  - Example: greedy counting in O(n)

- **Binary Search Bounds**
  - Left bound: usually minimum possible value
  - Right bound: usually maximum possible value (or infinity)
  - Ensure answer exists in [left, right]
  - Typical: [1, 10^9] or [0, sum]

- **Floating-Point Binary Search**
  - For real-valued targets (e.g., find sqrt)
  - Use epsilon for convergence: |left - right| < epsilon
  - Fixed iterations: ~20-30 for 10^-6 precision
  - No integer division truncation issues

- **Precision Considerations**
  - Integer: iterate until left+1 ‚â• right
  - Floating: iterate until epsilon precision
  - Off-by-one errors: be careful with boundaries
  - Answer verification: confirm feasibility

- **Combining Multiple Feasibilities**
  - Sometimes multiple conditions to check
  - Example: "Can we fit items in knapsack?" AND "deliver in time T?"
  - Check both, binary search on constraining parameter

---

## üìå WEEK 5: TIER 1 CRITICAL PATTERNS

### Weekly Goal
Master high-frequency patterns used in large fraction of interview problems.

### Weekly Outcomes
- Recognize when hash maps/sets apply and implement efficiently
- Use monotonic stacks for range problems
- Handle interval merging and scheduling
- Apply partitioning and Kadane's algorithm
- Use fast/slow pointers for cycle detection

### Summary
This week covers patterns that appear in 50%+ of interview problems. Each pattern addresses a family of problems with similar structure. Mastering these dramatically improves problem-solving speed and accuracy.

---

### üìÖ DAY 1: HASH MAP / HASH SET PATTERNS | 90 min

**Topics:**
- **Two-Sum & Complement Patterns**
  - Problem: find two numbers summing to target
  - Hash approach:
    - Build hash set on first pass
    - Check if (target - num) in set on second pass
  - Time: O(n), Space: O(n)
  - Works for sorted, unsorted arrays

- **Variations**
  - Two-Sum II (sorted): use two pointers instead
  - Three-Sum: sort, fix one element, use two-sum for rest
  - K-Sum: recursive reduction to 2-sum

- **Frequency Counting**
  - Anagrams: two strings are anagrams iff same character frequencies
    - Map each string to frequency
    - Check equality
  - Top K Frequent: count frequencies, use heap/bucket sort
  - Majority Element: count, return element with count > n/2

- **Valid Anagram**
  - Build frequency map for both strings
  - Compare maps (or sort strings and compare)
  - Time: O(n), Space: O(alphabet_size)

- **Anagrams in List**
  - Group anagrams together
  - Normalize each string (e.g., sort characters)
  - Use map: normalized ‚Üí list of original strings
  - Time: O(n √ó k log k) where k = word length

- **Top K Frequent Elements**
  - Count frequencies: O(n)
  - Find top K: use min-heap (K-freq pairs) or bucket sort
  - Heap: O(n log K)
  - Bucket: O(n) if frequencies [1, n]

- **Membership & Deduplication**
  - Remove duplicates: convert to set, size gives count
  - Contains duplicate: iterate and check set membership
  - Unique elements: return set size

- **Intersection of Sets**
  - Two arrays: iterate one, check if in other's set
  - Multiple arrays: intersect iteratively
  - Time: O(min_array_size √ó other_array_size)

- **Word Ladder Pattern**
  - Find neighbors by changing one character
  - Use set to track visited words
  - BFS to find shortest path

---

### üìÖ DAY 2: MONOTONIC STACK | 120 min

**Topics:**
- **Monotonic Stack Concept**
  - Stack maintaining elements in increasing or decreasing order
  - When adding new element: pop larger/smaller elements
  - Top of stack is always closest qualifying element
  - O(n) total (each element added/removed once)

- **Next Greater Element**
  - Problem: for each element, find next element larger than it
  - Naive: O(n¬≤) nested loops
  - Monotonic decreasing stack:
    - Iterate right to left
    - Pop smaller elements (found next greater for them)
    - Top of stack is next greater for current
  - Time: O(n)

- **Next Smaller Element**
  - Similar: use increasing stack
  - Pop larger elements when adding new

- **Previous Greater / Previous Smaller**
  - Iterate left to right
  - Stack tracks elements processed so far

- **Stock Span Problem**
  - For each day: how many consecutive previous days had price ‚â§ current?
  - Monotonic decreasing stack with (price, span) pairs
  - Accumulate spans of popped elements
  - Time: O(n)

- **Largest Rectangle in Histogram**
  - Heights array; find largest rectangle area
  - For each bar: find left and right boundaries
  - Monotonic stack approach:
    - Maintain indices in increasing height order
    - Pop taller bars (calculate rectangles for them)
  - Time: O(n)

- **Trapping Rain Water**
  - Water trapped = min(max_left, max_right) - current_height
  - Monotonic stack or precomputed max left/right
  - For each position, track max heights around it

- **Remove K Digits to Get Smallest Number**
  - Greedy with monotonic stack:
    - Maintain increasing stack
    - Pop if can remove and current smaller
    - Limit removals to K
  - Append remaining digits
  - Remove leading zeros
  - Time: O(n)

- **Implementation Details**
  - Store indices in stack (enables boundary calculations)
  - Careful about off-by-one errors
  - Handle remaining elements after main loop

- **When to Use**
  - Need next/previous qualifying element: natural fit
  - Range problems: consider monotonic stack
  - Efficiency boost from O(n¬≤) to O(n)

---

### üìÖ DAY 3: MERGE OPERATIONS & INTERVAL PATTERNS | 120 min

**Topics:**
- **Merging Sorted Arrays**
  - Two sorted arrays: merge in O(m+n)
  - Two pointers at start of each
  - Compare and take smaller, advance pointer
  - Combine into new array or in-place

- **Merge Sorted Lists**
  - Linked lists: similar approach
  - Create new list with pointers to both
  - Traverse and link nodes

- **Merge K Sorted Lists**
  - Naive: pairwise merging O(n¬≤k) time
  - Heap approach: maintain min-heap of heads
    - Extract min, add to result
    - Add next element from same list to heap
    - Time: O(nk log k)
  - Divide & conquer: merge recursively
    - Merge pairs, then pairs of results
    - Time: O(nk log k)

- **Merge Intervals**
  - Given overlapping intervals, merge overlaps
  - Sort by start time: O(n log n)
  - Iterate and merge if overlaps: O(n)
  - Overlap condition: new_start ‚â§ current_end

- **Insert Interval**
  - Add new interval to list of non-overlapping intervals
  - Collect non-overlapping intervals before new
  - Find overlapping intervals and merge with new
  - Collect non-overlapping intervals after merged
  - Time: O(n)

- **Meeting Rooms Problems**
  - Problem 1: can one person attend all meetings?
    - Check if meetings overlap
    - Sort, then check adjacent meetings
  - Problem 2: minimum rooms needed?
    - Sweep line algorithm: sort start/end times
    - Track concurrent meetings at each time point
    - Max concurrent = min rooms needed

- **Sweep Line Algorithm Details**
  - Create events: (time, type) where type = start/end
  - Sort events by time (ends before starts if tie)
  - Process events: +1 for start, -1 for end
  - Track current and maximum concurrent

- **Applications**
  - Calendar consolidation
  - Room scheduling
  - Resource allocation
  - Time slot management

---

### üìÖ DAY 4: PARTITION & CYCLIC SORT; KADANE'S ALGORITHM | 120 min

**Topics:**
- **Dutch National Flag Problem**
  - Partition array into three regions: 0s, 1s, 2s
  - In-place partitioning in O(n)
  - Three pointers: low (0s boundary), mid (current), high (2s boundary)
  - Invariant: [0, low) = 0s, [low, mid) = 1s, [high+1, n) = 2s

- **Partitioning Around Pivot**
  - Quicksort partition: elements < pivot on left, ‚â• pivot on right
  - Two-pointer approach
  - Randomized pivot prevents worst-case

- **Move Zeroes to End**
  - In-place moving zeros to end
  - Slow pointer: next position for non-zero
  - Fast pointer: scan for non-zeros
  - Swap when found
  - Time: O(n), Space: O(1)

- **Cyclic Sort Pattern**
  - For arrays containing 1..n or 0..n-1
  - If element i belongs at position i:
    - Swap to correct position
    - Repeat until element at position i is correct
  - Time: O(n), Space: O(1)

- **Finding Missing Number Using Cyclic Sort**
  - Array [0, n] with one missing
  - Cyclic sort: place each number at index equal to value
  - Scan: first index where element ‚â† index is missing
  - Time: O(n), Space: O(1)

- **Finding Duplicate Using Cyclic Sort**
  - One number repeated
  - Cyclic sort detects: when trying to place num, position already has num
  - Return when duplicate detected
  - Time: O(n), Space: O(1)

- **First Missing Positive**
  - Array with positive/negative numbers
  - Cyclic sort on positive numbers in [1, n] range
  - Scan for first missing
  - Time: O(n), Space: O(1)

- **Kadane's Algorithm - Maximum Subarray Sum**
  - Find contiguous subarray with largest sum
  - At each position: decide continue or start new
  - max_ending_here = max(arr[i], max_ending_here + arr[i])
  - max_global = max(max_global, max_ending_here)
  - Time: O(n), Space: O(1)

- **Kadane Example**
  - Array: [-2, 1, -3, 4, -1, 2, 1, -5, 4]
  - Trace: max subarray [4, -1, 2, 1] = 6

- **Maximum Product Subarray**
  - Similar to max sum but with multiplication
  - Complication: negative √ó negative = positive
  - Track both max and min ending at each position
  - max_here = max(arr[i], arr[i] √ó max_prev, arr[i] √ó min_prev)
  - min_here = min(arr[i], arr[i] √ó max_prev, arr[i] √ó min_prev)

- **Circular Array Variant**
  - Array is circular: last connects to first
  - Two cases:
    - Normal: max subarray doesn't wrap
    - Wrapping: max subarray wraps around
  - Wrap case: total_sum - min_subarray
  - Return max of both cases

- **Variants with Constraints**
  - Subarray of exactly size K: O(n) sliding window
  - Subarray of at most K: DP or greedy approaches
  - Maximum average subarray: binary search on answer

- **Applications**
  - Stock trading: maximum profit on one transaction
  - Financial: peak revenue period analysis
  - Network: max throughput finding

---

### üìÖ DAY 5: FAST & SLOW POINTERS | 120 min

**Topics:**
- **Floyd's Cycle Detection (Tortoise & Hare)**
  - Problem: detect if linked list has cycle
  - Slow pointer: moves 1 step
  - Fast pointer: moves 2 steps
  - If cycle exists: they meet eventually
  - If no cycle: fast reaches end
  - Time: O(n), Space: O(1)

- **Why It Works**
  - In cycle: fast gains 1 step per iteration
  - Eventually fast catches slow
  - Not in cycle: fast reaches null first

- **Finding Cycle Start**
  - After detecting cycle, reset one pointer to head
  - Move both one step at a time
  - They meet at cycle start
  - Mathematical proof: distances work out

- **Cycle Length Calculation**
  - When pointers meet in cycle, count steps
  - Or: distance from head to cycle start + cycle length

- **Finding Middle of Linked List**
  - Slow moves 1 step, fast moves 2 steps
  - When fast reaches end, slow is at middle
  - Handle even/odd length lists

- **Splitting Linked List at Middle**
  - Find middle using fast/slow
  - Split into two lists at middle
  - Useful for merge sort on linked lists

- **Happy Number Problem**
  - Number is happy if repeatedly summing digit squares reaches 1
  - Unhappy if cycles infinitely
  - Use fast/slow to detect cycle
  - If cycle is [1], happy; otherwise unhappy

- **Palindrome Linked List**
  - Find middle using fast/slow
  - Reverse second half
  - Compare first and second halves
  - Time: O(n), Space: O(1)

- **Remove Nth Node from End**
  - Use two pointers with gap
  - Lead pointer moves N steps ahead
  - Then both move until lead reaches end
  - Trailer now before node to remove
  - Handle edge cases: removing head, single node

- **Reorder Linked List (L1‚ÜíLn‚ÜíL2‚ÜíLn-1‚Üí...)**
  - Find middle using fast/slow
  - Reverse second half
  - Merge two halves interleaving
  - Time: O(n), Space: O(1)

- **When to Use**
  - Cycle detection: first thought
  - Finding structure: middle, kth element
  - In-place operations on linked lists
  - Space optimization: O(1) vs O(n) with extra pointers

---

## üìå WEEK 6: TIER 1.5 STRING MANIPULATION PATTERNS

### Weekly Goal
Master string-specific patterns: palindromes, substrings, brackets, transformations, matching.

### Weekly Outcomes
- Solve all palindrome problems efficiently
- Master substring problems with sliding window
- Handle bracket matching and generation
- Perform string transformations and conversions
- Apply string matching algorithms

### Summary
This week applies pattern knowledge to strings. Strings are arrays of characters, so earlier patterns apply. Additionally, strings have unique patterns like palindromes and bracket matching that require specialized techniques.

---

### üìÖ DAY 1: PALINDROME PATTERNS | 90 min

**Topics:**
- **Simple Palindrome Check**
  - Two pointers from ends converging
  - Compare characters
  - If any mismatch: not palindrome
  - Time: O(n), Space: O(1)

- **Longest Palindromic Substring**
  - Naive: O(n¬≥) check all substrings
  - Expand around center: O(n¬≤)
    - For each center (odd and even length)
    - Expand while characters match
    - Track longest found
  - Manacher's algorithm: O(n) (advanced, optional)

- **Center Expansion Details**
  - Odd-length: single character center
  - Even-length: between two characters center
  - For each center: expand left and right simultaneously
  - Stop when mismatch or boundary

- **Valid Palindrome with Non-Alphanumeric**
  - Ignore non-alphanumeric characters
  - Two pointers skip non-letters/digits
  - Compare letters (case-insensitive)
  - Time: O(n)

- **Valid Palindrome II (Remove at Most One)**
  - Two pointers from ends
  - When mismatch: try removing left or right
  - Check if either results in palindrome
  - Recursive or iterative

- **Palindrome Partitioning**
  - Partition string into palindromic substrings
  - Backtracking: at each position, try all palindromic prefixes
  - Memoization: cache (start, end) ‚Üí is_palindrome
  - Time: O(n √ó 2^n) worst case

- **Shortest Palindrome by Adding Characters**
  - Find longest palindrome from start
  - Add reverse of suffix to start
  - KMP for efficiency

- **Count Palindromic Substrings**
  - Expand around center for each center
  - Count palindromes found
  - Time: O(n¬≤)

- **Memoization for Efficiency**
  - Cache palindrome checks
  - Table: dp[i][j] = is_palindrome(i, j)
  - Fill diagonally
  - Use in partitioning for efficiency

---

### üìÖ DAY 2: SUBSTRING & SLIDING WINDOW ON STRINGS | 120 min

**Topics:**
- **Longest Substring Without Repeating Characters**
  - Variable sliding window with character map
  - Expand right: add character
  - If character repeats: shrink left until no repeat
  - Time: O(n), Space: O(alphabet_size)

- **Character Map Tracking**
  - Map character ‚Üí last seen index
  - On repeat: move left pointer to after previous occurrence
  - Or use frequency counts

- **Longest Substring with At Most K Distinct Characters**
  - Similar approach: maintain at most K distinct
  - When K+1 distinct: shrink until K again
  - Variation: exactly K (use at-most-k minus at-most-(k-1))

- **Permutation in String (Pattern Matching)**
  - Fixed window size = pattern length
  - Check if frequency map matches pattern frequency
  - Slide window, update frequency
  - Time: O(n)

- **Frequency Matching Optimization**
  - Instead of full comparison each step
  - Track count of characters with correct frequency
  - When count equals required characters, match found

- **Find All Anagrams of Substring**
  - Similar: fixed window size
  - For each window: check if anagram of pattern
  - Collect starting indices
  - Time: O(n)

- **Minimum Window Substring**
  - Find smallest window containing all pattern characters
  - Expand right until complete
  - Shrink left while complete
  - Track best window found
  - Time: O(n)

- **Character Count Tracking**
  - Map character ‚Üí count needed
  - Update counts as slide window
  - Check if all characters have enough count

- **Substring with Concatenation of All Words**
  - Fixed window size = sum of word lengths
  - Check if window contains exactly all words
  - Use frequency map for words
  - Time: O(n √ó m¬≤) where m = number of words

- **Repeated DNA Sequences (Rolling Hash)**
  - Find 10-character sequences appearing more than once
  - Rolling hash for efficiency
  - Hash to set, check for duplicates
  - Time: O(n)

---

### üìÖ DAY 3: PARENTHESES & BRACKET MATCHING | 90 min

**Topics:**
- **Valid Parentheses via Stack**
  - Push opening brackets: (, [, {
  - On closing: pop and check if matching
  - Mismatch: invalid
  - Stack not empty at end: invalid
  - Time: O(n), Space: O(n)

- **Stack Processing**
  - Push: (‚Üí'(', [‚Üí'[', {‚Üí'{'
  - Pop and compare:
    - ')' with '(', ']' with '[', '}' with '{'
  - Mismatch or empty stack: invalid

- **Generate All Valid Parentheses**
  - N pairs: generate all valid combinations
  - Backtracking: track open count and close count
  - Only add:
    - Opening: if less than n used
    - Closing: if less than opening used
  - Time: O(Catalan number) = O(4^n / sqrt(n))

- **Catalan Number**
  - nth Catalan number: (2n)! / ((n+1)! √ó n!)
  - Grows roughly 4^n / (n^1.5)
  - Used in many combinatorial problems

- **Longest Valid Parentheses**
  - Find longest valid substring
  - Stack approach: store indices
    - Push -1 initially
    - On '(': push index
    - On ')': pop index
      - If empty: push current (start of new potential)
      - Else: length = current - top of stack
  - Time: O(n)

- **Minimum Remove to Make Valid Parentheses**
  - Remove minimum characters to make valid
  - Two-pass:
    - First pass: mark invalid ones
    - Second pass: build result without marked
  - Or use stack to track invalid indices

- **Balanced Parentheses with Limits**
  - Maximum depth K
  - Track current depth
  - Reject if exceed K
  - Validate as building

- **Score of Parentheses**
  - Each pair contributes: inner score √ó 2
  - Nested pairs multiply contributions
  - Use stack to track scores
  - Time: O(n)

---

### üìÖ DAY 4: STRING TRANSFORMATIONS & BUILDING | 120 min

**Topics:**
- **String to Integer (atoi)**
  - Parse string to integer
  - Handle whitespace: trim leading
  - Handle sign: +/-
  - Handle non-digit characters: stop
  - Handle overflow: check bounds
  - Time: O(n)

- **Whitespace Trimming**
  - Skip leading spaces
  - Read optional sign: + or -
  - Read digits
  - Stop on non-digit

- **Integer to String**
  - Reverse digits and build
  - Handle negative numbers (sign separately)
  - Handle zero
  - Time: O(log n) for n bits

- **Integer ‚Üî Roman Numerals**
  - Mapping: {1000: 'M', 900: 'CM', 500: 'D', ...}
  - Integer to Roman: greedily use largest symbols
  - Roman to Integer: sum values (handle subtractive cases)
  - Time: O(1) for fixed range

- **Subtractive Notation**
  - IV = 4, IX = 9, XL = 40, XC = 90, CD = 400, CM = 900
  - When smaller precedes larger: subtract
  - Otherwise: add

- **Zigzag Conversion**
  - Read string in zigzag pattern, output row by row
  - Simulate zigzag or use index formula
  - Character at row depends on k (number of rows) and position

- **Reverse String / Reverse Words**
  - Reverse entire string: O(1) space using two pointers
  - Reverse words: reverse entire, then reverse each word
  - In-place operations possible

- **String Compression (RLE)**
  - Replace runs with character + count
  - Example: "aabbbccc" ‚Üí "a2b3c3"
  - Track when character changes
  - Time: O(n)

- **Decode String**
  - Pattern: "3[a2[c]]" ‚Üí "accaccacc"
  - Use stack to handle nesting
  - Push: characters and counts
  - On ']': pop and decode
  - Time: O(output length)

- **StringBuilder for Efficiency**
  - String immutability in many languages
  - Use StringBuilder instead of concatenation
  - Concatenation in loop: O(n¬≤) ‚Üí builder: O(n)
  - Avoid repeated copies

---

### üìÖ DAY 5 (OPTIONAL): STRING MATCHING & ROLLING HASH | 90 min

**Topics:**
- **Karp-Rabin Algorithm**
  - Rolling hash for pattern matching
  - Compute hash of pattern and first window
  - Slide window, update hash in O(1)
  - On match: verify full string (avoid false positives)
  - Time: O(n + m) average, O((n-m) √ó m) worst case

- **Rolling Hash Formula**
  - Hash = (s[0] √ó b^(m-1) + s[1] √ó b^(m-2) + ... + s[m-1]) mod p
  - Update: (hash - s[i] √ó b^(m-1)) √ó b + s[i+m] mod p
  - Choose: b = 256 (alphabet size), p = large prime

- **Collision Handling**
  - On hash match: verify actual string (Karp-Rabin feature)
  - Reduces false positives
  - Handles hash collisions transparently

- **Repeated Substring Pattern**
  - Find if string is repetition of smaller string
  - Rolling hash: efficient checking
  - Or use string matching: pattern appears in (s+s) without boundaries

- **Find Repeated DNA Sequences**
  - 10-character sequences in DNA string
  - Rolling hash to find all 10-mers
  - Store in set, find duplicates
  - Alphabet = 4 (A, C, G, T)

- **Comparing Pattern Matching Algorithms**
  - KMP: O(n+m), no extra space
  - Boyer-Moore: O(n/m) average, slow on short patterns
  - Karp-Rabin: O(n+m) average, simple hash computation
  - Choice depends on application and alphabet size

---

# üü® PHASE C: TREES, GRAPHS & DYNAMIC PROGRAMMING
## Weeks 7-11 | 60-70 hours | Advanced Data Structures & Algorithms

### Phase Goals & Outcomes
**Goals:** Master tree structures, graph algorithms, and dynamic programming techniques.

**Learning Outcomes:**
- Implement and traverse binary trees and graphs
- Build and operate balanced search trees
- Solve shortest path and minimum spanning tree problems
- Master dynamic programming from fundamentals to advanced patterns
- Apply DP to trees, DAGs, and subset problems

---

## üìå WEEK 7: TREES & BALANCED SEARCH TREES

### Weekly Goal
Understand tree structures, traversals, BSTs, and balance mechanisms for O(log n) operations.

### Weekly Outcomes
- Perform all tree traversals (preorder, inorder, postorder, level-order)
- Implement BST operations (search, insert, delete)
- Understand balance requirements for performance
- Solve tree pattern problems efficiently
- Apply augmented trees to advanced queries

### Summary
This week introduces hierarchical data structures. Trees are fundamental for organizing data and enabling efficient search. Balanced trees guarantee logarithmic operations by maintaining height bound. Students learn when and how to use different tree types.

---

### üìÖ DAY 1: BINARY TREES & TRAVERSALS | 120 min

**Topics:**
- **Tree Terminology**
  - Root: topmost node, entry point
  - Leaf: node with no children
  - Parent, child, sibling: relationships
  - Depth: distance from root
  - Height: distance to farthest leaf
  - Ancestor, descendant: relationships

- **Tree Types**
  - Full tree: every internal node has exactly 2 children
  - Complete tree: filled left-to-right, bottom level may be partial
  - Balanced tree: height O(log n)

- **Preorder Traversal (Root, Left, Right)**
  - Visit node, then left subtree, then right subtree
  - Use case: copying tree, serialization
  - Recursive: visit(), recurse_left(), recurse_right()
  - Iterative: use stack, pop and process, push right then left

- **Inorder Traversal (Left, Root, Right)**
  - Left subtree, visit node, right subtree
  - For BSTs: produces sorted sequence
  - Key for BST applications
  - Recursive or iterative with stack

- **Postorder Traversal (Left, Right, Root)**
  - Left subtree, right subtree, visit node
  - Use case: deleting tree (delete children before parent)
  - Computing heights, sizes bottom-up
  - Use stack or recursion

- **Level-Order Traversal (BFS)**
  - Visit level by level from top to bottom
  - Use queue instead of stack
  - Essential for finding path lengths, siblings
  - Tree serialization level-by-level

- **Iterative Traversals via Stack**
  - Preorder: push node, pop-process-push(right, left)
  - Inorder: push all left, pop-process-push-right
  - Postorder: more complex, two stacks or careful ordering

- **Spiral Traversal (Zigzag)**
  - Alternate direction each level
  - Use queue and direction flag
  - Collect results in lists per level

- **Boundary Traversal**
  - Left boundary (without leaves), bottom leaves, right boundary (reversed, without leaves)
  - Combination of different traversals

---

### üìÖ DAY 2: BINARY SEARCH TREES (BSTs) | 120 min

**Topics:**
- **BST Property & Definition**
  - For each node: all left descendants < node < all right descendants
  - Enables efficient search, insert, delete
  - Inorder traversal produces sorted sequence

- **Search in BST**
  - Recursive: compare, recurse left or right
  - Time: O(h) where h = height
  - Best: O(log n) for balanced tree
  - Worst: O(n) if tree is degenerate (linked list-like)

- **Insert in BST**
  - Find correct position: follow comparisons
  - Create new node and link
  - Time: O(h)
  - No duplicates (usually) or allow with convention

- **Delete from BST**
  - Case 1: Leaf node - simply remove
  - Case 2: One child - replace with child
  - Case 3: Two children:
    - Find in-order successor (smallest in right subtree)
    - Replace node value with successor value
    - Delete successor (now has ‚â§ 1 child)
  - Time: O(h)

- **In-Order Successor/Predecessor**
  - Successor: smallest element in right subtree
  - Predecessor: largest element in left subtree
  - Or: traverse to find in in-order sequence

- **Verification of BST**
  - Simple check: left < node < right (incorrect for nested)
  - Correct: maintain min/max bounds as traverse
  - Each left subtree: must be < current node
  - Each right subtree: must be > current node

- **Building BST from Sorted Array**
  - Insert middle element, recursively build left and right
  - Ensures balanced tree
  - Time: O(n)

- **Degenerate BST Problem**
  - Sorted input creates linear tree (height n)
  - Random insertion prevents this
  - Motivation for balanced BSTs

- **Balanced BST Search**
  - O(log n) guarantee: height bounded by log(n)
  - Most operations: O(log n)
  - Examples: AVL, Red-Black, B-trees

---

### üìÖ DAY 3: BALANCED BSTS - AVL & RED-BLACK (OVERVIEW) | 120 min

**Topics:**
- **Why Balance Matters**
  - Unbalanced BST: degenerates to O(n)
  - Balanced BST: guarantees O(log n)
  - Self-balancing: maintain balance during insert/delete

- **AVL Trees**
  - Balance factor: height(left) - height(right)
  - Invariant: balance factor ‚àà {-1, 0, 1}
  - Four rotation cases:
    - Left-left (LL): single right rotation
    - Right-right (RR): single left rotation
    - Left-right (LR): left rotation then right rotation
    - Right-left (RL): right rotation then left rotation
  - After insertion: check all ancestors, rebalance if needed
  - After deletion: similar process

- **Rotations**
  - Left rotation: move right child up, left child of right becomes right child
  - Right rotation: symmetric
  - Preserve BST property during rotation
  - Update heights/balances

- **Red-Black Trees**
  - Color each node red or black
  - Invariants:
    - Root is black
    - Red node has black children
    - All root-to-leaf paths have same number of black nodes
  - Guarantees: height ‚â§ 2 log(n+1)
  - Fewer rotations than AVL on average
  - More relaxed balance than AVL

- **Complexity Summary**
  - All balanced BSTs: O(log n) search, insert, delete
  - Space: O(n)
  - Practical differences: constants and rotation frequency

- **When to Use Which**
  - AVL: more rigid balance, faster search, slower insert/delete
  - Red-Black: more relaxed balance, faster insert/delete, comparable search
  - For interviews: understand concept, implement basic, use library

- **Real-World Implementations**
  - C++ std::map: red-black trees
  - Java TreeMap: red-black trees
  - Python: collections.OrderedDict (insertion order, not balance)

- **Self-Balancing Not Required**
  - Usually don't implement balance mechanisms in interviews
  - Use library implementations (TreeMap, TreeSet)
  - Understand trade-offs and when to use

---

### üìÖ DAY 4: TREE PATTERNS | 120 min

**Topics:**
- **Path Sum Problems**
  - Root to leaf sum: DFS from root, accumulate sum, check at leaves
  - Path sum with target: recursively check left and right
  - General path sum: any node to any descendant

- **Root-to-Leaf Paths**
  - Find all paths from root to leaves
  - Backtracking: build path as go down
  - At leaf: record path
  - Backtrack: remove node from path

- **Tree Diameter**
  - Longest path in tree (not necessarily through root)
  - For each node: compute height of left and right subtrees
  - Diameter through this node: left_height + right_height + 1
  - Recurrence: diameter = max(left_diameter, right_diameter, left_height + right_height + 1)
  - Time: O(n)

- **Lowest Common Ancestor (LCA)**
  - Simplest approach: find paths to both nodes, compare
  - For general trees: DFS and find common ancestor
  - Binary lifting: preprocess for O(log n) queries

- **Height vs Depth**
  - Depth: distance from root (BFS)
  - Height: distance to farthest leaf (DFS from each node)
  - Often computed together in single DFS

- **Subtree Problems**
  - Identical subtrees: compare structures recursively
  - Subtree matching: check if one tree is subtree of another
  - Serialize subtrees for efficient comparison

- **Serialization & Deserialization**
  - Level-order based: store structure info (null nodes)
  - Preorder/postorder based: encode in specific order
  - Deserialization: reconstruct from encoding
  - Applications: persistence, transmission

- **Balanced Tree Check**
  - For each node: check balance factor (difference of heights)
  - Recursive check: efficient O(n) vs naive O(n¬≤)
  - Prune early if imbalanced subtree found

- **Lowest Common Ancestor Variants**
  - With parent pointers: climb up from both nodes
  - Without parent: DFS and find common ancestor
  - Binary lifting: precompute ancestors at powers of 2

---

### üìÖ DAY 5 (OPTIONAL): AUGMENTED TREES & ORDER-STATISTICS | 90 min

**Topics:**
- **Augmenting Trees with Metadata**
  - Store additional info at each node
  - Example: subtree size at each node
  - Use in recursive calculations

- **Order-Statistics Trees**
  - Store size of each subtree
  - kth smallest element: compare k with left subtree size
  - Rank queries: find how many elements ‚â§ x
  - Time: O(log n) with balanced tree

- **Building Order-Statistics Tree**
  - Start with BST with sizes
  - On insert: update sizes of all ancestors
  - On delete: similar update
  - Maintain balance for O(log n) operations

- **Range Count Queries**
  - Count elements in [L, R] efficiently
  - Use augmented BST to prune subtrees
  - Time: O(log n)

- **Applications in Systems**
  - Database indexes: find kth record efficiently
  - Statistical calculations: percentiles
  - Range searching: spatial databases

- **Other Augmentations**
  - Sum of subtree: compute range sum queries
  - Min/max of subtree: range min/max queries
  - Segment trees: more complex augmentation

---

## üìå WEEK 8: GRAPH FUNDAMENTALS

### Weekly Goal
Master graph models and core traversal algorithms. Build intuition for graph structures and basic exploration.

### Weekly Outcomes
- Represent graphs using appropriate data structures
- Perform BFS for shortest paths and connectivity
- Perform DFS for exploration and topological sorting
- Detect cycles and find connected components
- Apply BFS/DFS to solve diverse graph problems

### Summary
Graphs model relationships and enable powerful algorithms. This week covers fundamental concepts and traversals used in most graph problems. Students learn representations, traversals, and how to apply them systematically.

---

### üìÖ DAY 1: GRAPH MODELS & REPRESENTATIONS | 90 min

**Topics:**
- **Graph Types**
  - Directed vs undirected: edges have direction or not
  - Weighted vs unweighted: edges have costs or not
  - Sparse vs dense: few vs many edges relative to vertices

- **Adjacency Matrix Representation**
  - 2D array: matrix[i][j] = weight (or 1 if present)
  - Space: O(V¬≤)
  - Time: O(1) edge lookup, O(V) to list neighbors
  - Best for: dense graphs, small graphs
  - Worst for: sparse graphs (wasteful space)

- **Adjacency List Representation**
  - Array of lists: list[i] = neighbors of i
  - Space: O(V + E)
  - Time: O(degree) to list neighbors, O(degree) edge lookup
  - Best for: sparse graphs, large graphs
  - Most common in interviews

- **Edge List Representation**
  - List of edges: [(u, v, weight), ...]
  - Space: O(E)
  - Time: O(E) to find neighbors (linear scan)
  - Use case: some algorithms like Kruskal MST

- **Choosing Representation**
  - Sparse (E << V¬≤): adjacency list
  - Dense (E ‚âà V¬≤): adjacency matrix
  - Space-time tradeoff important

- **Implicit Graphs**
  - Graph defined by rules, not explicit storage
  - Example: grids (2D arrays with neighbors defined by adjacency)
  - Example: game states (each state is node, moves are edges)
  - Explore via DFS/BFS without building graph

- **Graph Modeling from Problems**
  - Identify nodes: elements, states, positions
  - Identify edges: relationships, transitions, adjacencies
  - Identify weights: costs, distances, probabilities
  - Example: social network ‚Üí nodes=people, edges=friendships

- **Building Graphs from Input**
  - Parse input to build adjacency list
  - Handle directed vs undirected (add reverse edge?)
  - Handle weights (store with edges)
  - Directed: add edge u‚Üív only
  - Undirected: add edges u‚Üîv

---

### üìÖ DAY 2: BREADTH-FIRST SEARCH (BFS) | 120 min

**Topics:**
- **BFS Algorithm Template**
  - Queue-based frontier expansion
  - Visited set to avoid revisiting
  - Process nodes layer by layer
  - Time: O(V + E)

- **BFS from Single Source**
  - Start from source node
  - Add to queue and mark visited
  - While queue not empty:
    - Dequeue node
    - Process node
    - Enqueue unvisited neighbors
  - Guarantees shortest path in unweighted graphs

- **Shortest Path (Unweighted)**
  - Distance[node] = layer at which node discovered
  - Parent tracking enables path reconstruction
  - Distance[source] = 0, distance[neighbors] = 1, etc.
  - Time: O(V + E)

- **Connected Components**
  - For undirected graphs: run BFS from each unvisited node
  - Each BFS traversal = one component
  - Count components: number of times BFS initiated

- **Bipartite Graph Check**
  - Color nodes with two colors
  - For each node: neighbors must have different color
  - BFS: assign colors, check conflicts
  - Bipartite iff no conflicts
  - Time: O(V + E)

- **Level-Order Tree Traversal**
  - BFS on tree structure
  - Layer-wise processing
  - Same template as graph BFS

- **Shortest Path Applications**
  - Social networks: degrees of separation
  - Route finding: quickest path on unweighted network
  - Puzzle solving: minimum moves

- **BFS Optimizations**
  - Bidirectional BFS: search from both ends (faster for large graphs)
  - Early termination: if goal found, stop
  - Queue optimization: use deque for efficiency

---

### üìÖ DAY 3: DEPTH-FIRST SEARCH & TOPOLOGICAL SORT | 120 min

**Topics:**
- **DFS Algorithm Template**
  - Recursive exploration (or stack-based)
  - Visit node, explore all descendants, backtrack
  - Time: O(V + E)

- **Recursive DFS**
  - Mark visited, process node
  - Recursively visit unvisited neighbors
  - Implicit stack (function call stack)

- **Iterative DFS (Stack-Based)**
  - Explicit stack instead of recursion
  - Push start node
  - While stack not empty:
    - Pop node
    - Process if not visited
    - Push unvisited neighbors

- **DFS Tree & Edge Types**
  - Tree edges: edges to unvisited nodes
  - Back edges: edges to ancestor (indicates cycle in directed graph)
  - Forward edges: edges to descendant (only in directed)
  - Cross edges: edges to already-finished node (only in directed)
  - Classification depends on DFS order

- **Cycle Detection in Directed Graphs**
  - Use DFS with states: unvisited, visiting, visited
  - Back edge indicates cycle
  - White-gray-black coloring: unvisited=white, visiting=gray, visited=black
  - Back edge appears when visiting node with gray neighbor

- **Topological Sort - DFS Approach**
  - DFS post-order: nodes finished later come first
  - Reverse post-order: topological sort for DAG
  - Time: O(V + E)

- **Topological Sort - Kahn's Algorithm (BFS)**
  - In-degree based approach
  - Process nodes with in-degree 0
  - Decrease in-degree of neighbors
  - Add to sort when in-degree becomes 0
  - Time: O(V + E)

- **Applications of DFS**
  - Path existence: can reach B from A?
  - Connected components: similar to BFS
  - Topological sorting: dependency ordering
  - Strongly connected components: advanced

- **DFS vs BFS**
  - DFS: uses stack, explores depth first
  - BFS: uses queue, explores breadth first
  - Choose based on problem needs

---

### üìÖ DAY 4: CONNECTIVITY & BIPARTITE GRAPHS | 90 min

**Topics:**
- **Connected Components (Undirected)**
  - Nodes are grouped if reachable from each other
  - DFS/BFS from each unvisited node finds one component
  - Count components
  - Label each component

- **Strongly Connected Components (Directed)**
  - Nodes in SCC can reach each other (cycle exists)
  - Kosaraju algorithm: two-pass DFS
  - Tarjan algorithm: single-pass DFS
  - (High-level only for interviews)

- **Bipartite Graph Check**
  - Graph is bipartite iff 2-colorable
  - No odd-length cycles
  - Use BFS or DFS to color
  - Time: O(V + E)

- **2-Coloring Problem**
  - Assign 0/1 to nodes such that adjacent nodes differ
  - BFS/DFS and color checking
  - Detect conflict on neighbor already colored

- **Bridges & Articulation Points**
  - Bridge: edge whose removal disconnects graph
  - Articulation point: node whose removal disconnects graph
  - Applications: network reliability, road networks
  - (High-level, detailed algorithms optional)

- **Weakly Connected Components (Directed)**
  - Nodes in same component if connected when ignoring direction
  - DFS treating graph as undirected

- **Applications**
  - Grouping objects with relations
  - Clustering in networks
  - Checking bipartiteness in matching problems

---

### üìÖ DAY 5 (OPTIONAL): STRONGLY CONNECTED COMPONENTS | 90 min

**Topics:**
- **SCC Definition**
  - Set of nodes where each can reach each other
  - Graph of SCCs: collapsing each SCC to one node creates DAG

- **Kosaraju Algorithm**
  - First DFS: compute finish times (post-order)
  - Transpose graph: reverse all edges
  - Second DFS on transposed: visit in decreasing finish time order
  - Each DFS tree in second pass is one SCC
  - Time: O(V + E)

- **Kosaraju Details**
  - Stack of finished nodes from first pass
  - Process stack in second DFS
  - Each DFS tree is one SCC

- **Tarjan Algorithm (Conceptual)**
  - Single-pass DFS with low-link values
  - Uses stack to track SCC candidate nodes
  - Identifies SCC complete during DFS
  - Time: O(V + E)

- **Applications**
  - Deadlock detection: processes in cycle = deadlock
  - Condensation graph: collapse SCC to analyze overall flow
  - Schedule problems: dependencies

- **Component DAG**
  - After finding SCCs, build DAG of components
  - Topological sort on component DAG

---

## üìå WEEK 9: SHORTEST PATHS, MST & UNION-FIND

### Weekly Goal
Learn fundamental optimization algorithms on graphs. Master shortest path and minimum spanning tree.

### Weekly Outcomes
- Implement Dijkstra's algorithm for single-source shortest paths
- Handle negative weights with Bellman-Ford
- Compute all-pairs shortest paths
- Build minimum spanning trees efficiently
- Use Union-Find for connectivity and MST

### Summary
This week covers two major graph optimization problems: finding shortest paths and building minimum spanning trees. Dijkstra's is essential for many applications; Bellman-Ford handles negative weights; Floyd-Warshall solves all-pairs; Kruskal and Prim build MSTs; Union-Find enables efficient MST and connectivity queries.

---

### üìÖ DAY 1: DIJKSTRA'S ALGORITHM | 120 min

**Topics:**
- **Single-Source Shortest Path Problem**
  - Given: source node, weighted graph
  - Find: shortest path to all nodes from source
  - Constraint: non-negative weights

- **Dijkstra Algorithm Overview**
  - Greedy algorithm: always expand nearest unvisited node
  - Use priority queue for efficiency
  - Time: O((V + E) log V) with binary heap

- **Dijkstra Details**
  - Distance[source] = 0, others = infinity
  - Priority queue: (distance, node)
  - While queue not empty:
    - Extract min distance node
    - For each neighbor: relax edge
    - Relaxation: if distance[u] + weight < distance[v], update
  - Extract ensures smallest distance processed

- **Relaxation Operation**
  - Core operation: try to improve distance via current edge
  - If new path shorter: update distance and add to queue
  - Edge weight must be non-negative for correctness

- **Why Non-Negative?**
  - Greedy choice assumes explored nodes won't improve
  - Negative weights violate this assumption
  - Negative cycles are undefined (infinite improvement)

- **Complexity Analysis**
  - Extract min: O(log V) √ó V times = O(V log V)
  - Relaxation: O(1) √ó E times = O(E)
  - Total: O(V log V + E log V) = O((V + E) log V)
  - With Fibonacci heap: O(V log V + E)

- **Path Reconstruction**
  - Store parent[node] when updating distance
  - Trace back from destination to source

- **Applications**
  - GPS navigation: shortest route
  - Network routing: minimum latency path
  - Social networks: degrees of separation (unweighted)

- **Variants**
  - Single-target: can run backward from target
  - Multiple sources: modify initialization
  - With constraints: time windows, capacity limits

---

### üìÖ DAY 2: BELLMAN-FORD & NEGATIVE WEIGHTS | 120 min

**Topics:**
- **Problem: Negative Weights**
  - Dijkstra fails with negative weights
  - Need algorithm handling negatives but not negative cycles

- **Bellman-Ford Algorithm**
  - Relax all edges repeatedly: V-1 times
  - Each iteration: process all edges, improve distances
  - After V-1 iterations: shortest paths found (if no negative cycle)

- **Bellman-Ford Details**
  - Distance[source] = 0, others = infinity
  - For i = 1 to V-1:
    - For each edge (u, v):
      - Relax: if distance[u] + weight < distance[v], update
  - Time: O(V √ó E)

- **Negative Cycle Detection**
  - Extra iteration: if distances improve, negative cycle exists
  - May not be reachable from source

- **Why It Works**
  - Shortest path has at most V-1 edges
  - Each iteration finds paths with one more edge
  - After V-1 iterations: optimal found

- **Complexity**
  - Time: O(V √ó E) vs O((V + E) log V) for Dijkstra
  - Slower but handles negatives
  - Dense graphs: similar (E ‚âà V¬≤)
  - Sparse graphs: Bellman-Ford worse

- **Applications**
  - Currency exchange arbitrage detection
  - Finance: detecting profit opportunities
  - Networks with negative costs (rarely)

- **SPFA Optimization**
  - Shortest Path Faster Algorithm: queue-based
  - Process only edges from updated nodes
  - Average case O(E), worst case O(V √ó E)

---

### üìÖ DAY 3: FLOYD-WARSHALL (ALL-PAIRS SHORTEST PATHS) | 120 min

**Topics:**
- **Problem: All-Pairs Shortest Paths**
  - Find shortest paths between all pairs of nodes
  - Different from running Dijkstra V times

- **Floyd-Warshall Algorithm**
  - DP formulation: intermediate vertices up to k
  - dist[i][j][k] = shortest path from i to j using vertices 0..k
  - Base: dist[i][j][0] = direct edge weight or infinity
  - Recurrence: dist[i][j][k] = min(dist[i][j][k-1], dist[i][k][k-1] + dist[k][j][k-1])
  - Optimize: can use 2D array and update in-place

- **Floyd-Warshall Implementation**
  - Initialize: copy adjacency matrix
  - Three nested loops: for k, i, j (order matters!)
  - Update: dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])
  - Time: O(V¬≥)

- **Complexity Analysis**
  - Time: O(V¬≥) - triple nested loops
  - Space: O(V¬≤) - distance matrix
  - Works for negative weights (but not negative cycles)
  - Detectable: if dist[i][i] < 0 after algorithm

- **When to Use**
  - Small graphs: V ‚â§ 500
  - Need all pairs distances
  - Dense graphs
  - Simple implementation preferred

- **Path Reconstruction**
  - Store next[i][j]: next node on shortest path from i to j
  - On update: set next[i][j] = k (intermediate node)
  - Trace path using next matrix

- **Applications**
  - Transitive closure: which pairs are connected
  - Network reliability: min latency between all pairs
  - Game analysis: all distances in game state graph

---

### üìÖ DAY 4: MINIMUM SPANNING TREES - KRUSKAL & PRIM | 120 min

**Topics:**
- **MST Problem**
  - Spanning tree: connects all V vertices with V-1 edges, no cycles
  - Minimum spanning tree: spanning tree with minimum total weight
  - May not be unique if edge weights repeat

- **Kruskal Algorithm (Edge-Based)**
  - Sort edges by weight ascending
  - Iterate edges: add if doesn't create cycle
  - Use DSU (Disjoint Set Union) to detect cycles
  - Time: O(E log E) for sorting + O(E Œ±(V)) for DSU

- **Kruskal Details**
  - Sort edges: O(E log E)
  - For each edge:
    - Find components of endpoints
    - If different: add edge, union components
  - Continue until V-1 edges added or edges exhausted

- **Prim Algorithm (Vertex-Based)**
  - Start with one vertex
  - Grow tree by always adding minimum weight edge to new vertex
  - Use priority queue
  - Time: O((V + E) log V) with binary heap

- **Prim Details**
  - Start: add source to tree
  - While tree not complete:
    - Find minimum weight edge from tree to non-tree vertex
    - Add to tree
  - Priority queue helps find minimum edge

- **Cut Property & Correctness**
  - Cut: partition vertices into two sets
  - Minimum edge crossing cut is in some MST
  - Exchange argument: if used different edge, can swap for minimum

- **Kruskal vs Prim**
  - Kruskal: works on disconnected graphs (produces forest)
  - Prim: requires connected component
  - Kruskal: good for sparse graphs (E small)
  - Prim: good for dense graphs (E large)
  - Both O((V + E) log V) or better with optimization

- **Applications**
  - Network design: minimum cost to connect all nodes
  - Clustering: build hierarchy of clusters
  - Traveling salesman: MST approximation

---

### üìÖ DAY 5: UNION-FIND (DISJOINT SET UNION) | 90 min

**Topics:**
- **Disjoint Set Problem**
  - Maintain partitions of elements
  - Operations: find (which set?), union (merge sets)
  - Applications: connectivity, cycle detection, MST

- **Union-Find Data Structure**
  - Array parent[i]: parent of i in forest
  - Find: follow parent pointers to root
  - Union: merge two trees

- **Path Compression Optimization**
  - During find: point all nodes directly to root
  - Flatten tree structure
  - Amortized O(Œ±(n)) per operation (inverse Ackermann)
  - Practical: essentially O(1)

- **Union by Rank Optimization**
  - Track tree rank (height)
  - Union: attach smaller tree to larger
  - Prevents degeneration into linked list
  - Combined with path compression: O(Œ±(n))

- **Implementation**
  - Find with path compression:
    ```
    def find(x):
      if parent[x] != x:
        parent[x] = find(parent[x])  # compress path
      return parent[x]
    ```
  - Union by rank:
    ```
    def union(x, y):
      rx, ry = find(x), find(y)
      if rank[rx] < rank[ry]: rx, ry = ry, rx
      parent[ry] = rx
      if rank[rx] == rank[ry]: rank[rx] += 1
    ```

- **Complexity Analysis**
  - Without optimization: O(n) worst case
  - With path compression: O(n log n)
  - With union by rank: O(n log n)
  - With both: O(n Œ±(n)) ‚âà O(n)

- **Applications**
  - Kruskel MST: detect cycles
  - Network connectivity queries
  - Finding connected components
  - Offline dynamic connectivity

---

# üü® PHASE C (continued): DYNAMIC PROGRAMMING
## Weeks 10-11 | 30-35 hours | Mastering Optimization via DP

### Phase Goals (DP Focus)

**Learning Outcomes:**
- Recognize overlapping subproblems and optimal substructure
- Master top-down (memoization) and bottom-up (tabulation) approaches
- Solve 1D, 2D, and sequence-based DP problems
- Apply DP to trees, DAGs, and subsets
- Optimize DP using space and time reduction techniques

---

## üìå WEEK 10: DYNAMIC PROGRAMMING I - FUNDAMENTALS

### Weekly Goal
Master DP fundamentals from recursion + memoization to tabulation, solving 1D, 2D, and sequence problems.

### Weekly Outcomes
- Identify overlapping subproblems and optimal substructure
- Implement top-down and bottom-up DP
- Solve knapsack family problems
- Master grid and edit distance problems
- Apply DP to sequences

### Summary
Dynamic programming is a powerful optimization technique. This week teaches the mindset: break into subproblems, cache results, build up solution. Students learn the fundamental patterns that apply across hundreds of problems.

---

### üìÖ DAY 1: DP FUNDAMENTALS | 90 min

**Topics:**
- **Overlapping Subproblems**
  - Same subproblem solved repeatedly in recursion
  - Example: Fibonacci
    - fib(5) = fib(4) + fib(3)
    - fib(4) = fib(3) + fib(2)
    - fib(3) computed multiple times
  - Exponential time without caching

- **Optimal Substructure**
  - Optimal solution contains optimal solutions to subproblems
  - Example: max subarray = max of subarrays ending at each position
  - Allows recursive formulation

- **Memoization (Top-Down DP)**
  - Check cache first before computing
  - If cached: return immediately
  - Else: compute, store, return
  - Dramatically reduces time

- **Fibonacci with Memoization**
  - memo[n] = fib(n) result
  - Base: memo[0]=0, memo[1]=1
  - Recursive: if n in memo return memo[n], else compute
  - Time: O(n) instead of O(2^n)

- **Tabulation (Bottom-Up DP)**
  - Solve small problems first
  - Build up to full problem
  - No recursion, just iteration
  - More efficient (fewer function calls)
  - Need to identify order of computation

- **DP Table & State Definition**
  - State: what does each cell represent?
  - Transitions: how to compute from other cells?
  - Base case: simplest subproblems?
  - Answer: which cell contains solution?

- **Example: Climbing Stairs**
  - Problem: n stairs, can take 1 or 2 steps per move
  - State: ways[i] = number of ways to reach stair i
  - Base: ways[0]=1 (at start), ways[1]=1 (one way)
  - Transition: ways[i] = ways[i-1] + ways[i-2]
  - Answer: ways[n]

---

### üìÖ DAY 2: 1D DP & KNAPSACK FAMILY | 120 min

**Topics:**
- **1D DP Patterns**
  - Array dp[i] representing answer for problem of size i
  - Transition: dp[i] depends on dp[j] for j < i
  - Time: O(n), Space: O(n) or O(1) if optimizable

- **Climbing Stairs Variants**
  - Basic: ways to reach stair n taking 1 or 2 steps
  - With cost: minimum cost to reach stair n
  - With constraints: can't take consecutive 2-step moves
  - General: given step sizes, compute answer

- **House Robber (Non-Adjacent Sum)**
  - Given array of house values, rob non-adjacent houses
  - Maximize: rob[i] = max(rob[i-1], rob[i-2] + arr[i])
  - Intuition: at each house, either rob it or skip
  - Time: O(n), Space: O(1) optimized

- **House Robber II (Circular)**
  - Houses in circle: can't rob first and last together
  - Solution: max(rob excluding first, rob excluding last)

- **Coin Change**
  - Minimum coins to make amount
  - State: dp[i] = min coins for amount i
  - Transition: dp[i] = min(dp[i-coin] + 1 for each coin)
  - Time: O(amount √ó coins)

- **Coin Change II (Number of Ways)**
  - Count ways to make amount
  - Different DP: dp[i] = number of ways for amount i
  - Transition: dp[i] += dp[i-coin] for each coin
  - Careful: order matters (different from permutations)

- **0/1 Knapsack (Single Item)**
  - Given items with weight and value, capacity W
  - Maximize value without exceeding weight
  - Classic DP problem
  - dp[i][w] = max value with first i items and weight w
  - Transition: take or skip item
  - Time: O(n √ó W), Space: O(W) optimized

- **Unbounded Knapsack**
  - Each item available unlimited times
  - dp[i] = max value with capacity i
  - Transition: dp[i] = max(dp[i-weight[j]] + value[j] for all items j)
  - Time: O(W √ó items)

---

### üìÖ DAY 3: 2D DP - GRIDS & EDIT DISTANCE | 120 min

**Topics:**
- **2D Grid DP**
  - State: dp[i][j] = answer for cell (i, j)
  - Often: number of ways or minimum cost
  - Transition: combine from previous cells

- **Unique Paths in Grid**
  - Given grid with obstacles, count unique paths from top-left to bottom-right
  - Can move right or down
  - State: dp[i][j] = ways to reach (i, j)
  - Base: dp[0][0] = 1 if no obstacle
  - Transition: dp[i][j] = dp[i-1][j] + dp[i][j-1] if no obstacle
  - Time: O(m √ó n)

- **Minimum Path Sum**
  - Given grid with costs, find minimum cost path
  - dp[i][j] = minimum cost to reach (i, j)
  - Transition: dp[i][j] = cost[i][j] + min(dp[i-1][j], dp[i][j-1])

- **Edit Distance (Levenshtein)**
  - Transform word1 to word2 with insert/delete/replace
  - State: dp[i][j] = min edits to transform word1[0..i-1] to word2[0..j-1]
  - Transitions:
    - If characters match: dp[i][j] = dp[i-1][j-1]
    - Else: min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1
  - Time: O(m √ó n)

- **Longest Common Subsequence (LCS)**
  - Longest sequence present in both strings
  - State: dp[i][j] = length of LCS of word1[0..i-1] and word2[0..j-1]
  - Transitions:
    - If match: dp[i][j] = dp[i-1][j-1] + 1
    - Else: dp[i][j] = max(dp[i-1][j], dp[i][j-1])
  - Time: O(m √ó n)

- **Space Optimization**
  - 2D DP often reducible to 1D if only previous row needed
  - Save space from O(m √ó n) to O(n)

---

### üìÖ DAY 4: DP ON SEQUENCES | 120 min

**Topics:**
- **Longest Increasing Subsequence (LIS)**
  - Find longest subsequence with increasing values
  - O(n¬≤) DP: dp[i] = longest ending at i
    - Transition: dp[i] = max(dp[j] + 1 for all j < i where arr[j] < arr[i])
  - O(n log n) binary search: maintain tail array
  - Time: O(n¬≤) or O(n log n)

- **Longest Decreasing Subsequence**
  - Symmetric to LIS
  - Same approaches apply

- **Maximum Subarray Sum (Kadane)**
  - dp[i] = max sum ending at i
  - Transition: dp[i] = max(arr[i], dp[i-1] + arr[i])
  - Answer: max(dp[i])

- **Weighted Interval Scheduling**
  - Intervals with weights, select non-overlapping to maximize weight
  - Sort by end time
  - dp[i] = max weight using intervals up to i
  - Transition: include i or exclude i
  - Find previous compatible interval via binary search
  - Time: O(n log n)

- **Subsequence Variants**
  - Distinct subsequences
  - Delete minimum characters to make palindrome
  - Shortest supersequence containing two strings

---

### üìÖ DAY 5 (OPTIONAL): STORY-DRIVEN DP | 90 min

**Topics:**
- **Problem Interpretation**
  - Complex DP problems often have story/real-world context
  - Key: translate story into state and transitions
  - Example: text justification, blackjack, etc.

- **Text Justification**
  - Format text with width constraint
  - Minimize "badness" (wasted space penalty)
  - DP: dp[i] = min badness for words 0..i
  - Transition: try all ending positions for current line

- **State Design Principles**
  - State should capture: what matters for future decisions?
  - Example: in activity scheduling, only finish time matters
  - Example: in coin change, only remaining amount matters
  - Careful: too specific ‚Üí huge state space; too general ‚Üí can't recover answer

---

## üìå WEEK 11: DYNAMIC PROGRAMMING II - TREES, DAGS & ADVANCED

### Weekly Goal
Extend DP to more complex structures. Master tree DP, DAG DP, bitmask DP, and advanced patterns.

### Weekly Outcomes
- Solve tree DP problems (diameter, maximum independent set)
- Apply DP to DAGs (longest path)
- Solve bitmask DP problems (TSP, subset selection)
- Optimize DP using state compression

### Summary
DP is incredibly powerful. This week extends basic patterns to more complex structures. Students learn when to apply DP, how to design states for different problems, and when to combine DP with other techniques.

---

### üìÖ DAY 1: DP ON TREES | 120 min

**Topics:**
- **Tree DP Framework**
  - Solve for each subtree, combine into parent
  - Post-order traversal (solve children before parent)
  - State: what's the answer for this subtree?

- **Maximum Independent Set**
  - Select nodes with no edges between them
  - Maximize value of selected nodes
  - dp[node][0] = max value excluding node
  - dp[node][1] = max value including node
  - Transition: combine children's answers

- **Tree Diameter via DP**
  - Longest path in tree
  - dp[node] = max distance down from node
  - Answer: max(left_depth + right_depth + 1)
  - Time: O(n)

- **Subtree Computation**
  - Simple: sum all nodes in subtree
  - Post-order: solve children, add to node

- **Tree Coloring (K Colors)**
  - Color nodes with K colors, adjacent different
  - dp[node][color] = count of ways
  - Transition: children must use different colors

- **Practical Considerations**
  - Space: often work with adjacency list (no explicit table)
  - Time: often O(n) if each node processed once
  - Recursion depth: can hit stack limits for very deep trees

---

### üìÖ DAY 2: DP ON DAGS | 120 min

**Topics:**
- **DAG (Directed Acyclic Graph)**
  - Directed graph with no cycles
  - Enables topological ordering
  - DP naturally applies to DAG

- **Longest Path in DAG**
  - No cycles ‚Üí DP works
  - Topological sort order ‚Üí compute in order
  - dp[node] = longest path starting at node
  - Transition: max(1 + dp[neighbor] for all neighbors)
  - Time: O(V + E)

- **Shortest Path in DAG with Negative Weights**
  - Bellman-Ford O(VE) needed for general graphs
  - DAG allows O(V + E) via topo sort
  - Process nodes in topo order, relax edges

- **Vertex Weight Sum Paths**
  - Path value = sum of vertex weights (not edge weights)
  - Similar DP but sum vertices instead of edges

- **All Paths in DAG**
  - Count or enumerate paths from s to t
  - DP: dp[node] = number of paths from node to t
  - Transition: sum paths through neighbors

- **Topo Sort Ordering**
  - Critical: must process nodes where all predecessors processed
  - Failure to respect order ‚Üí wrong answer

---

### üìÖ DAY 3: BITMASK & SUBSET DP | 120 min

**Topics:**
- **Bitmask Representation**
  - Set of n elements ‚Üí bitmask of 2^n states
  - Bit i = 1 if element i in subset
  - Iterate over subsets: 0 to 2^n - 1

- **Subset Enumeration**
  - For each mask: iterate elements to see which included
  - Iterate submasks: clever bit manipulation

- **TSP with DP (Traveling Salesman)**
  - Visit n cities exactly once, minimum cost
  - Naive: O(n!) permutations
  - DP: state = (visited_cities, current_city)
  - dp[mask][last] = min cost visiting cities in mask, ending at last
  - Transition: visit next unvisited city
  - Time: O(2^n √ó n¬≤)

- **Subset Sum Problems**
  - Count/find subsets with specific sum
  - dp[i] = ways to make sum i
  - Transition: include/exclude elements

- **Maximum Weight Independent Set (Small Graph)**
  - Can use bitmask if vertices few (‚â§ 20)
  - dp[mask] = max weight with vertices in mask, no edges between
  - Enumerate all masks, check if valid independent set

- **Complexity**
  - Time: O(2^n √ó f(n)) where f(n) is work per state
  - Space: O(2^n) for DP table
  - Feasible: n ‚â§ 20, marginal for n ‚â§ 25

---

### üìÖ DAY 4 (OPTIONAL): STATE COMPRESSION & OPTIMIZATIONS | 90 min

**Topics:**
- **Space Optimization**
  - Sliding window: keep only current and previous states
  - Example: 2D ‚Üí 1D if only need previous row
  - Saves from O(m √ó n) to O(n)

- **Dimension Reduction**
  - 3D DP ‚Üí 2D by observing what matters
  - Example: coin change with denominations ‚Üí doesn't need 3D

- **Memoization vs Tabulation Trade-off**
  - Memoization: compute only needed states
  - Tabulation: compute all, more cache-friendly
  - Choose based on state space sparsity

- **Pruning in DP**
  - Skip invalid states
  - Early termination if answer found
  - Example: if current cost exceeds best known, prune

---

### üìÖ DAY 5 (OPTIONAL): MIXED DP PROBLEMS | 90 min

**Topics:**
- **Multi-Concept Problems**
  - Combining DP with other algorithms
  - Example: DP on graph shortest paths
  - Example: DP with greedy choices

- **Recognition & Intuition**
  - Overlapping subproblems ‚Üí DP candidate
  - Optimal substructure ‚Üí DP candidate
  - Practice: solve many problems builds intuition

---

# üüß PHASE D: ALGORITHM PARADIGMS
## Weeks 12-13 | 25-30 hours | Greedy, Backtracking, B&B, Analysis

*(Abbreviated for space, full content available in detailed syllabus)*

---

# üü™ PHASE E: INTEGRATION & EXTENSIONS
## Weeks 14-15 | 25-30 hours | Specialized Techniques

*(Abbreviated for space, full content available in detailed syllabus)*

---

# üü´ PHASE F: ADVANCED DEEP DIVES (Optional)
## Weeks 16-18 | 35-40 hours | Competitive Programming

*(Optional advanced material, full content available in detailed syllabus)*

---

# üî¥ PHASE G: MOCK INTERVIEWS & FINAL REVIEW
## Week 19 | 10-12 hours | Interview Preparation

*(Interview practice and final review)*

---

## üìä CURRICULUM STATISTICS

### Coverage by Phase
| Phase | Weeks | Data Structures | Algorithms | Paradigms | Patterns |
|-------|-------|-----------------|-----------|-----------|----------|
| A | 1-3 | Arrays, Lists, Stacks, Queues, Heaps, Hash Tables | Search, Sort, Hash | Recursion | Fundamentals |
| B | 4-6 | Arrays, Strings | Two-Pointer, Sliding Window, Hashing | Greedy, BS | 50+ Patterns |
| C | 7-11 | Trees, Graphs | DFS/BFS, Shortest Path, MST, DP | Graph, DP | 100+ Patterns |
| D | 12-13 | Trees, Graphs | MST, Greedy, Backtracking, B&B | Greedy, Backtrack, B&B | Analysis |
| E | 14-15 | Special, Strings, Flow | Advanced String, Network Flow | Integration | Specialized |
| F | 16-18 | Segment Trees, Geometry | Advanced Graph, Geometry | Competitive | Advanced |
| G | 19 | - | Interview Problems | Mixed | Problem-Solving |

### Topics Count
- **Data Structures:** 20+ types
- **Algorithm Paradigms:** 15+ paradigms
- **Techniques:** 50+ techniques
- **Problems:** 200+ problem types

---

## üéì LEARNING FRAMEWORK

### By Difficulty Level
- ‚≠ê‚≠ê Easy: 30 days (~Weeks 1-2, select weeks)
- ‚≠ê‚≠ê‚≠ê Medium: 45 days (~Weeks 3-6, 8-10)
- ‚≠ê‚≠ê‚≠ê‚≠ê Hard: 25 days (~Weeks 7, 9, 11-13)
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Expert: 15 days (~Weeks 14-18)

### Study Methodology
1. **Daily:** 90-120 minute sessions with topics and practice
2. **Weekly:** Integrate topics, solve week-long problems
3. **Monthly:** Mock interviews, comprehensive assessment
4. **Continuous:** Reinforce weak areas, expand practice

### Success Indicators
- [ ] Master all core data structures
- [ ] Recognize patterns within seconds
- [ ] Implement solutions efficiently
- [ ] Explain reasoning clearly
- [ ] Solve novel problems independently

---

**Complete Data Structures & Algorithms Curriculum v13**  
*Professional-Grade 19-Week Comprehensive Syllabus*  
*Ready for immediate implementation and learning*

---